{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrFrozenBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n",
    "    torchvision.models.resnet[18,34,50,101] produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n",
    "    ):\n",
    "        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it user-friendly\n",
    "        weight = self.weight.reshape(1, -1, 1, 1)\n",
    "        bias = self.bias.reshape(1, -1, 1, 1)\n",
    "        running_var = self.running_var.reshape(1, -1, 1, 1)\n",
    "        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        epsilon = 1e-5\n",
    "        scale = weight * (running_var + epsilon).rsqrt()\n",
    "        bias = bias - running_mean * scale\n",
    "        return x * scale + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_batch_norm(m, name=\"\"):\n",
    "    for attr_str in dir(m):\n",
    "        target_attr = getattr(m, attr_str)\n",
    "        if isinstance(target_attr, nn.BatchNorm2d):\n",
    "            frozen = DetrFrozenBatchNorm2d(target_attr.num_features)\n",
    "            bn = getattr(m, attr_str)\n",
    "            frozen.weight.data.copy_(bn.weight)\n",
    "            frozen.bias.data.copy_(bn.bias)\n",
    "            frozen.running_mean.data.copy_(bn.running_mean)\n",
    "            frozen.running_var.data.copy_(bn.running_var)\n",
    "            setattr(m, attr_str, frozen)\n",
    "    for n, ch in m.named_children():\n",
    "        replace_batch_norm(ch, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = DetrFrozenBatchNorm2d(mid_channels)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = DetrFrozenBatchNorm2d(mid_channels)\n",
    "        self.drop_block = nn.Identity()\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "        self.aa = nn.Identity()\n",
    "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = DetrFrozenBatchNorm2d(out_channels)\n",
    "        self.act3 = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.drop_block(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.aa(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.act3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=True)\n",
    "        self.bn1 = DetrFrozenBatchNorm2d(mid_channels)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn2 = DetrFrozenBatchNorm2d(mid_channels)\n",
    "        self.drop_block = nn.Identity()\n",
    "        self.act2 = nn.ReLU(inplace=True)\n",
    "        self.aa = nn.Identity()\n",
    "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=True)\n",
    "        self.bn3 = DetrFrozenBatchNorm2d(out_channels)\n",
    "        self.act3 = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.drop_block(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.aa(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.act3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureListNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureListNet, self).__init__()\n",
    "        # Add all the layers as per the provided model description\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        self.bn1 = DetrFrozenBatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        self.layer1 = self._make_layer(32, 32, 64, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 32, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 64, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 128, 512, 3, stride=2)\n",
    "\n",
    "    def _make_layer(self, in_channels, mid_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0:\n",
    "                downsample = None\n",
    "                if stride != 1 or in_channels != out_channels:\n",
    "                    downsample = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                        DetrFrozenBatchNorm2d(out_channels)\n",
    "                    )\n",
    "                layers.append(Bottleneck(in_channels, mid_channels, out_channels, stride, downsample))\n",
    "            else:\n",
    "                layers.append(Bottleneck(out_channels, mid_channels, out_channels, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        out.append(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        out.append(x)\n",
    "        x = self.layer2(x)\n",
    "        out.append(x)\n",
    "        x = self.layer3(x)\n",
    "        out.append(x)\n",
    "        x = self.layer4(x)\n",
    "        out.append(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional backbone, using either the AutoBackbone API or one from the timm library.\n",
    "    nn.BatchNorm2d layers are replaced by DetrFrozenBatchNorm2d as defined above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone1 = FeatureListNet()\n",
    "        self.intermediate_channel_sizes = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # replace batch norm by frozen batch norm\n",
    "        with torch.no_grad():\n",
    "            replace_batch_norm(backbone1)\n",
    "        self.model = backbone1\n",
    "        #self.intermediate_channel_sizes = self.model.channels\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n",
    "        # send pixel_values through the model to get list of feature maps\n",
    "        \n",
    "        features = self.model(pixel_values)\n",
    "        out = []\n",
    "        for feature_map in features:\n",
    "            # downsample pixel_mask to match shape of corresponding feature_map\n",
    "            mask = nn.functional.interpolate(pixel_mask[None].float(), size=feature_map.shape[-2:]).to(torch.bool)[0]\n",
    "            out.append((feature_map, mask))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrConvModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This module adds 2D position embeddings to all intermediate feature maps of the convolutional encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conv_encoder, position_embedding):\n",
    "        super().__init__()\n",
    "        self.conv_encoder = conv_encoder\n",
    "        self.position_embedding = position_embedding\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        # send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\n",
    "        out = self.conv_encoder(pixel_values, pixel_mask)\n",
    "        pos = []\n",
    "        for feature_map, mask in out:\n",
    "            # position encoding\n",
    "            pos.append(self.position_embedding(feature_map, mask).to(feature_map.dtype))\n",
    "\n",
    "        return out, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, target_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[batch_size, seq_len]` to `[batch_size, 1, target_seq_len, source_seq_len]`.\n",
    "    \"\"\"\n",
    "    batch_size, source_len = mask.size()\n",
    "    target_len = target_len if target_len is not None else source_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(batch_size, 1, target_len, source_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrSinePositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n",
    "    need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        if pixel_mask is None:\n",
    "            raise ValueError(\"No pixel mask provided\")\n",
    "        y_embed = pixel_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = pixel_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + 1e-6) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + 1e-6) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.embedding_dim, dtype=torch.float32, device=pixel_values.device)\n",
    "        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.embedding_dim)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrLearnedPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        self.row_embeddings = nn.Embedding(50, embedding_dim)\n",
    "        self.column_embeddings = nn.Embedding(50, embedding_dim)\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask=None):\n",
    "        height, width = pixel_values.shape[-2:]\n",
    "        width_values = torch.arange(width, device=pixel_values.device)\n",
    "        height_values = torch.arange(height, device=pixel_values.device)\n",
    "        x_emb = self.column_embeddings(width_values)\n",
    "        y_emb = self.row_embeddings(height_values)\n",
    "        pos = torch.cat([x_emb.unsqueeze(0).repeat(height, 1, 1), y_emb.unsqueeze(1).repeat(1, width, 1)], dim=-1)\n",
    "        pos = pos.permute(2, 0, 1)\n",
    "        pos = pos.unsqueeze(0)\n",
    "        pos = pos.repeat(pixel_values.shape[0], 1, 1, 1)\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_position_encoding(d_model, position_embedding_type):\n",
    "    n_steps = d_model // 2\n",
    "    if position_embedding_type == \"sine\":\n",
    "        # TODO find a better way of exposing other arguments\n",
    "        position_embedding = DetrSinePositionEmbedding(n_steps, normalize=True)\n",
    "    elif position_embedding_type == \"learned\":\n",
    "        position_embedding = DetrLearnedPositionEmbedding(n_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported {position_embedding_type}\")\n",
    "\n",
    "    return position_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-headed attention from 'Attention Is All You Need' paper.\n",
    "    Here, we add position embeddings to the queries and keys (as explained in the DETR paper).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        if self.head_dim * num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, batch_size: int):\n",
    "        return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n",
    "        return tensor if position_embeddings is None else tensor + position_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_embeddings: Optional[torch.Tensor] = None,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        key_value_position_embeddings: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        batch_size, target_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # add position embeddings to the hidden states before projecting to queries and keys\n",
    "        if position_embeddings is not None:\n",
    "            hidden_states_original = hidden_states\n",
    "            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n",
    "\n",
    "        # add key-value position embeddings to the key value states\n",
    "        if key_value_position_embeddings is not None:\n",
    "            key_value_states_original = key_value_states\n",
    "            key_value_states = self.with_pos_embed(key_value_states, key_value_position_embeddings)\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, batch_size)\n",
    "            value_states = self._shape(self.v_proj(key_value_states_original), -1, batch_size)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, batch_size)\n",
    "            value_states = self._shape(self.v_proj(hidden_states_original), -1, batch_size)\n",
    "\n",
    "        proj_shape = (batch_size * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, target_len, batch_size).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        source_len = key_states.size(1)\n",
    "\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (batch_size * self.num_heads, target_len, source_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size * self.num_heads, target_len, source_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (batch_size, 1, target_len, source_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(batch_size, 1, target_len, source_len)}, but is\"\n",
    "                    f\" {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(batch_size, self.num_heads, target_len, source_len)\n",
    "            attn_weights = attn_weights_reshaped.view(batch_size * self.num_heads, target_len, source_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (batch_size * self.num_heads, target_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size, self.num_heads, target_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(batch_size, self.num_heads, target_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(batch_size, target_len, embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, encoder_attention_heads, attention_dropout, activation_dropout, dropout, encoder_ffn_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "        self.self_attn = DetrAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=encoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_embeddings: torch.Tensor = None,\n",
    "        output_attentions: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n",
    "                values.\n",
    "            position_embeddings (`torch.FloatTensor`, *optional*): position embeddings, to be added to hidden_states.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if self.training:\n",
    "            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
    "                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, decoder_attention_heads, attention_dropout, activation_dropout, dropout, decoder_ffn_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "\n",
    "        self.self_attn = DetrAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.encoder_attn = DetrAttention(\n",
    "            self.embed_dim,\n",
    "            decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.embed_dim, decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_embeddings: Optional[torch.Tensor] = None,\n",
    "        query_position_embeddings: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n",
    "                values.\n",
    "            position_embeddings (`torch.FloatTensor`, *optional*):\n",
    "                position embeddings that are added to the queries and keys\n",
    "            in the cross-attention layer.\n",
    "            query_position_embeddings (`torch.FloatTensor`, *optional*):\n",
    "                position embeddings that are added to the queries and keys\n",
    "            in the self-attention layer.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n",
    "                values.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=query_position_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "\n",
    "            hidden_states, cross_attn_weights = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                position_embeddings=query_position_embeddings,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                key_value_position_embeddings=position_embeddings,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, inner_dim: int, num_classes: int, pooler_dropout: float):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.out_proj(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`DetrEncoderLayer`].\n",
    "    The encoder updates the flattened feature map through multiple self-attention layers.\n",
    "    Small tweak for DETR:\n",
    "    - position_embeddings are added to the forward pass.\n",
    "    Args:\n",
    "        config: DetrConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layers, encoder_layerdrop, d_model, encoder_attention_heads, \n",
    "                 attention_dropout, activation_dropout, dropout, encoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = encoder_layerdrop\n",
    "\n",
    "        self.layers = nn.ModuleList([DetrEncoderLayer(d_model, encoder_attention_heads, \n",
    "                 attention_dropout, activation_dropout, dropout, encoder_ffn_dim) for _ in range(encoder_layers)])\n",
    "\n",
    "        # in the original DETR, no layernorm is used at the end of the encoder, as \"normalize_before\" is set to False by default\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        position_embeddings=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Flattened feature map (output of the backbone + projection layer) that is passed to the encoder.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding pixel features. Mask values selected in `[0, 1]`:\n",
    "                - 1 for pixel features that are real (i.e. **not masked**),\n",
    "                - 0 for pixel features that are padding (i.e. **masked**).\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            position_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                Position embeddings that are added to the queries and keys in each self-attention layer.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        for i, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                # we add position_embeddings as extra input to the encoder_layer\n",
    "                layer_outputs = encoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`DetrDecoderLayer`].\n",
    "    The decoder updates the query embeddings through multiple self-attention and cross-attention layers.\n",
    "    Some small tweaks for DETR:\n",
    "    - position_embeddings and query_position_embeddings are added to the forward pass.\n",
    "    - if self.config.auxiliary_loss is set to True, also returns a stack of activations from all decoding layers.\n",
    "    Args:\n",
    "        config: DetrConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_layers, decoder_layerdrop, d_model, decoder_attention_heads, \n",
    "                 attention_dropout, activation_dropout, dropout, decoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict, auxiliary_loss):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = decoder_layerdrop\n",
    "\n",
    "        self.layers = nn.ModuleList([DetrDecoderLayer(d_model, decoder_attention_heads, \n",
    "                 attention_dropout, activation_dropout, dropout, decoder_ffn_dim) for _ in range(decoder_layers)])\n",
    "        # in DETR, the decoder uses layernorm after the last decoder layer output\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "        self.auxiliary_loss = auxiliary_loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        position_embeddings=None,\n",
    "        query_position_embeddings=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                The query embeddings that are passed into the decoder.\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on certain queries. Mask values selected in `[0, 1]`:\n",
    "                - 1 for queries that are **not masked**,\n",
    "                - 0 for queries that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
    "                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n",
    "                in `[0, 1]`:\n",
    "                - 1 for pixels that are real (i.e. **not masked**),\n",
    "                - 0 for pixels that are padding (i.e. **masked**).\n",
    "            position_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Position embeddings that are added to the queries and keys in each cross-attention layer.\n",
    "            query_position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n",
    "                , *optional*): Position embeddings that are added to the queries and keys in each self-attention layer.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        if inputs_embeds is not None:\n",
    "            hidden_states = inputs_embeds\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        combined_attention_mask = None\n",
    "\n",
    "        if attention_mask is not None and combined_attention_mask is not None:\n",
    "            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n",
    "            combined_attention_mask = combined_attention_mask + _expand_mask(\n",
    "                attention_mask, inputs_embeds.dtype, target_len=input_shape[-1]\n",
    "            )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [batch_size, seq_len] -> [batch_size, 1, target_seq_len, source_seq_len]\n",
    "            encoder_attention_mask = _expand_mask(\n",
    "                encoder_attention_mask, inputs_embeds.dtype, target_len=input_shape[-1]\n",
    "            )\n",
    "\n",
    "        # optional intermediate hidden states\n",
    "        intermediate = () if self.auxiliary_loss else None\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):\n",
    "                continue\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    combined_attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    None,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=combined_attention_mask,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                    query_position_embeddings=query_position_embeddings,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.auxiliary_loss:\n",
    "                hidden_states = self.layernorm(hidden_states)\n",
    "                intermediate += (hidden_states,)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # finally, apply layernorm\n",
    "        hidden_states = self.layernorm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        # stack intermediate decoder activations\n",
    "        if self.auxiliary_loss:\n",
    "            intermediate = torch.stack(intermediate)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, all_hidden_states, all_self_attns, all_cross_attentions, intermediate]\n",
    "                if v is not None\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrModel(nn.Module):\n",
    "    def __init__(self, use_timm_backbone, dilation, backbone, use_pretrained_backbone, \n",
    "                 num_channels, backbone_config, d_model, num_queries, position_embedding_type, \n",
    "                 encoder_layers, decoder_layers, encoder_layerdrop, decoder_layerdrop, \n",
    "                 encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 activation_dropout, dropout, encoder_ffn_dim, decoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict, auxiliary_loss):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create backbone + positional encoding\n",
    "        backbone = DetrConvEncoder(num_channels)\n",
    "        position_embeddings = build_position_encoding(d_model, position_embedding_type)\n",
    "        self.backbone = DetrConvModel(backbone, position_embeddings)\n",
    "\n",
    "        # Create projection layer\n",
    "        self.input_projection = nn.Conv2d(backbone.intermediate_channel_sizes[-1], d_model, kernel_size=1)\n",
    "\n",
    "        self.query_position_embeddings = nn.Embedding(num_queries, d_model)\n",
    "\n",
    "        self.encoder = DetrEncoder(encoder_layers, encoder_layerdrop, d_model, \n",
    "                 encoder_attention_heads, attention_dropout, activation_dropout, \n",
    "                 dropout, encoder_ffn_dim, output_attentions, output_hidden_states, \n",
    "                 use_return_dict)\n",
    "        self.decoder = DetrDecoder(decoder_layers, decoder_layerdrop, d_model, \n",
    "                 decoder_attention_heads, attention_dropout, activation_dropout, \n",
    "                 dropout, decoder_ffn_dim, output_attentions, output_hidden_states, \n",
    "                 use_return_dict, auxiliary_loss)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for name, param in self.backbone.conv_encoder.model.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for name, param in self.backbone.conv_encoder.model.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        pixel_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from transformers import AutoImageProcessor, DetrModel\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        >>> model = DetrModel.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        >>> # prepare image for the model\n",
    "        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        >>> # forward pass\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> # the last hidden states are the final query embeddings of the Transformer decoder\n",
    "        >>> # these are of shape (batch_size, num_queries, hidden_size)\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "        >>> list(last_hidden_states.shape)\n",
    "        [1, 100, 256]\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        device = pixel_values.device\n",
    "\n",
    "        if pixel_mask is None:\n",
    "            pixel_mask = torch.ones(((batch_size, height, width)), device=device)\n",
    "\n",
    "        # First, sent pixel_values + pixel_mask through Backbone to obtain the features\n",
    "        # pixel_values should be of shape (batch_size, num_channels, height, width)\n",
    "        # pixel_mask should be of shape (batch_size, height, width)     \n",
    "        features, position_embeddings_list = self.backbone(pixel_values, pixel_mask)\n",
    "\n",
    "        # get final feature map and downsampled mask\n",
    "        feature_map, mask = features[-1]\n",
    "\n",
    "        if mask is None:\n",
    "            raise ValueError(\"Backbone does not return downsampled pixel mask\")\n",
    "\n",
    "        # Second, apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)\n",
    "        projected_feature_map = self.input_projection(feature_map)\n",
    "\n",
    "        # Third, flatten the feature map + position embeddings of shape NxCxHxW to NxCxHW, and permute it to NxHWxC\n",
    "        # In other words, turn their shape into (batch_size, sequence_length, hidden_size)\n",
    "        flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n",
    "        position_embeddings = position_embeddings_list[-1].flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        flattened_mask = mask.flatten(1)\n",
    "\n",
    "        # Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\n",
    "        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n",
    "        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n",
    "        \n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs_embeds=flattened_features,\n",
    "                attention_mask=flattened_mask,\n",
    "                position_embeddings=position_embeddings,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        # Fifth, sent query embeddings + position embeddings through the decoder (which is conditioned on the encoder output)\n",
    "        query_position_embeddings = self.query_position_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        queries = torch.zeros_like(query_position_embeddings)\n",
    "\n",
    "        # decoder outputs consists of (dec_features, dec_hidden, dec_attn)\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=queries,\n",
    "            attention_mask=None,\n",
    "            position_embeddings=position_embeddings,\n",
    "            query_position_embeddings=query_position_embeddings,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            encoder_attention_mask=flattened_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrMLPPredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Very simple multi-layer perceptron (MLP, also called FFN), used to predict the normalized center coordinates,\n",
    "    height and width of a bounding box w.r.t. an image.\n",
    "    Copied from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrForObjectDetection(nn.Module):\n",
    "    def __init__(self, num_labels, use_timm_backbone, dilation, backbone, use_pretrained_backbone, \n",
    "                 num_channels, backbone_config, d_model, num_queries, position_embedding_type, \n",
    "                 encoder_layers, decoder_layers, encoder_layerdrop, decoder_layerdrop, \n",
    "                 encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 activation_dropout, dropout, encoder_ffn_dim, decoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict, auxiliary_loss):\n",
    "        super().__init__()\n",
    "\n",
    "        # DETR encoder-decoder model\n",
    "        self.model = DetrModel(use_timm_backbone, dilation, backbone, use_pretrained_backbone, \n",
    "                 num_channels, backbone_config, d_model, num_queries, position_embedding_type, \n",
    "                 encoder_layers, decoder_layers, encoder_layerdrop, decoder_layerdrop, \n",
    "                 encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 activation_dropout, dropout, encoder_ffn_dim, decoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict, auxiliary_loss)\n",
    "\n",
    "        # Object detection heads\n",
    "        self.class_labels_classifier = nn.Linear(\n",
    "            d_model, num_labels + 1\n",
    "        )  # We add one for the \"no object\" class\n",
    "        self.bbox_predictor = DetrMLPPredictionHead(\n",
    "            input_dim=d_model, hidden_dim=d_model, output_dim=4, num_layers=3\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.num_queries = num_queries\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n",
    "    @torch.jit.unused\n",
    "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
    "        # this is a workaround to make torchscript happy, as torchscript\n",
    "        # doesn't support dictionary with non-homogeneous values, such\n",
    "        # as a dict having both a Tensor and a list.\n",
    "        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        pixel_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n",
    "            Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n",
    "            following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n",
    "            respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n",
    "            in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "        >>> import torch\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        >>> model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> # convert outputs (bounding boxes and class logits) to COCO API\n",
    "        >>> target_sizes = torch.tensor([image.size[::-1]])\n",
    "        >>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n",
    "        ...     0\n",
    "        ... ]\n",
    "        >>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        ...     box = [round(i, 2) for i in box.tolist()]\n",
    "        ...     print(\n",
    "        ...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        ...         f\"{round(score.item(), 3)} at location {box}\"\n",
    "        ...     )\n",
    "        Detected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\n",
    "        Detected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\n",
    "        Detected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\n",
    "        Detected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\n",
    "        Detected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\n",
    "        ```\"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        # First, sent images through DETR base model to obtain encoder + decoder outputs\n",
    "        outputs = self.model(\n",
    "            pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # class logits + predicted bounding boxes\n",
    "        logits = self.class_labels_classifier(sequence_output)\n",
    "        pred_boxes = self.bbox_predictor(sequence_output).sigmoid()\n",
    "\n",
    "        loss, loss_dict, auxiliary_outputs = None, None, None\n",
    "        if labels is not None:\n",
    "            # First: create the matcher\n",
    "            matcher = DetrHungarianMatcher(\n",
    "                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n",
    "            )\n",
    "            # Second: create the criterion\n",
    "            losses = [\"labels\", \"boxes\", \"cardinality\"]\n",
    "            criterion = DetrLoss(\n",
    "                matcher=matcher,\n",
    "                num_classes=self.config.num_labels,\n",
    "                eos_coef=self.config.eos_coefficient,\n",
    "                losses=losses,\n",
    "            )\n",
    "            criterion.to(self.device)\n",
    "            # Third: compute the losses, based on outputs and labels\n",
    "            outputs_loss = {}\n",
    "            outputs_loss[\"logits\"] = logits\n",
    "            outputs_loss[\"pred_boxes\"] = pred_boxes\n",
    "            if self.config.auxiliary_loss:\n",
    "                intermediate = outputs.intermediate_hidden_states if return_dict else outputs[4]\n",
    "                outputs_class = self.class_labels_classifier(intermediate)\n",
    "                outputs_coord = self.bbox_predictor(intermediate).sigmoid()\n",
    "                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n",
    "\n",
    "            loss_dict = criterion(outputs_loss, labels)\n",
    "            # Fourth: compute total loss, as a weighted sum of the various losses\n",
    "            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n",
    "            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n",
    "            if self.config.auxiliary_loss:\n",
    "                aux_weight_dict = {}\n",
    "                for i in range(self.config.decoder_layers - 1):\n",
    "                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "                weight_dict.update(aux_weight_dict)\n",
    "            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        if not return_dict:\n",
    "            if auxiliary_outputs is not None:\n",
    "                output = (logits, pred_boxes) + auxiliary_outputs + outputs\n",
    "            else:\n",
    "                output = (logits, pred_boxes) + outputs\n",
    "            return ((loss, loss_dict) + output) if loss is not None else output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand(tensor, length: int):\n",
    "    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrMaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm. Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, context_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim % 8 != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden_size + number of attention heads must be divisible by 8 as the number of groups in\"\n",
    "                \" GroupNorm is set to 8\"\n",
    "            )\n",
    "\n",
    "        inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 32, context_dim // 64, context_dim // 128]\n",
    "\n",
    "        self.lay1 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(8, dim)\n",
    "        self.lay2 = nn.Conv2d(dim, inter_dims[1], 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, inter_dims[1])\n",
    "        self.lay3 = nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(8, inter_dims[2])\n",
    "        self.lay4 = nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)\n",
    "        self.gn4 = nn.GroupNorm(8, inter_dims[3])\n",
    "        self.lay5 = nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)\n",
    "        self.gn5 = nn.GroupNorm(8, inter_dims[4])\n",
    "        self.lay6 = nn.Conv2d(inter_dims[4], inter_dims[5], 3, padding=1)\n",
    "        self.gn6 = nn.GroupNorm(8, inter_dims[5])\n",
    "        self.lay7 = nn.Conv2d(inter_dims[5], inter_dims[6], 3, padding=1)\n",
    "        self.gn7 = nn.GroupNorm(8, inter_dims[6])\n",
    "        \n",
    "        self.out_lay = nn.Conv2d(inter_dims[6], 1, 3, padding=1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = nn.Conv2d(fpn_dims[0], inter_dims[1], 1)\n",
    "        self.adapter2 = nn.Conv2d(fpn_dims[1], inter_dims[2], 1)\n",
    "        self.adapter3 = nn.Conv2d(fpn_dims[2], inter_dims[3], 1)  \n",
    "        self.adapter4 = nn.Conv2d(fpn_dims[3], inter_dims[4], 1)\n",
    "        self.adapter5 = nn.Conv2d(fpn_dims[4], inter_dims[5], 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n",
    "        # here we concatenate x, the projected feature map, of shape (batch_size, d_model, heigth/32, width/32) with\n",
    "        # the bbox_mask = the attention maps of shape (batch_size, n_queries, n_heads, height/32, width/32).\n",
    "        # We expand the projected feature map to match the number of heads.\n",
    "        x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)\n",
    "\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        cur_fpn = self.adapter4(fpns[3])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay6(x)\n",
    "        x = self.gn6(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        cur_fpn = self.adapter5(fpns[4])\n",
    "        if cur_fpn.size(0) != x.size(0):\n",
    "            cur_fpn = _expand(cur_fpn, x.size(0) // cur_fpn.size(0))\n",
    "        x = cur_fpn + nn.functional.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay7(x)\n",
    "        x = self.gn7(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.out_lay(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrMHAttentionMap(nn.Module):\n",
    "    \"\"\"This is a 2D attention module, which only returns the attention softmax (no multiplication by value)\"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0.0, bias=True, std=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)\n",
    "\n",
    "        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, q, k, mask: Optional[Tensor] = None):\n",
    "        q = self.q_linear(q)\n",
    "        k = nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)\n",
    "        queries_per_head = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        keys_per_head = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])\n",
    "        weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n",
    "        weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n",
    "        weights = self.dropout(weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrForSegmentation(nn.Module):\n",
    "    def __init__(self, num_labels, use_timm_backbone, dilation, backbone, use_pretrained_backbone, \n",
    "                 num_channels, backbone_config, d_model, num_queries, position_embedding_type, \n",
    "                 encoder_layers, decoder_layers, encoder_layerdrop, decoder_layerdrop, \n",
    "                 encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 activation_dropout, dropout, encoder_ffn_dim, decoder_ffn_dim, output_attentions, \n",
    "                 output_hidden_states, use_return_dict, auxiliary_loss, init_xavier_std):\n",
    "        super().__init__()\n",
    "\n",
    "        # object detection model\n",
    "        self.detr = DetrForObjectDetection(num_labels, use_timm_backbone, dilation, backbone, \n",
    "                 use_pretrained_backbone, num_channels, backbone_config, d_model, num_queries, \n",
    "                 position_embedding_type, encoder_layers, decoder_layers, encoder_layerdrop, \n",
    "                 decoder_layerdrop, encoder_attention_heads, decoder_attention_heads, \n",
    "                 attention_dropout, activation_dropout, dropout, encoder_ffn_dim, decoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict, auxiliary_loss)\n",
    "\n",
    "        # segmentation head\n",
    "        hidden_size, number_of_heads = d_model, encoder_attention_heads\n",
    "        intermediate_channel_sizes = self.detr.model.backbone.conv_encoder.intermediate_channel_sizes\n",
    "\n",
    "        self.mask_head = DetrMaskHeadSmallConv(\n",
    "            hidden_size + number_of_heads, intermediate_channel_sizes[::-1][-5:], hidden_size\n",
    "        )\n",
    "\n",
    "        self.bbox_attention = DetrMHAttentionMap(\n",
    "            hidden_size, hidden_size, number_of_heads, dropout=0.0, std=init_xavier_std\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "        \n",
    "        self.upsampling = nn.Conv2d(num_queries, 1, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        pixel_mask=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n",
    "            Labels for computing the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, each\n",
    "            dictionary containing at least the following 3 keys: 'class_labels', 'boxes' and 'masks' (the class labels,\n",
    "            bounding boxes and segmentation masks of an image in the batch respectively). The class labels themselves\n",
    "            should be a `torch.LongTensor` of len `(number of bounding boxes in the image,)`, the boxes a\n",
    "            `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)` and the masks a\n",
    "            `torch.FloatTensor` of shape `(number of bounding boxes in the image, height, width)`.\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> import io\n",
    "        >>> import requests\n",
    "        >>> from PIL import Image\n",
    "        >>> import torch\n",
    "        >>> import numpy\n",
    "        >>> from transformers import AutoImageProcessor, DetrForSegmentation\n",
    "        >>> from transformers.image_transforms import rgb_to_id\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "        >>> model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "        >>> # prepare image for the model\n",
    "        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        >>> # forward pass\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> # Use the `post_process_panoptic_segmentation` method of the `image_processor` to retrieve post-processed panoptic segmentation maps\n",
    "        >>> # Segmentation results are returned as a list of dictionaries\n",
    "        >>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(300, 500)])\n",
    "        >>> # A tensor of shape (height, width) where each value denotes a segment id, filled with -1 if no segment is found\n",
    "        >>> panoptic_seg = result[0][\"segmentation\"]\n",
    "        >>> # Get prediction score and segment_id to class_id mapping of each segment\n",
    "        >>> panoptic_segments_info = result[0][\"segments_info\"]\n",
    "        ```\"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        device = pixel_values.device\n",
    "\n",
    "        if pixel_mask is None:\n",
    "            pixel_mask = torch.ones((batch_size, height, width), device=device)\n",
    "\n",
    "        # First, get list of feature maps and position embeddings\n",
    "        features, position_embeddings_list = self.detr.model.backbone(pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        # Second, apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)\n",
    "        feature_map, mask = features[-1]\n",
    "        batch_size, num_channels, height, width = feature_map.shape\n",
    "        projected_feature_map = self.detr.model.input_projection(feature_map)\n",
    "\n",
    "        # Third, flatten the feature map + position embeddings of shape NxCxHxW to NxCxHW, and permute it to NxHWxC\n",
    "        # In other words, turn their shape into (batch_size, sequence_length, hidden_size)\n",
    "        flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n",
    "        position_embeddings = position_embeddings_list[-1].flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        flattened_mask = mask.flatten(1)\n",
    "\n",
    "        # Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\n",
    "        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n",
    "        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.detr.model.encoder(\n",
    "                inputs_embeds=flattened_features,\n",
    "                attention_mask=flattened_mask,\n",
    "                position_embeddings=position_embeddings,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        # Fifth, sent query embeddings + position embeddings through the decoder (which is conditioned on the encoder output)\n",
    "        query_position_embeddings = self.detr.model.query_position_embeddings.weight.unsqueeze(0).repeat(\n",
    "            batch_size, 1, 1\n",
    "        )\n",
    "        queries = torch.zeros_like(query_position_embeddings)\n",
    "\n",
    "        # decoder outputs consists of (dec_features, dec_hidden, dec_attn)\n",
    "        decoder_outputs = self.detr.model.decoder(\n",
    "            inputs_embeds=queries,\n",
    "            attention_mask=None,\n",
    "            position_embeddings=position_embeddings,\n",
    "            query_position_embeddings=query_position_embeddings,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            encoder_attention_mask=flattened_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Sixth, compute logits, pred_boxes and pred_masks     \n",
    "        logits = self.detr.class_labels_classifier(sequence_output)\n",
    "        pred_boxes = self.detr.bbox_predictor(sequence_output).sigmoid()\n",
    "\n",
    "        memory = encoder_outputs[0].permute(0, 2, 1).view(batch_size, self.d_model, height, width)\n",
    "        mask = flattened_mask.view(batch_size, height, width)\n",
    "\n",
    "        # FIXME h_boxes takes the last one computed, keep this in mind\n",
    "        # important: we need to reverse the mask, since in the original implementation the mask works reversed\n",
    "        # bbox_mask is of shape (batch_size, num_queries, number_of_attention_heads in bbox_attention, height/32, width/32)\n",
    "        bbox_mask = self.bbox_attention(sequence_output, memory, mask=~mask)\n",
    "\n",
    "        seg_masks = self.mask_head(projected_feature_map, bbox_mask, [features[4][0], features[3][0], features[2][0], features[1][0], features[0][0]])\n",
    "\n",
    "        pred_masks = seg_masks.view(batch_size, self.detr.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])\n",
    "        pred_masks = self.upsampling(pred_masks)\n",
    "\n",
    "        loss, loss_dict, auxiliary_outputs = None, None, None\n",
    "        if labels is not None:\n",
    "            # First: create the matcher\n",
    "            matcher = DetrHungarianMatcher(\n",
    "                class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost\n",
    "            )\n",
    "            # Second: create the criterion\n",
    "            losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n",
    "            criterion = DetrLoss(\n",
    "                matcher=matcher,\n",
    "                num_classes=self.config.num_labels,\n",
    "                eos_coef=self.config.eos_coefficient,\n",
    "                losses=losses,\n",
    "            )\n",
    "            criterion.to(self.device)\n",
    "            # Third: compute the losses, based on outputs and labels\n",
    "            outputs_loss = {}\n",
    "            outputs_loss[\"logits\"] = logits\n",
    "            outputs_loss[\"pred_boxes\"] = pred_boxes\n",
    "            outputs_loss[\"pred_masks\"] = pred_masks\n",
    "            if self.config.auxiliary_loss:\n",
    "                intermediate = decoder_outputs.intermediate_hidden_states if return_dict else decoder_outputs[-1]\n",
    "                outputs_class = self.class_labels_classifier(intermediate)\n",
    "                outputs_coord = self.bbox_predictor(intermediate).sigmoid()\n",
    "                auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)\n",
    "                outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n",
    "\n",
    "            loss_dict = criterion(outputs_loss, labels)\n",
    "            # Fourth: compute total loss, as a weighted sum of the various losses\n",
    "            weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}\n",
    "            weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient\n",
    "            weight_dict[\"loss_mask\"] = self.config.mask_loss_coefficient\n",
    "            weight_dict[\"loss_dice\"] = self.config.dice_loss_coefficient\n",
    "            if self.config.auxiliary_loss:\n",
    "                aux_weight_dict = {}\n",
    "                for i in range(self.config.decoder_layers - 1):\n",
    "                    aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "                weight_dict.update(aux_weight_dict)\n",
    "            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        if not return_dict:\n",
    "            if auxiliary_outputs is not None:\n",
    "                output = pred_masks\n",
    "            else:\n",
    "                output = pred_masks\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "use_timm_backbone = True\n",
    "dilation = True\n",
    "backbone = \"resnet50\"\n",
    "use_pretrained_backbone = False\n",
    "num_channels = 3\n",
    "backbone_config = None\n",
    "d_model = 512\n",
    "num_queries = 10\n",
    "position_embedding_type = \"sine\"\n",
    "encoder_layers = 2\n",
    "decoder_layers = 2\n",
    "activation_dropout = 0\n",
    "dropout = 0\n",
    "encoder_layerdrop = 0.0\n",
    "decoder_layerdrop = 0.0\n",
    "encoder_attention_heads = 8\n",
    "decoder_attention_heads = 8\n",
    "attention_dropout = 0.0\n",
    "encoder_ffn_dim = 64\n",
    "decoder_ffn_dim = 64\n",
    "output_attentions = False\n",
    "output_hidden_states = False\n",
    "use_return_dict = False\n",
    "auxiliary_loss = True\n",
    "init_xavier_std = 0.01\n",
    "\n",
    "model = DetrForSegmentation(\n",
    "    num_labels=num_labels,\n",
    "    use_timm_backbone=use_timm_backbone,\n",
    "    dilation=dilation,\n",
    "    backbone=backbone,\n",
    "    use_pretrained_backbone=use_pretrained_backbone,\n",
    "    num_channels=num_channels,\n",
    "    backbone_config=backbone_config,\n",
    "    d_model=d_model,\n",
    "    num_queries=num_queries,\n",
    "    position_embedding_type=position_embedding_type,\n",
    "    encoder_layers=encoder_layers,\n",
    "    decoder_layers=decoder_layers,\n",
    "    activation_dropout=activation_dropout,\n",
    "    dropout=dropout,\n",
    "    encoder_layerdrop=encoder_layerdrop,\n",
    "    decoder_layerdrop=decoder_layerdrop,\n",
    "    encoder_attention_heads=encoder_attention_heads,\n",
    "    decoder_attention_heads=decoder_attention_heads,\n",
    "    attention_dropout=attention_dropout,\n",
    "    encoder_ffn_dim=encoder_ffn_dim,\n",
    "    decoder_ffn_dim=decoder_ffn_dim,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    use_return_dict=use_return_dict,\n",
    "    auxiliary_loss=auxiliary_loss,\n",
    "    init_xavier_std=init_xavier_std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 160, 240])\n"
     ]
    }
   ],
   "source": [
    "images = torch.randn(5, 3, 160, 240)\n",
    "outputs = model(pixel_values=images)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#from dataset import CarvanaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint_detr1.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_ds = CarvanaDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CarvanaDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cpu\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"saved_images_detr1/\", device=\"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2443422/3072000 with acc 79.54\n",
      "Dice score: 0.0011577124241739511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [40:04<00:00,  7.68s/it, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 2986419/3072000 with acc 97.21\n",
      "Dice score: 0.93243807554245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [38:06<00:00,  7.31s/it, loss=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3014420/3072000 with acc 98.13\n",
      "Dice score: 0.9546750783920288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [38:03<00:00,  7.30s/it, loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3020161/3072000 with acc 98.31\n",
      "Dice score: 0.9598810076713562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [37:53<00:00,  7.26s/it, loss=0.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3017832/3072000 with acc 98.24\n",
      "Dice score: 0.9579505920410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [38:13<00:00,  7.33s/it, loss=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3034554/3072000 with acc 98.78\n",
      "Dice score: 0.9705454707145691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [38:06<00:00,  7.30s/it, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3032774/3072000 with acc 98.72\n",
      "Dice score: 0.9689838290214539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [37:48<00:00,  7.25s/it, loss=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3035191/3072000 with acc 98.80\n",
      "Dice score: 0.9708572626113892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [37:58<00:00,  7.28s/it, loss=0.0339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3037519/3072000 with acc 98.88\n",
      "Dice score: 0.9727870225906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [37:54<00:00,  7.27s/it, loss=0.0299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3043261/3072000 with acc 99.06\n",
      "Dice score: 0.977099597454071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [37:50<00:00,  7.25s/it, loss=0.0287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3043246/3072000 with acc 99.06\n",
      "Dice score: 0.977076530456543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from model import UNET\n",
    "#from utils import (\n",
    "#    load_checkpoint,\n",
    "#    save_checkpoint,\n",
    "#    get_loaders,\n",
    "#    check_accuracy,\n",
    "#    save_predictions_as_imgs,\n",
    "#)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\train\"\n",
    "TRAIN_MASK_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\train_masks\"\n",
    "VAL_IMG_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\test\"\n",
    "VAL_MASK_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\test_masks\"\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(\"checkpoint_detr1.pth.tar\"), model)\n",
    "\n",
    "\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\":optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        # print some examples to a folder\n",
    "        save_predictions_as_imgs(\n",
    "            val_loader, model, folder=\"saved_images_detr1/\", device=DEVICE\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3043246/3072000 with acc 99.06\n",
      "Dice score: 0.977076530456543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [39:33<00:00,  7.58s/it, loss=0.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 3033972/3072000 with acc 98.76\n",
      "Dice score: 0.9699600338935852\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from model import UNET\n",
    "#from utils import (\n",
    "#    load_checkpoint,\n",
    "#    save_checkpoint,\n",
    "#    get_loaders,\n",
    "#    check_accuracy,\n",
    "#    save_predictions_as_imgs,\n",
    "#)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\train\"\n",
    "TRAIN_MASK_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\train_masks\"\n",
    "VAL_IMG_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\test\"\n",
    "VAL_MASK_DIR = \"D:\\\\deep learning specialization\\\\DETR\\\\Carvana dataset\\\\carvana-image-masking-challenge\\\\test_masks\"\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(\"checkpoint_detr1.pth.tar\"), model)\n",
    "\n",
    "\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\":optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "        # print some examples to a folder\n",
    "        save_predictions_as_imgs(\n",
    "            val_loader, model, folder=\"saved_images_detr1/\", device=DEVICE\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
