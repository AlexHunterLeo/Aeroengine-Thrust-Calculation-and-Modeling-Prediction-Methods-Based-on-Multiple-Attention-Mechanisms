{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed a sequence of categorical features.\n",
    "    Args:\n",
    "        cardinalities (`list[int]`):\n",
    "            List of cardinalities of the categorical features.\n",
    "        embedding_dims (`list[int]`):\n",
    "            List of embedding dimensions of the categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = len(cardinalities)\n",
    "        self.embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_features > 1:\n",
    "            # we slice the last dimension, giving an array of length\n",
    "            # self.num_features with shape (N,T) or (N)\n",
    "            cat_feature_slices = torch.chunk(features, self.num_features, dim=-1)\n",
    "        else:\n",
    "            cat_feature_slices = [features]\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                embed(cat_feature_slice.squeeze(-1))\n",
    "                for embed, cat_feature_slice in zip(self.embedders, cat_feature_slices)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerStdScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Standardize features by calculating the mean and scaling along some given dimension `dim`, and then normalizes it\n",
    "    by subtracting from the mean and dividing by the standard deviation.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to calculate the mean and standard deviation.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-5):\n",
    "            Default scale that is used for elements that are constantly zero along dimension `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False, minimum_scale: float = 1e-5):\n",
    "        super().__init__()\n",
    "        if not dim > 0:\n",
    "            raise ValueError(\"Cannot compute scale along dim = 0 (batch dimension), please provide dim > 0\")\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        denominator = weights.sum(self.dim, keepdim=self.keepdim)\n",
    "        denominator = denominator.clamp_min(1.0)\n",
    "        loc = (data * weights).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "\n",
    "        variance = (((data - loc) * weights) ** 2).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "        scale = torch.sqrt(variance + self.minimum_scale)\n",
    "        return (data - loc) / scale, loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerMeanScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a scaling factor as the weighted average absolute value along dimension `dim`, and scales the data\n",
    "    accordingly.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        default_scale (`float`, *optional*, defaults to `None`):\n",
    "            Default scale that is used for elements that are constantly zero. If `None`, we use the scale of the batch.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-10):\n",
    "            Default minimum possible scale that is used for any item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int = -1, keepdim: bool = True, default_scale: Optional[float] = None, minimum_scale: float = 1e-10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "        self.default_scale = default_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, observed_indicator: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # shape: (N, [C], T=1)\n",
    "        ts_sum = (data * observed_indicator).abs().sum(self.dim, keepdim=True)\n",
    "        num_observed = observed_indicator.sum(self.dim, keepdim=True)\n",
    "\n",
    "        scale = ts_sum / torch.clamp(num_observed, min=1)\n",
    "\n",
    "        # If `default_scale` is provided, we use it, otherwise we use the scale\n",
    "        # of the batch.\n",
    "        if self.default_scale is None:\n",
    "            batch_sum = ts_sum.sum(dim=0)\n",
    "            batch_observations = torch.clamp(num_observed.sum(0), min=1)\n",
    "            default_scale = torch.squeeze(batch_sum / batch_observations)\n",
    "        else:\n",
    "            default_scale = self.default_scale * torch.ones_like(scale)\n",
    "\n",
    "        # apply default scale where there are no observations\n",
    "        scale = torch.where(num_observed > 0, scale, default_scale)\n",
    "\n",
    "        # ensure the scale is at least `self.minimum_scale`\n",
    "        scale = torch.clamp(scale, min=self.minimum_scale)\n",
    "        scaled_data = data / scale\n",
    "\n",
    "        if not self.keepdim:\n",
    "            scale = scale.squeeze(dim=self.dim)\n",
    "\n",
    "        return scaled_data, torch.zeros_like(scale), scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerNOPScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Assigns a scaling factor equal to 1 along dimension `dim`, and therefore applies no scaling to the input data.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        scale = torch.ones_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        loc = torch.zeros_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        return data, loc, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "    Args:\n",
    "        input_tensor (`torch.FloatTensor`):\n",
    "            Input tensor, of which the average must be computed.\n",
    "        weights (`torch.FloatTensor`, *optional*):\n",
    "            Weights tensor, of the same shape as `input_tensor`.\n",
    "        dim (`int`, *optional*):\n",
    "            The dim along which to average `input_tensor`.\n",
    "    Returns:\n",
    "        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n",
    "        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n",
    "        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n",
    "    else:\n",
    "        return input_tensor.mean(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the negative log likelihood loss from input distribution with respect to target.\n",
    "    \"\"\"\n",
    "    return -input.log_prob(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerValueEmbedding(nn.Module):\n",
    "    def __init__(self, feature_size, d_model):\n",
    "        super().__init__()\n",
    "        self.value_projection = nn.Linear(in_features=feature_size, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value_projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerProbSparseAttention(nn.Module):\n",
    "    \"\"\"Probabilistic Attention mechanism to select the \"active\"\n",
    "    queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and\n",
    "    memory requirements of vanilla attention\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        sampling_factor: int = 5,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.factor = sampling_factor\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        key_states_time_length = key_states.size(1)  # L_K\n",
    "        log_key_states_time_length = np.ceil(np.log1p(key_states_time_length)).astype(\"int\").item()  # log_L_K\n",
    "\n",
    "        query_states_time_length = query_states.size(1)  # L_Q\n",
    "        log_query_states_time_length = np.ceil(np.log1p(query_states_time_length)).astype(\"int\").item()  # log_L_Q\n",
    "\n",
    "        u_part = min(self.factor * query_states_time_length * log_key_states_time_length, key_states_time_length)\n",
    "        u = min(self.factor * log_query_states_time_length, query_states_time_length)\n",
    "\n",
    "        if key_states_time_length > 0:\n",
    "            index_sample = torch.randint(0, key_states_time_length, (u_part,))\n",
    "            k_sample = key_states[:, index_sample, :]\n",
    "        else:\n",
    "            k_sample = key_states\n",
    "\n",
    "        queries_keys_sample = torch.bmm(query_states, k_sample.transpose(1, 2))  # Q_K_sampled\n",
    "\n",
    "        # find the Top_k query with sparsity measurement\n",
    "        if u > 0:\n",
    "            sparsity_measurement = queries_keys_sample.max(dim=-1)[0] - torch.div(\n",
    "                queries_keys_sample.sum(dim=-1), key_states_time_length\n",
    "            )  # M\n",
    "            top_u_sparsity_measurement = sparsity_measurement.topk(u, sorted=False)[1]  # M_top\n",
    "\n",
    "            # calculate q_reduce: query_states[:, top_u_sparsity_measurement]\n",
    "            dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\n",
    "            q_reduce = query_states[dim_for_slice, top_u_sparsity_measurement]\n",
    "        else:\n",
    "            q_reduce = query_states\n",
    "            top_u_sparsity_measurement = None\n",
    "\n",
    "        # Use q_reduce to calculate attention weights\n",
    "        attn_weights = torch.bmm(q_reduce, key_states.transpose(1, 2))\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        if attn_weights.size() != (bsz * self.num_heads, u, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, u, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            prob_mask = attention_mask.expand(bsz, self.num_heads, tgt_len, src_len).reshape(\n",
    "                bsz * self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "\n",
    "            if top_u_sparsity_measurement is not None:\n",
    "                dim_for_slice = torch.arange(prob_mask.size(0)).unsqueeze(-1)\n",
    "                prob_mask = prob_mask[dim_for_slice, top_u_sparsity_measurement, :]\n",
    "\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, u, src_len) + prob_mask.view(\n",
    "                bsz, self.num_heads, u, src_len\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, u, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, u, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        # calculate context for updating the attn_output, based on:\n",
    "        # https://github.com/zhouhaoyi/Informer2020/blob/ac59c7447135473fb2aafeafe94395f884d5c7a5/models/attn.py#L74\n",
    "        if self.is_decoder:\n",
    "            context = value_states.cumsum(dim=-2)\n",
    "        else:\n",
    "            v_mean_dim_time = value_states.mean(dim=-2)\n",
    "            context = (\n",
    "                v_mean_dim_time.unsqueeze(dim=1)\n",
    "                .expand(bsz * self.num_heads, query_states_time_length, v_mean_dim_time.size(-1))\n",
    "                .clone()\n",
    "            )\n",
    "\n",
    "        if top_u_sparsity_measurement is not None:\n",
    "            # update context: copy the attention output to the context at top_u_sparsity_measurement index\n",
    "            dim_for_slice = torch.arange(context.size(0)).unsqueeze(-1)\n",
    "            context[dim_for_slice, top_u_sparsity_measurement, :] = attn_output\n",
    "            attn_output = context\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode=\"circular\",\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, encoder_attention_heads, attention_dropout, sampling_factor, \n",
    "                 dropout, activation_dropout, encoder_ffn_dim, attention_type=\"prob\"):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "        if attention_type == \"prob\":\n",
    "            self.self_attn = InformerProbSparseAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=encoder_attention_heads,\n",
    "                dropout=attention_dropout,\n",
    "                sampling_factor=sampling_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.self_attn = InformerAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=encoder_attention_heads,\n",
    "                dropout=attention_dropout,\n",
    "            )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "        layer_head_mask: torch.FloatTensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, decoder_attention_heads, attention_dropout, sampling_factor,\n",
    "                 dropout, activation_dropout, decoder_ffn_dim, attention_type=\"prob\"):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "\n",
    "        if attention_type == \"prob\":\n",
    "            self.self_attn = InformerProbSparseAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=decoder_attention_heads,\n",
    "                dropout=attention_dropout,\n",
    "                sampling_factor=sampling_factor,\n",
    "                is_decoder=True,\n",
    "            )\n",
    "        else:\n",
    "            self.self_attn = InformerAttention(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=decoder_attention_heads,\n",
    "                dropout=attention_dropout,\n",
    "                is_decoder=True,\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.encoder_attn = InformerAttention(\n",
    "            self.embed_dim,\n",
    "            decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.embed_dim, decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                layer_head_mask=cross_attn_layer_head_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer encoder consisting of *config.encoder_layers* self attention layers with distillation layers. Each\n",
    "    attention layer is an [`InformerEncoderLayer`].\n",
    "    Args:\n",
    "        config: InformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, encoder_layers, encoder_layerdrop, context_length, \n",
    "                 prediction_length, d_model, encoder_attention_heads, attention_dropout, \n",
    "                 sampling_factor, dropout, activation_dropout, encoder_ffn_dim, output_attentions,\n",
    "                 output_hidden_states, use_return_dict, distil=True, attention_type=\"prob\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = encoder_layerdrop\n",
    "        self.gradient_checkpointing = False\n",
    "        if prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = InformerValueEmbedding(feature_size=feature_size, \n",
    "                                                      d_model=d_model)\n",
    "        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n",
    "            context_length + prediction_length, d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([InformerEncoderLayer(d_model, encoder_attention_heads, \n",
    "                 attention_dropout, sampling_factor, dropout, activation_dropout, encoder_ffn_dim, \n",
    "                 attention_type) for _ in range(encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(d_model)\n",
    "\n",
    "        if distil:\n",
    "            self.conv_layers = nn.ModuleList(\n",
    "                [InformerConvLayer(d_model) for _ in range(encoder_layers - 1)]\n",
    "            )\n",
    "            self.conv_layers.append(None)\n",
    "        else:\n",
    "            self.conv_layers = [None] * encoder_layers\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size())\n",
    "\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, (encoder_layer, conv_layer) in enumerate(zip(self.layers, self.conv_layers)):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = torch.utils.checkpoint.checkpoint(conv_layer, layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "                    if conv_layer is not None:\n",
    "                        output = conv_layer(layer_outputs[0])\n",
    "                        layer_outputs = (output,) + layer_outputs[1:]\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer decoder consisting of *config.decoder_layers* layers. Each layer is a [`InformerDecoderLayer`]\n",
    "    Args:\n",
    "        config: InformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, decoder_layers, decoder_layerdrop, context_length, \n",
    "                 prediction_length, d_model, decoder_attention_heads, attention_dropout, \n",
    "                 sampling_factor, dropout, activation_dropout, decoder_ffn_dim, output_attentions,\n",
    "                 output_hidden_states, use_return_dict, attention_type=\"prob\", use_cache=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = decoder_layerdrop\n",
    "        if prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = InformerValueEmbedding(feature_size=feature_size, \n",
    "                                                      d_model=d_model)\n",
    "        self.embed_positions = InformerSinusoidalPositionalEmbedding(\n",
    "            context_length + prediction_length, d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([InformerDecoderLayer(d_model, decoder_attention_heads, \n",
    "                 attention_dropout, sampling_factor, dropout, activation_dropout, decoder_ffn_dim, \n",
    "                 attention_type) for _ in range(decoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_cache = use_cache\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                device=inputs_embeds.device,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
    "                inputs_embeds.device\n",
    "            )\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
    "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
    "                selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
    "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size(), past_key_values_length=self.context_length)\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                if attn_mask.size()[0] != (len(self.layers)):\n",
    "                    raise ValueError(\n",
    "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                        f\" {head_mask.size()[0]}.\"\n",
    "                    )\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):\n",
    "                continue\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, output_attentions, use_cache)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
    "                    None,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    cross_attn_layer_head_mask=(\n",
    "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
    "                    ),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerModel(nn.Module):\n",
    "    def __init__(self, num_static_categorical_features, cardinality, embedding_dimension, \n",
    "                 feature_size, encoder_layers, encoder_layerdrop, decoder_layers, \n",
    "                 decoder_layerdrop, context_length, prediction_length, lags_sequence, input_size, \n",
    "                 d_model, encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 sampling_factor, dropout, activation_dropout, encoder_ffn_dim, decoder_ffn_dim,\n",
    "                 output_attentions, output_hidden_states, use_return_dict, use_cache, \n",
    "                 scaling = \"std\", distil=True, attention_type=\"prob\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if scaling == \"mean\" or scaling:\n",
    "            self.scaler = InformerMeanScaler(dim=1, keepdim=True)\n",
    "        elif scaling == \"std\":\n",
    "            self.scaler = InformerStdScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = InformerNOPScaler(dim=1, keepdim=True)\n",
    "\n",
    "        if num_static_categorical_features > 0:\n",
    "            self.embedder = InformerFeatureEmbedder(\n",
    "                cardinalities=cardinality,\n",
    "                embedding_dims=embedding_dimension,\n",
    "            )\n",
    "\n",
    "        # transformer encoder-decoder and mask initializer\n",
    "        self.encoder = InformerEncoder(feature_size, encoder_layers, encoder_layerdrop, \n",
    "                 context_length, prediction_length, d_model, encoder_attention_heads, \n",
    "                 attention_dropout, sampling_factor, dropout, activation_dropout, encoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict,\n",
    "                 distil, attention_type)\n",
    "        self.decoder = InformerDecoder(feature_size, decoder_layers, decoder_layerdrop, \n",
    "                 context_length, prediction_length, d_model, decoder_attention_heads, \n",
    "                 attention_dropout, sampling_factor, dropout, activation_dropout, decoder_ffn_dim,\n",
    "                 output_attentions, output_hidden_states, use_return_dict, attention_type, use_cache)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.lags_sequence = lags_sequence\n",
    "        self.input_size = input_size\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_cache = use_cache\n",
    "        self.use_return_dict = use_return_dict\n",
    "        self.W = nn.Linear(d_model, 1)\n",
    "\n",
    "    @property\n",
    "    def _past_length(self) -> int:\n",
    "        return self.context_length + max(self.lags_sequence)\n",
    "\n",
    "    def get_lagged_subsequences(\n",
    "        self, sequence: torch.Tensor, subsequences_length: int, shift: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence. Returns a tensor of shape (N, S, C, I),\n",
    "            where S = subsequences_length and I = len(indices), containing lagged subsequences. Specifically, lagged[i,\n",
    "            j, :, k] = sequence[i, -indices[k]-S+j, :].\n",
    "        Args:\n",
    "            sequence: Tensor\n",
    "                The sequence from which lagged subsequences should be extracted. Shape: (N, T, C).\n",
    "            subsequences_length : int\n",
    "                Length of the subsequences to be extracted.\n",
    "            shift: int\n",
    "                Shift the lags by this amount back.\n",
    "        \"\"\"\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = [lag - shift for lag in self.lags_sequence]\n",
    "\n",
    "        if max(indices) + subsequences_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "                f\"while history length is only {sequence_length}\"\n",
    "            )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "\n",
    "    def create_network_inputs(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        past_observed_mask: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        # time feature\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_features[:, self._past_length - self.context_length :, ...],\n",
    "                    future_time_features,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_values is not None\n",
    "            else past_time_features[:, self._past_length - self.context_length :, ...]\n",
    "        )\n",
    "\n",
    "        # target\n",
    "        if past_observed_mask is None:\n",
    "            past_observed_mask = torch.ones_like(past_values)\n",
    "\n",
    "        context = past_values[:, -self.context_length :]\n",
    "        observed_context = past_observed_mask[:, -self.context_length :]\n",
    "        _, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        inputs = (\n",
    "            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n",
    "            if future_values is not None\n",
    "            else (past_values - loc) / scale\n",
    "        )\n",
    "\n",
    "        # static features\n",
    "        log_abs_loc = loc.abs().log1p() if self.input_size == 1 else loc.squeeze(1).abs().log1p()\n",
    "        log_scale = scale.log() if self.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n",
    "\n",
    "        if static_real_features is not None:\n",
    "            static_feat = torch.cat((static_real_features, static_feat), dim=1)\n",
    "        if static_categorical_features is not None:\n",
    "            embedded_cat = self.embedder(static_categorical_features)\n",
    "            static_feat = torch.cat((embedded_cat, static_feat), dim=1)\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n",
    "\n",
    "        # all features\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        # lagged features\n",
    "        subsequences_length = (\n",
    "            self.context_length + self.prediction_length\n",
    "            if future_values is not None\n",
    "            else self.context_length\n",
    "        )\n",
    "        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n",
    "            )\n",
    "\n",
    "        # transformer inputs\n",
    "        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        return transformer_inputs, loc, scale, static_feat\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: Optional[torch.LongTensor] = None,\n",
    "        past_observed_mask: Optional[torch.LongTensor] = None,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from huggingface_hub import hf_hub_download\n",
    "        >>> import torch\n",
    "        >>> from transformers import InformerModel\n",
    "        >>> file = hf_hub_download(\n",
    "        ...     repo_id=\"kashif/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    "        ... )\n",
    "        >>> batch = torch.load(file)\n",
    "        >>> model = InformerModel.from_pretrained(\"huggingface/informer-tourism-monthly\")\n",
    "        >>> # during training, one provides both past and future values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> outputs = model(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_values=batch[\"future_values\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        batch_size, sequence_length, num_features = past_values.shape\n",
    "        past_values = past_values\n",
    "        past_time_features = torch.randn(batch_size, sequence_length, num_features)\n",
    "        past_observed_mask = torch.ones(batch_size, sequence_length, num_features)\n",
    "        static_categorical_features = torch.randint(0, 4, (batch_size, 2))\n",
    "        static_real_features = torch.randn(batch_size, num_features) # Changed this line to match the expected dimension\n",
    "        future_values = future_values\n",
    "        future_time_features = torch.randn(batch_size, 1, num_features)\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            enc_input = transformer_inputs[:, : self.context_length, ...]\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs_embeds=enc_input,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        dec_input = transformer_inputs[:, self.context_length :, ...]\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=dec_input,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        decoder_outputs = self.W(torch.sum(decoder_outputs[0],dim=1)/decoder_outputs[0].size(1))\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input tensors\n",
    "batch_size = 5\n",
    "sequence_length = 21\n",
    "num_features = 20\n",
    "\n",
    "past_values = torch.randn(batch_size, sequence_length, num_features)\n",
    "future_values = torch.randn(batch_size, 1, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InformerModel(\n",
    "    num_static_categorical_features=2,\n",
    "    cardinality=[4, 4],\n",
    "    embedding_dimension=[2, 2],\n",
    "    feature_size=144,\n",
    "    encoder_layers=2,\n",
    "    encoder_layerdrop=0,\n",
    "    decoder_layers=2,\n",
    "    decoder_layerdrop=0,\n",
    "    context_length=21,\n",
    "    prediction_length=1,\n",
    "    lags_sequence=[0, 0, 0],\n",
    "    input_size=20,\n",
    "    d_model=20,\n",
    "    encoder_attention_heads=2,\n",
    "    decoder_attention_heads=2,\n",
    "    attention_dropout=0,\n",
    "    sampling_factor=1,\n",
    "    dropout=0,\n",
    "    activation_dropout=0,\n",
    "    encoder_ffn_dim=64,\n",
    "    decoder_ffn_dim=64,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    use_return_dict=False,\n",
    "    use_cache=True,\n",
    "    scaling=\"std\",\n",
    "    distil=True,\n",
    "    attention_type=\"prob\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    past_values=past_values,\n",
    "    future_values=future_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[1.1120],\n",
      "        [0.6687],\n",
      "        [0.8442],\n",
      "        [1.2532],\n",
      "        [0.2854]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>AOAL</th>\n",
       "      <th>AOAR</th>\n",
       "      <th>PITCH</th>\n",
       "      <th>W</th>\n",
       "      <th>MACH</th>\n",
       "      <th>AIRSPD</th>\n",
       "      <th>TEFLAPL</th>\n",
       "      <th>XIDA</th>\n",
       "      <th>T</th>\n",
       "      <th>...</th>\n",
       "      <th>OPR</th>\n",
       "      <th>OTL</th>\n",
       "      <th>OTR</th>\n",
       "      <th>VIBN1L</th>\n",
       "      <th>VIBN1R</th>\n",
       "      <th>VIBN2L</th>\n",
       "      <th>VIBN2R</th>\n",
       "      <th>FE</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>FS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-10.55</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63901.05853</td>\n",
       "      <td>0.084</td>\n",
       "      <td>56.3</td>\n",
       "      <td>5</td>\n",
       "      <td>6.855</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>23995.070700</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>577.896783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-9.84</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63900.67347</td>\n",
       "      <td>0.087</td>\n",
       "      <td>58.2</td>\n",
       "      <td>5</td>\n",
       "      <td>6.325</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>23453.775200</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1119.192283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-9.14</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63900.23197</td>\n",
       "      <td>0.090</td>\n",
       "      <td>60.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.710</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22821.215600</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1751.751883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-8.09</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>63899.73805</td>\n",
       "      <td>0.093</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.630</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22738.950900</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1834.016583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>63899.14333</td>\n",
       "      <td>0.096</td>\n",
       "      <td>63.9</td>\n",
       "      <td>5</td>\n",
       "      <td>5.365</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22464.769900</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>2108.197583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>11544</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.87869</td>\n",
       "      <td>0.150</td>\n",
       "      <td>56.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.950</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7668.487126</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11095.857388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>11545</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>56378.72144</td>\n",
       "      <td>0.150</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.220</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>7393.237132</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11371.107383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>11546</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>56378.57427</td>\n",
       "      <td>0.150</td>\n",
       "      <td>49.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.130</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7484.958267</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11279.386247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>11547</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.43517</td>\n",
       "      <td>0.150</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.950</td>\n",
       "      <td>24.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7652.561590</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11111.782924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>11548</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.30816</td>\n",
       "      <td>0.150</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.125</td>\n",
       "      <td>24.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7474.749119</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11289.595395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11524 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   AOAL  AOAR  PITCH            W   MACH  AIRSPD  TEFLAPL   XIDA  \\\n",
       "0          0 -10.55 -4.22  -0.53  63901.05853  0.084    56.3        5  6.855   \n",
       "1          1  -9.84 -3.87  -0.53  63900.67347  0.087    58.2        5  6.325   \n",
       "2          2  -9.14 -3.34  -0.53  63900.23197  0.090    60.1        5  5.710   \n",
       "3          3  -8.09 -3.87  -0.35  63899.73805  0.093    62.0        5  5.630   \n",
       "4          4  -7.56 -3.87  -0.35  63899.14333  0.096    63.9        5  5.365   \n",
       "...      ...    ...   ...    ...          ...    ...     ...      ...    ...   \n",
       "11519  11544  -2.81 -3.69  -0.70  56378.87869  0.150    56.0       30  3.950   \n",
       "11520  11545  -2.99 -3.69  -0.88  56378.72144  0.150    52.0       30  4.220   \n",
       "11521  11546  -2.81 -3.69  -0.88  56378.57427  0.150    49.0       30  4.130   \n",
       "11522  11547  -2.81 -3.69  -0.70  56378.43517  0.150    46.0       30  3.950   \n",
       "11523  11548  -3.16 -3.69  -0.70  56378.30816  0.150    45.0       30  4.125   \n",
       "\n",
       "           T  ...  OPR  OTL  OTR  VIBN1L  VIBN1R  VIBN2L  VIBN2R  \\\n",
       "0      20.50  ...   24  110  110    0.03    0.05    0.05    0.05   \n",
       "1      20.50  ...   26  110  110    0.03    0.05    0.07    0.05   \n",
       "2      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "3      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "4      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "...      ...  ...  ...  ...  ...     ...     ...     ...     ...   \n",
       "11519  23.75  ...   43   98   98    0.09    0.10    0.10    0.05   \n",
       "11520  23.75  ...   31   98   98    0.10    0.10    0.12    0.07   \n",
       "11521  23.75  ...   31   98   98    0.13    0.12    0.12    0.09   \n",
       "11522  24.00  ...   31   97   97    0.13    0.15    0.18    0.09   \n",
       "11523  24.00  ...   31   97   97    0.14    0.15    0.25    0.09   \n",
       "\n",
       "                 FE          TMAX            FS  \n",
       "0      23995.070700  24572.967483    577.896783  \n",
       "1      23453.775200  24572.967483   1119.192283  \n",
       "2      22821.215600  24572.967483   1751.751883  \n",
       "3      22738.950900  24572.967483   1834.016583  \n",
       "4      22464.769900  24572.967483   2108.197583  \n",
       "...             ...           ...           ...  \n",
       "11519   7668.487126  18764.344514  11095.857388  \n",
       "11520   7393.237132  18764.344514  11371.107383  \n",
       "11521   7484.958267  18764.344514  11279.386247  \n",
       "11522   7652.561590  18764.344514  11111.782924  \n",
       "11523   7474.749119  18764.344514  11289.595395  \n",
       "\n",
       "[11524 rows x 28 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['date','AOAL','AOAR','PITCH','W','MACH','AIRSPD','TEFLAPL','XIDA',\n",
    "               'T','ALTSTD','N1L','N1R','N2L','N2R','EGTL','EGTR','OPL','OPR','OTL',\n",
    "               'OTR','VIBN1L','VIBN1R','VIBN2L','VIBN2R','FE','TMAX','FS']\n",
    "\n",
    "input=pd.read_excel('D:\\\\研究生毕设\\\\practice\\\\QAR_xunlian.xlsx',names=column_names)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['W','MACH','AIRSPD','TEFLAPL','XIDA',\n",
    "               'T','ALTSTD','N1L','N1R','N2L','N2R','EGTL','EGTR','OPL','OPR','OTL',\n",
    "               'OTR','VIBN1L','VIBN1R','VIBN2L','VIBN2R','FE']\n",
    "\n",
    "df=input[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training=df[feature_names].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11524, 22)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler=scaler.fit(df_for_training)\n",
    "df_for_training_scaled=scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=[]\n",
    "trainY=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future=1\n",
    "n_past=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_past,len(df_for_training_scaled)-n_future+1):\n",
    "    trainX.append(df_for_training_scaled[i-n_past:i,0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i:i+n_future,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape==(11504, 20, 22).\n",
      "trainY shape==(11504, 1).\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY=np.array(trainX),np.array(trainY)\n",
    "print('trainX shape=={}.'.format(trainX.shape))\n",
    "print('trainY shape=={}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX0=np.zeros((trainX.shape[0],22,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX0[i]=trainX[i].T\n",
    "trainX1=np.zeros((trainX.shape[0],21,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX1[i]=trainX0[i][0:21,:]\n",
    "trainX2=np.zeros((trainX.shape[0],1,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX2[i]=trainX0[i][21,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX1_copy,trainX2_copy,trainY_copy=trainX1.copy(),trainX2.copy(),trainY.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(data1,data2,label):\n",
    "    randnum = np.random.randint(0, len(label))\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(data1)\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(data2)\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(label)\n",
    "    return data1,data2,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1,data2,label=random_shuffle(trainX1,trainX2,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training data\n",
    "data1 = torch.from_numpy(data1).float()\n",
    "data2 = torch.from_numpy(data2).float()\n",
    "label = torch.from_numpy(label).float()\n",
    "train_dataset = TensorDataset(data1, data2, label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "min_val_acc=1000000000000000\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0 )\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.2801380157470703\n",
      "Epoch: 1 Loss: 2.1228671073913574\n",
      "Epoch: 2 Loss: 0.23718008399009705\n",
      "Epoch: 3 Loss: 0.4644332528114319\n",
      "Epoch: 4 Loss: 0.2204333394765854\n",
      "Epoch: 5 Loss: 0.2580004334449768\n",
      "Epoch: 6 Loss: 1.9792736768722534\n",
      "Epoch: 7 Loss: 0.26527783274650574\n",
      "Epoch: 8 Loss: 0.18182292580604553\n",
      "Epoch: 9 Loss: 0.34660273790359497\n",
      "Epoch: 10 Loss: 0.17544446885585785\n",
      "Epoch: 11 Loss: 0.08748054504394531\n",
      "Epoch: 12 Loss: 1.7365531921386719\n",
      "Epoch: 13 Loss: 0.20878560841083527\n",
      "Epoch: 14 Loss: 0.3315025866031647\n",
      "Epoch: 15 Loss: 1.3992900848388672\n",
      "Epoch: 16 Loss: 0.12105223536491394\n",
      "Epoch: 17 Loss: 0.10943785309791565\n",
      "Epoch: 18 Loss: 0.1955268383026123\n",
      "Epoch: 19 Loss: 0.12431227415800095\n",
      "Epoch: 20 Loss: 1.3722658157348633\n",
      "Epoch: 21 Loss: 0.4535718560218811\n",
      "Epoch: 22 Loss: 0.19576482474803925\n",
      "Epoch: 23 Loss: 0.3096347451210022\n",
      "Epoch: 24 Loss: 0.537380576133728\n",
      "Epoch: 25 Loss: 0.4687415361404419\n",
      "Epoch: 26 Loss: 0.3353405296802521\n",
      "Epoch: 27 Loss: 0.19998514652252197\n",
      "Epoch: 28 Loss: 0.21293051540851593\n",
      "Epoch: 29 Loss: 0.6215982437133789\n",
      "Epoch: 30 Loss: 0.12326651066541672\n",
      "Epoch: 31 Loss: 0.06818663328886032\n",
      "Epoch: 32 Loss: 0.4282014071941376\n",
      "Epoch: 33 Loss: 0.20816847681999207\n",
      "Epoch: 34 Loss: 0.6567174792289734\n",
      "Epoch: 35 Loss: 0.5405616164207458\n",
      "Epoch: 36 Loss: 0.18584303557872772\n",
      "Epoch: 37 Loss: 0.19342182576656342\n",
      "Epoch: 38 Loss: 0.20919187366962433\n",
      "Epoch: 39 Loss: 0.3674350380897522\n",
      "Epoch: 40 Loss: 0.2881634831428528\n",
      "Epoch: 41 Loss: 0.09692379087209702\n",
      "Epoch: 42 Loss: 2.0503792762756348\n",
      "Epoch: 43 Loss: 0.15809664130210876\n",
      "Epoch: 44 Loss: 0.6837358474731445\n",
      "Epoch: 45 Loss: 0.08199538290500641\n",
      "Epoch: 46 Loss: 0.3267999291419983\n",
      "Epoch: 47 Loss: 0.15261374413967133\n",
      "Epoch: 48 Loss: 0.11453095078468323\n",
      "Epoch: 49 Loss: 0.33424538373947144\n",
      "Epoch: 50 Loss: 0.1004873514175415\n",
      "Epoch: 51 Loss: 0.1639532893896103\n",
      "Epoch: 52 Loss: 0.11784110218286514\n",
      "Epoch: 53 Loss: 0.18626070022583008\n",
      "Epoch: 54 Loss: 0.2728728950023651\n",
      "Epoch: 55 Loss: 0.11235613375902176\n",
      "Epoch: 56 Loss: 0.2079968899488449\n",
      "Epoch: 57 Loss: 0.20139358937740326\n",
      "Epoch: 58 Loss: 1.109454870223999\n",
      "Epoch: 59 Loss: 0.18678691983222961\n",
      "Epoch: 60 Loss: 0.2872418761253357\n",
      "Epoch: 61 Loss: 0.09910811483860016\n",
      "Epoch: 62 Loss: 0.3679933547973633\n",
      "Epoch: 63 Loss: 0.11780562996864319\n",
      "Epoch: 64 Loss: 0.2695733308792114\n",
      "Epoch: 65 Loss: 0.1045123040676117\n",
      "Epoch: 66 Loss: 0.22670334577560425\n",
      "Epoch: 67 Loss: 0.22968129813671112\n",
      "Epoch: 68 Loss: 0.1595059037208557\n",
      "Epoch: 69 Loss: 0.1467505693435669\n",
      "Epoch: 70 Loss: 0.1485203504562378\n",
      "Epoch: 71 Loss: 0.14688189327716827\n",
      "Epoch: 72 Loss: 0.142580047249794\n",
      "Epoch: 73 Loss: 0.19932076334953308\n",
      "Epoch: 74 Loss: 0.15613137185573578\n",
      "Epoch: 75 Loss: 0.2993990480899811\n",
      "Epoch: 76 Loss: 0.3320175111293793\n",
      "Epoch: 77 Loss: 0.34464386105537415\n",
      "Epoch: 78 Loss: 0.05070090666413307\n",
      "Epoch: 79 Loss: 0.053980328142642975\n",
      "Epoch: 80 Loss: 0.07393939048051834\n",
      "Epoch: 81 Loss: 0.5972595810890198\n",
      "Epoch: 82 Loss: 0.09804188460111618\n",
      "Epoch: 83 Loss: 0.8907860517501831\n",
      "Epoch: 84 Loss: 0.4030831456184387\n",
      "Epoch: 85 Loss: 0.060048658400774\n",
      "Epoch: 86 Loss: 0.18493801355361938\n",
      "Epoch: 87 Loss: 0.19825340807437897\n",
      "Epoch: 88 Loss: 0.17147698998451233\n",
      "Epoch: 89 Loss: 0.22996054589748383\n",
      "Epoch: 90 Loss: 0.327576220035553\n",
      "Epoch: 91 Loss: 0.2357231229543686\n",
      "Epoch: 92 Loss: 0.19105124473571777\n",
      "Epoch: 93 Loss: 0.10743000358343124\n",
      "Epoch: 94 Loss: 0.13065803050994873\n",
      "Epoch: 95 Loss: 0.13566811382770538\n",
      "Epoch: 96 Loss: 0.08433864265680313\n",
      "Epoch: 97 Loss: 0.19888019561767578\n",
      "Epoch: 98 Loss: 0.12005544453859329\n",
      "Epoch: 99 Loss: 0.11773499846458435\n",
      "Epoch: 100 Loss: 0.4554210603237152\n",
      "Epoch: 101 Loss: 0.09278149157762527\n",
      "Epoch: 102 Loss: 0.3006971478462219\n",
      "Epoch: 103 Loss: 0.16905607283115387\n",
      "Epoch: 104 Loss: 0.22777923941612244\n",
      "Epoch: 105 Loss: 0.1765049397945404\n",
      "Epoch: 106 Loss: 0.18374434113502502\n",
      "Epoch: 107 Loss: 0.14246797561645508\n",
      "Epoch: 108 Loss: 0.08642266690731049\n",
      "Epoch: 109 Loss: 0.10269060730934143\n",
      "Epoch: 110 Loss: 0.22402001917362213\n",
      "Epoch: 111 Loss: 0.18501968681812286\n",
      "Epoch: 112 Loss: 0.411363810300827\n",
      "Epoch: 113 Loss: 0.10378636419773102\n",
      "Epoch: 114 Loss: 0.156958669424057\n",
      "Epoch: 115 Loss: 0.12836311757564545\n",
      "Epoch: 116 Loss: 0.17110231518745422\n",
      "Epoch: 117 Loss: 0.22938089072704315\n",
      "Epoch: 118 Loss: 0.2446657419204712\n",
      "Epoch: 119 Loss: 0.13429966568946838\n",
      "Epoch: 120 Loss: 0.24516171216964722\n",
      "Epoch: 121 Loss: 0.2889130115509033\n",
      "Epoch: 122 Loss: 0.14118874073028564\n",
      "Epoch: 123 Loss: 0.1893734335899353\n",
      "Epoch: 124 Loss: 0.18958744406700134\n",
      "Epoch: 125 Loss: 0.29479697346687317\n",
      "Epoch: 126 Loss: 0.3890875279903412\n",
      "Epoch: 127 Loss: 0.06262210756540298\n",
      "Epoch: 128 Loss: 0.3859214186668396\n",
      "Epoch: 129 Loss: 0.33555370569229126\n",
      "Epoch: 130 Loss: 0.19868215918540955\n",
      "Epoch: 131 Loss: 0.23347102105617523\n",
      "Epoch: 132 Loss: 0.16256234049797058\n",
      "Epoch: 133 Loss: 0.18182405829429626\n",
      "Epoch: 134 Loss: 0.13591885566711426\n",
      "Epoch: 135 Loss: 0.14825603365898132\n",
      "Epoch: 136 Loss: 0.15107397735118866\n",
      "Epoch: 137 Loss: 0.060574766248464584\n",
      "Epoch: 138 Loss: 0.12775960564613342\n",
      "Epoch: 139 Loss: 0.13701745867729187\n",
      "Epoch: 140 Loss: 0.1492256224155426\n",
      "Epoch: 141 Loss: 0.22127768397331238\n",
      "Epoch: 142 Loss: 0.11841762810945511\n",
      "Epoch: 143 Loss: 0.13251516222953796\n",
      "Epoch: 144 Loss: 0.15528075397014618\n",
      "Epoch: 145 Loss: 0.24969691038131714\n",
      "Epoch: 146 Loss: 0.043464746326208115\n",
      "Epoch: 147 Loss: 0.1250392496585846\n",
      "Epoch: 148 Loss: 0.09475637972354889\n",
      "Epoch: 149 Loss: 0.09063885360956192\n",
      "Epoch: 150 Loss: 0.28973978757858276\n",
      "Epoch: 151 Loss: 0.14416591823101044\n",
      "Epoch: 152 Loss: 0.08214718103408813\n",
      "Epoch: 153 Loss: 0.07958028465509415\n",
      "Epoch: 154 Loss: 0.34611913561820984\n",
      "Epoch: 155 Loss: 0.10227470844984055\n",
      "Epoch: 156 Loss: 0.16047051548957825\n",
      "Epoch: 157 Loss: 0.12498356401920319\n",
      "Epoch: 158 Loss: 0.07918432354927063\n",
      "Epoch: 159 Loss: 0.10613106191158295\n",
      "Epoch: 160 Loss: 0.1202971562743187\n",
      "Epoch: 161 Loss: 0.279163122177124\n",
      "Epoch: 162 Loss: 0.0886499434709549\n",
      "Epoch: 163 Loss: 0.07819519191980362\n",
      "Epoch: 164 Loss: 0.0983491837978363\n",
      "Epoch: 165 Loss: 0.0720594972372055\n",
      "Epoch: 166 Loss: 0.07771255075931549\n",
      "Epoch: 167 Loss: 0.07425394654273987\n",
      "Epoch: 168 Loss: 0.1904272884130478\n",
      "Epoch: 169 Loss: 0.19195953011512756\n",
      "Epoch: 170 Loss: 0.3684725761413574\n",
      "Epoch: 171 Loss: 0.22094836831092834\n",
      "Epoch: 172 Loss: 0.23834489285945892\n",
      "Epoch: 173 Loss: 0.11742894351482391\n",
      "Epoch: 174 Loss: 0.08762431144714355\n",
      "Epoch: 175 Loss: 0.16531673073768616\n",
      "Epoch: 176 Loss: 0.20357148349285126\n",
      "Epoch: 177 Loss: 0.28846144676208496\n",
      "Epoch: 178 Loss: 0.2067519724369049\n",
      "Epoch: 179 Loss: 0.1401638686656952\n",
      "Epoch: 180 Loss: 0.20948195457458496\n",
      "Epoch: 181 Loss: 0.13153548538684845\n",
      "Epoch: 182 Loss: 0.1100335344672203\n",
      "Epoch: 183 Loss: 0.0662897378206253\n",
      "Epoch: 184 Loss: 0.13202950358390808\n",
      "Epoch: 185 Loss: 0.2606481611728668\n",
      "Epoch: 186 Loss: 0.0665692612528801\n",
      "Epoch: 187 Loss: 0.07330943644046783\n",
      "Epoch: 188 Loss: 0.17899751663208008\n",
      "Epoch: 189 Loss: 0.08866000920534134\n",
      "Epoch: 190 Loss: 0.1319715678691864\n",
      "Epoch: 191 Loss: 0.08701515197753906\n",
      "Epoch: 192 Loss: 0.07064986228942871\n",
      "Epoch: 193 Loss: 0.07652757316827774\n",
      "Epoch: 194 Loss: 0.16674834489822388\n",
      "Epoch: 195 Loss: 0.1138671413064003\n",
      "Epoch: 196 Loss: 0.19940616190433502\n",
      "Epoch: 197 Loss: 0.0974470004439354\n",
      "Epoch: 198 Loss: 0.10728981345891953\n",
      "Epoch: 199 Loss: 0.05622177943587303\n",
      "Epoch: 200 Loss: 0.12290976196527481\n",
      "Epoch: 201 Loss: 0.18634231388568878\n",
      "Epoch: 202 Loss: 0.0650719478726387\n",
      "Epoch: 203 Loss: 0.36906808614730835\n",
      "Epoch: 204 Loss: 0.08458514511585236\n",
      "Epoch: 205 Loss: 0.22148942947387695\n",
      "Epoch: 206 Loss: 0.09781806915998459\n",
      "Epoch: 207 Loss: 0.19014385342597961\n",
      "Epoch: 208 Loss: 0.2716393768787384\n",
      "Epoch: 209 Loss: 0.10820179432630539\n",
      "Epoch: 210 Loss: 0.10849408805370331\n",
      "Epoch: 211 Loss: 0.2432352900505066\n",
      "Epoch: 212 Loss: 0.11669565737247467\n",
      "Epoch: 213 Loss: 0.12844134867191315\n",
      "Epoch: 214 Loss: 0.08101857453584671\n",
      "Epoch: 215 Loss: 0.1491674780845642\n",
      "Epoch: 216 Loss: 0.1052827388048172\n",
      "Epoch: 217 Loss: 0.10362136363983154\n",
      "Epoch: 218 Loss: 0.10073908418416977\n",
      "Epoch: 219 Loss: 0.10782386362552643\n",
      "Epoch: 220 Loss: 0.08627217262983322\n",
      "Epoch: 221 Loss: 0.19812536239624023\n",
      "Epoch: 222 Loss: 0.07774421572685242\n",
      "Epoch: 223 Loss: 0.23661917448043823\n",
      "Epoch: 224 Loss: 0.12438428401947021\n",
      "Epoch: 225 Loss: 0.07932040095329285\n",
      "Epoch: 226 Loss: 0.18562385439872742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227 Loss: 0.28057369589805603\n",
      "Epoch: 228 Loss: 0.2074487805366516\n",
      "Epoch: 229 Loss: 0.20149467885494232\n",
      "Epoch: 230 Loss: 0.08136511594057083\n",
      "Epoch: 231 Loss: 0.08845355361700058\n",
      "Epoch: 232 Loss: 0.1421295404434204\n",
      "Epoch: 233 Loss: 0.19217924773693085\n",
      "Epoch: 234 Loss: 0.07118053734302521\n",
      "Epoch: 235 Loss: 0.0877336710691452\n",
      "Epoch: 236 Loss: 0.10358746349811554\n",
      "Epoch: 237 Loss: 0.2936745584011078\n",
      "Epoch: 238 Loss: 0.04786837473511696\n",
      "Epoch: 239 Loss: 0.05086959898471832\n",
      "Epoch: 240 Loss: 0.06814434379339218\n",
      "Epoch: 241 Loss: 0.17592725157737732\n",
      "Epoch: 242 Loss: 0.055316634476184845\n",
      "Epoch: 243 Loss: 0.3670330345630646\n",
      "Epoch: 244 Loss: 0.14188405871391296\n",
      "Epoch: 245 Loss: 0.13256531953811646\n",
      "Epoch: 246 Loss: 0.11155229806900024\n",
      "Epoch: 247 Loss: 0.0893586054444313\n",
      "Epoch: 248 Loss: 0.16340452432632446\n",
      "Epoch: 249 Loss: 0.1400436908006668\n",
      "Epoch: 250 Loss: 0.16963759064674377\n",
      "Epoch: 251 Loss: 0.08567873388528824\n",
      "Epoch: 252 Loss: 0.17268089950084686\n",
      "Epoch: 253 Loss: 0.17530936002731323\n",
      "Epoch: 254 Loss: 0.24360983073711395\n",
      "Epoch: 255 Loss: 0.1200484037399292\n",
      "Epoch: 256 Loss: 0.11248712241649628\n",
      "Epoch: 257 Loss: 0.34456345438957214\n",
      "Epoch: 258 Loss: 0.0851295068860054\n",
      "Epoch: 259 Loss: 0.06433586776256561\n",
      "Epoch: 260 Loss: 0.10839689522981644\n",
      "Epoch: 261 Loss: 0.12795567512512207\n",
      "Epoch: 262 Loss: 0.07774641364812851\n",
      "Epoch: 263 Loss: 0.19738538563251495\n",
      "Epoch: 264 Loss: 0.18414181470870972\n",
      "Epoch: 265 Loss: 0.3577326238155365\n",
      "Epoch: 266 Loss: 0.12228631228208542\n",
      "Epoch: 267 Loss: 0.15572457015514374\n",
      "Epoch: 268 Loss: 0.06098538264632225\n",
      "Epoch: 269 Loss: 0.0686866044998169\n",
      "Epoch: 270 Loss: 0.07000847160816193\n",
      "Epoch: 271 Loss: 0.09522517025470734\n",
      "Epoch: 272 Loss: 0.1656758040189743\n",
      "Epoch: 273 Loss: 0.09193096309900284\n",
      "Epoch: 274 Loss: 0.17557881772518158\n",
      "Epoch: 275 Loss: 0.0648791640996933\n",
      "Epoch: 276 Loss: 0.1896762251853943\n",
      "Epoch: 277 Loss: 0.19500164687633514\n",
      "Epoch: 278 Loss: 0.11603444069623947\n",
      "Epoch: 279 Loss: 0.10099813342094421\n",
      "Epoch: 280 Loss: 0.25204989314079285\n",
      "Epoch: 281 Loss: 0.10375940799713135\n",
      "Epoch: 282 Loss: 0.10218903422355652\n",
      "Epoch: 283 Loss: 0.1779661923646927\n",
      "Epoch: 284 Loss: 0.18081223964691162\n",
      "Epoch: 285 Loss: 0.2510407269001007\n",
      "Epoch: 286 Loss: 0.1550491452217102\n",
      "Epoch: 287 Loss: 0.1413724571466446\n",
      "Epoch: 288 Loss: 0.07236205041408539\n",
      "Epoch: 289 Loss: 0.10744576156139374\n",
      "Epoch: 290 Loss: 0.10080069303512573\n",
      "Epoch: 291 Loss: 0.07291855663061142\n",
      "Epoch: 292 Loss: 0.210198312997818\n",
      "Epoch: 293 Loss: 0.09234615415334702\n",
      "Epoch: 294 Loss: 0.1493859738111496\n",
      "Epoch: 295 Loss: 0.1053740531206131\n",
      "Epoch: 296 Loss: 0.10320219397544861\n",
      "Epoch: 297 Loss: 0.0438569076359272\n",
      "Epoch: 298 Loss: 0.13203878700733185\n",
      "Epoch: 299 Loss: 0.142029270529747\n",
      "Epoch: 300 Loss: 0.23401087522506714\n",
      "Epoch: 301 Loss: 0.20465046167373657\n",
      "Epoch: 302 Loss: 0.1560758352279663\n",
      "Epoch: 303 Loss: 0.06931468844413757\n",
      "Epoch: 304 Loss: 0.19567660987377167\n",
      "Epoch: 305 Loss: 0.11978837847709656\n",
      "Epoch: 306 Loss: 0.12520737946033478\n",
      "Epoch: 307 Loss: 0.1363866776227951\n",
      "Epoch: 308 Loss: 0.13199587166309357\n",
      "Epoch: 309 Loss: 0.10295718163251877\n",
      "Epoch: 310 Loss: 0.16552799940109253\n",
      "Epoch: 311 Loss: 0.0505962073802948\n",
      "Epoch: 312 Loss: 0.25262200832366943\n",
      "Epoch: 313 Loss: 0.11524128913879395\n",
      "Epoch: 314 Loss: 0.2036709487438202\n",
      "Epoch: 315 Loss: 0.13683293759822845\n",
      "Epoch: 316 Loss: 0.23110969364643097\n",
      "Epoch: 317 Loss: 0.04966263100504875\n",
      "Epoch: 318 Loss: 0.040730129927396774\n",
      "Epoch: 319 Loss: 0.12116090208292007\n",
      "Epoch: 320 Loss: 0.1990479975938797\n",
      "Epoch: 321 Loss: 0.09303905814886093\n",
      "Epoch: 322 Loss: 0.23357205092906952\n",
      "Epoch: 323 Loss: 0.04379615560173988\n",
      "Epoch: 324 Loss: 0.2097322642803192\n",
      "Epoch: 325 Loss: 0.09821426123380661\n",
      "Epoch: 326 Loss: 0.34615403413772583\n",
      "Epoch: 327 Loss: 0.16006968915462494\n",
      "Epoch: 328 Loss: 0.16113927960395813\n",
      "Epoch: 329 Loss: 0.06942635774612427\n",
      "Epoch: 330 Loss: 0.08644077181816101\n",
      "Epoch: 331 Loss: 0.13363830745220184\n",
      "Epoch: 332 Loss: 0.08921296149492264\n",
      "Epoch: 333 Loss: 0.13035020232200623\n",
      "Epoch: 334 Loss: 0.1188112422823906\n",
      "Epoch: 335 Loss: 0.06296054273843765\n",
      "Epoch: 336 Loss: 0.3025791049003601\n",
      "Epoch: 337 Loss: 0.08357198536396027\n",
      "Epoch: 338 Loss: 0.08330714702606201\n",
      "Epoch: 339 Loss: 0.1613546907901764\n",
      "Epoch: 340 Loss: 0.05169716477394104\n",
      "Epoch: 341 Loss: 0.17257124185562134\n",
      "Epoch: 342 Loss: 0.2316780537366867\n",
      "Epoch: 343 Loss: 0.09588605910539627\n",
      "Epoch: 344 Loss: 0.060558613389730453\n",
      "Epoch: 345 Loss: 0.19897869229316711\n",
      "Epoch: 346 Loss: 0.08401841670274734\n",
      "Epoch: 347 Loss: 0.1395489126443863\n",
      "Epoch: 348 Loss: 0.050568535923957825\n",
      "Epoch: 349 Loss: 0.15044187009334564\n",
      "Epoch: 350 Loss: 0.09668105095624924\n",
      "Epoch: 351 Loss: 0.1866273432970047\n",
      "Epoch: 352 Loss: 0.1477373093366623\n",
      "Epoch: 353 Loss: 0.07302756607532501\n",
      "Epoch: 354 Loss: 0.08400187641382217\n",
      "Epoch: 355 Loss: 0.12397068738937378\n",
      "Epoch: 356 Loss: 0.12100457400083542\n",
      "Epoch: 357 Loss: 0.12753166258335114\n",
      "Epoch: 358 Loss: 0.11727270483970642\n",
      "Epoch: 359 Loss: 0.21283650398254395\n",
      "Epoch: 360 Loss: 0.11269427835941315\n",
      "Epoch: 361 Loss: 0.11042025685310364\n",
      "Epoch: 362 Loss: 0.151365265250206\n",
      "Epoch: 363 Loss: 0.06692374497652054\n",
      "Epoch: 364 Loss: 0.10128858685493469\n",
      "Epoch: 365 Loss: 0.0541214682161808\n",
      "Epoch: 366 Loss: 0.06085328385233879\n",
      "Epoch: 367 Loss: 0.04649750143289566\n",
      "Epoch: 368 Loss: 0.10056056827306747\n",
      "Epoch: 369 Loss: 0.06290253251791\n",
      "Epoch: 370 Loss: 0.20633579790592194\n",
      "Epoch: 371 Loss: 0.10433334112167358\n",
      "Epoch: 372 Loss: 0.23013918101787567\n",
      "Epoch: 373 Loss: 0.13963794708251953\n",
      "Epoch: 374 Loss: 0.054337069392204285\n",
      "Epoch: 375 Loss: 0.09342678636312485\n",
      "Epoch: 376 Loss: 0.0804775133728981\n",
      "Epoch: 377 Loss: 0.12886525690555573\n",
      "Epoch: 378 Loss: 0.28006771206855774\n",
      "Epoch: 379 Loss: 0.15030327439308167\n",
      "Epoch: 380 Loss: 0.08474790304899216\n",
      "Epoch: 381 Loss: 0.03700625151395798\n",
      "Epoch: 382 Loss: 0.20553624629974365\n",
      "Epoch: 383 Loss: 0.1350378394126892\n",
      "Epoch: 384 Loss: 0.14379821717739105\n",
      "Epoch: 385 Loss: 0.11112257093191147\n",
      "Epoch: 386 Loss: 0.2840133309364319\n",
      "Epoch: 387 Loss: 0.12239587306976318\n",
      "Epoch: 388 Loss: 0.05940614268183708\n",
      "Epoch: 389 Loss: 0.11630530655384064\n",
      "Epoch: 390 Loss: 0.07611089199781418\n",
      "Epoch: 391 Loss: 0.15072400867938995\n",
      "Epoch: 392 Loss: 0.07705401629209518\n",
      "Epoch: 393 Loss: 0.07663556933403015\n",
      "Epoch: 394 Loss: 0.041103508323431015\n",
      "Epoch: 395 Loss: 0.2362317442893982\n",
      "Epoch: 396 Loss: 0.2557084560394287\n",
      "Epoch: 397 Loss: 0.055055782198905945\n",
      "Epoch: 398 Loss: 0.0932820737361908\n",
      "Epoch: 399 Loss: 0.1931595504283905\n",
      "Epoch: 400 Loss: 0.19181644916534424\n",
      "Epoch: 401 Loss: 0.04849730432033539\n",
      "Epoch: 402 Loss: 0.174096018075943\n",
      "Epoch: 403 Loss: 0.08545960485935211\n",
      "Epoch: 404 Loss: 0.09907737374305725\n",
      "Epoch: 405 Loss: 0.2079983651638031\n",
      "Epoch: 406 Loss: 0.07285580784082413\n",
      "Epoch: 407 Loss: 0.09869737923145294\n",
      "Epoch: 408 Loss: 0.0766666829586029\n",
      "Epoch: 409 Loss: 0.21598897874355316\n",
      "Epoch: 410 Loss: 0.05989276245236397\n",
      "Epoch: 411 Loss: 0.14754526317119598\n",
      "Epoch: 412 Loss: 0.058407261967659\n",
      "Epoch: 413 Loss: 0.22274509072303772\n",
      "Epoch: 414 Loss: 0.199134960770607\n",
      "Epoch: 415 Loss: 0.09722746163606644\n",
      "Epoch: 416 Loss: 0.18043595552444458\n",
      "Epoch: 417 Loss: 0.27745670080184937\n",
      "Epoch: 418 Loss: 0.0967429131269455\n",
      "Epoch: 419 Loss: 0.11435844749212265\n",
      "Epoch: 420 Loss: 0.17531664669513702\n",
      "Epoch: 421 Loss: 0.19588811695575714\n",
      "Epoch: 422 Loss: 0.0784340649843216\n",
      "Epoch: 423 Loss: 0.13356679677963257\n",
      "Epoch: 424 Loss: 0.062187664210796356\n",
      "Epoch: 425 Loss: 0.14579859375953674\n",
      "Epoch: 426 Loss: 0.2949848175048828\n",
      "Epoch: 427 Loss: 0.04760217294096947\n",
      "Epoch: 428 Loss: 0.18076948821544647\n",
      "Epoch: 429 Loss: 0.06059611216187477\n",
      "Epoch: 430 Loss: 0.2840000092983246\n",
      "Epoch: 431 Loss: 0.2551812529563904\n",
      "Epoch: 432 Loss: 0.06506520509719849\n",
      "Epoch: 433 Loss: 0.05589491128921509\n",
      "Epoch: 434 Loss: 0.1994694024324417\n",
      "Epoch: 435 Loss: 0.14608784019947052\n",
      "Epoch: 436 Loss: 0.17072948813438416\n",
      "Epoch: 437 Loss: 0.2734660506248474\n",
      "Epoch: 438 Loss: 0.07288002967834473\n",
      "Epoch: 439 Loss: 0.07250923663377762\n",
      "Epoch: 440 Loss: 0.088519386947155\n",
      "Epoch: 441 Loss: 0.10436706244945526\n",
      "Epoch: 442 Loss: 0.14098450541496277\n",
      "Epoch: 443 Loss: 0.11932407319545746\n",
      "Epoch: 444 Loss: 0.08213502168655396\n",
      "Epoch: 445 Loss: 0.0781746581196785\n",
      "Epoch: 446 Loss: 0.05717374384403229\n",
      "Epoch: 447 Loss: 0.05627922713756561\n",
      "Epoch: 448 Loss: 0.09766862541437149\n",
      "Epoch: 449 Loss: 0.13038265705108643\n",
      "Epoch: 450 Loss: 0.07575366646051407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 451 Loss: 0.1595635563135147\n",
      "Epoch: 452 Loss: 0.07947810739278793\n",
      "Epoch: 453 Loss: 0.10426905751228333\n",
      "Epoch: 454 Loss: 0.23618990182876587\n",
      "Epoch: 455 Loss: 0.07406548410654068\n",
      "Epoch: 456 Loss: 0.19252924621105194\n",
      "Epoch: 457 Loss: 0.1055920198559761\n",
      "Epoch: 458 Loss: 0.09256146103143692\n",
      "Epoch: 459 Loss: 0.11779829114675522\n",
      "Epoch: 460 Loss: 0.1356886774301529\n",
      "Epoch: 461 Loss: 0.08849912881851196\n",
      "Epoch: 462 Loss: 0.06078192591667175\n",
      "Epoch: 463 Loss: 0.09965504705905914\n",
      "Epoch: 464 Loss: 0.09465855360031128\n",
      "Epoch: 465 Loss: 0.11092406511306763\n",
      "Epoch: 466 Loss: 0.0788012221455574\n",
      "Epoch: 467 Loss: 0.16230234503746033\n",
      "Epoch: 468 Loss: 0.05759066343307495\n",
      "Epoch: 469 Loss: 0.11019162833690643\n",
      "Epoch: 470 Loss: 0.1318105310201645\n",
      "Epoch: 471 Loss: 0.06413566321134567\n",
      "Epoch: 472 Loss: 0.1471584290266037\n",
      "Epoch: 473 Loss: 0.05312085524201393\n",
      "Epoch: 474 Loss: 0.1802385002374649\n",
      "Epoch: 475 Loss: 0.08892115950584412\n",
      "Epoch: 476 Loss: 0.05664747208356857\n",
      "Epoch: 477 Loss: 0.3654049336910248\n",
      "Epoch: 478 Loss: 0.07457079738378525\n",
      "Epoch: 479 Loss: 0.19104471802711487\n",
      "Epoch: 480 Loss: 0.06359273195266724\n",
      "Epoch: 481 Loss: 0.2273140698671341\n",
      "Epoch: 482 Loss: 0.07322324812412262\n",
      "Epoch: 483 Loss: 0.14757750928401947\n",
      "Epoch: 484 Loss: 0.11546266078948975\n",
      "Epoch: 485 Loss: 0.051288820803165436\n",
      "Epoch: 486 Loss: 0.2550300061702728\n",
      "Epoch: 487 Loss: 0.07945854961872101\n",
      "Epoch: 488 Loss: 0.08021571487188339\n",
      "Epoch: 489 Loss: 0.18910396099090576\n",
      "Epoch: 490 Loss: 0.1719173789024353\n",
      "Epoch: 491 Loss: 0.20759598910808563\n",
      "Epoch: 492 Loss: 0.0850958377122879\n",
      "Epoch: 493 Loss: 0.14239850640296936\n",
      "Epoch: 494 Loss: 0.05958309397101402\n",
      "Epoch: 495 Loss: 0.14371325075626373\n",
      "Epoch: 496 Loss: 0.050122421234846115\n",
      "Epoch: 497 Loss: 0.07406483590602875\n",
      "Epoch: 498 Loss: 0.1108097955584526\n",
      "Epoch: 499 Loss: 0.1271120011806488\n",
      "Epoch: 500 Loss: 0.1913660168647766\n",
      "Epoch: 501 Loss: 0.04795901104807854\n",
      "Epoch: 502 Loss: 0.061184950172901154\n",
      "Epoch: 503 Loss: 0.25815242528915405\n",
      "Epoch: 504 Loss: 0.16350021958351135\n",
      "Epoch: 505 Loss: 0.06154846027493477\n",
      "Epoch: 506 Loss: 0.16899007558822632\n",
      "Epoch: 507 Loss: 0.11313832551240921\n",
      "Epoch: 508 Loss: 0.08938561379909515\n",
      "Epoch: 509 Loss: 0.07629840075969696\n",
      "Epoch: 510 Loss: 0.2156442552804947\n",
      "Epoch: 511 Loss: 0.1711619794368744\n",
      "Epoch: 512 Loss: 0.11321333050727844\n",
      "Epoch: 513 Loss: 0.04303412139415741\n",
      "Epoch: 514 Loss: 0.06369137763977051\n",
      "Epoch: 515 Loss: 0.16218827664852142\n",
      "Epoch: 516 Loss: 0.0964011549949646\n",
      "Epoch: 517 Loss: 0.06303196400403976\n",
      "Epoch: 518 Loss: 0.1664835810661316\n",
      "Epoch: 519 Loss: 0.1096593514084816\n",
      "Epoch: 520 Loss: 0.17590568959712982\n",
      "Epoch: 521 Loss: 0.10511782765388489\n",
      "Epoch: 522 Loss: 0.10963991284370422\n",
      "Epoch: 523 Loss: 0.1625366508960724\n",
      "Epoch: 524 Loss: 0.0567302480340004\n",
      "Epoch: 525 Loss: 0.06367134302854538\n",
      "Epoch: 526 Loss: 0.05677630752325058\n",
      "Epoch: 527 Loss: 0.11336594820022583\n",
      "Epoch: 528 Loss: 0.10119567811489105\n",
      "Epoch: 529 Loss: 0.08948300033807755\n",
      "Epoch: 530 Loss: 0.050234295427799225\n",
      "Epoch: 531 Loss: 0.11822085827589035\n",
      "Epoch: 532 Loss: 0.2512461543083191\n",
      "Epoch: 533 Loss: 0.09201810508966446\n",
      "Epoch: 534 Loss: 0.19713430106639862\n",
      "Epoch: 535 Loss: 0.07771920412778854\n",
      "Epoch: 536 Loss: 0.054023243486881256\n",
      "Epoch: 537 Loss: 0.15905605256557465\n",
      "Epoch: 538 Loss: 0.09715382754802704\n",
      "Epoch: 539 Loss: 0.2696971297264099\n",
      "Epoch: 540 Loss: 0.1476311981678009\n",
      "Epoch: 541 Loss: 0.050287600606679916\n",
      "Epoch: 542 Loss: 0.34411823749542236\n",
      "Epoch: 543 Loss: 0.08896100521087646\n",
      "Epoch: 544 Loss: 0.05170124024152756\n",
      "Epoch: 545 Loss: 0.12316833436489105\n",
      "Epoch: 546 Loss: 0.058125127106904984\n",
      "Epoch: 547 Loss: 0.08692319691181183\n",
      "Epoch: 548 Loss: 0.14048035442829132\n",
      "Epoch: 549 Loss: 0.054856572300195694\n",
      "Epoch: 550 Loss: 0.04763271287083626\n",
      "Epoch: 551 Loss: 0.19560012221336365\n",
      "Epoch: 552 Loss: 0.05951490253210068\n",
      "Epoch: 553 Loss: 0.10075301676988602\n",
      "Epoch: 554 Loss: 0.5395761132240295\n",
      "Epoch: 555 Loss: 0.1728440225124359\n",
      "Epoch: 556 Loss: 0.07817063480615616\n",
      "Epoch: 557 Loss: 0.11069700121879578\n",
      "Epoch: 558 Loss: 0.13907262682914734\n",
      "Epoch: 559 Loss: 0.038206782191991806\n",
      "Epoch: 560 Loss: 0.14757168292999268\n",
      "Epoch: 561 Loss: 0.12912817299365997\n",
      "Epoch: 562 Loss: 0.2427586317062378\n",
      "Epoch: 563 Loss: 0.067125603556633\n",
      "Epoch: 564 Loss: 0.17099471390247345\n",
      "Epoch: 565 Loss: 0.07827524095773697\n",
      "Epoch: 566 Loss: 0.05019602179527283\n",
      "Epoch: 567 Loss: 0.16646096110343933\n",
      "Epoch: 568 Loss: 0.159830242395401\n",
      "Epoch: 569 Loss: 0.05108169838786125\n",
      "Epoch: 570 Loss: 0.08887951076030731\n",
      "Epoch: 571 Loss: 0.13510462641716003\n",
      "Epoch: 572 Loss: 0.1042933315038681\n",
      "Epoch: 573 Loss: 0.16349734365940094\n",
      "Epoch: 574 Loss: 0.13762901723384857\n",
      "Epoch: 575 Loss: 0.19280000030994415\n",
      "Epoch: 576 Loss: 0.10312606394290924\n",
      "Epoch: 577 Loss: 0.07714054733514786\n",
      "Epoch: 578 Loss: 0.07909274101257324\n",
      "Epoch: 579 Loss: 0.1573602259159088\n",
      "Epoch: 580 Loss: 0.17768841981887817\n",
      "Epoch: 581 Loss: 0.13260479271411896\n",
      "Epoch: 582 Loss: 0.1322256177663803\n",
      "Epoch: 583 Loss: 0.09596937894821167\n",
      "Epoch: 584 Loss: 0.061886828392744064\n",
      "Epoch: 585 Loss: 0.15740959346294403\n",
      "Epoch: 586 Loss: 0.10558585822582245\n",
      "Epoch: 587 Loss: 0.11027564108371735\n",
      "Epoch: 588 Loss: 0.06025193631649017\n",
      "Epoch: 589 Loss: 0.07812311500310898\n",
      "Epoch: 590 Loss: 0.09977001696825027\n",
      "Epoch: 591 Loss: 0.13292497396469116\n",
      "Epoch: 592 Loss: 0.061735525727272034\n",
      "Epoch: 593 Loss: 0.16702795028686523\n",
      "Epoch: 594 Loss: 0.055117081850767136\n",
      "Epoch: 595 Loss: 0.04939844459295273\n",
      "Epoch: 596 Loss: 0.07856297492980957\n",
      "Epoch: 597 Loss: 0.12680910527706146\n",
      "Epoch: 598 Loss: 0.07430055737495422\n",
      "Epoch: 599 Loss: 0.17114496231079102\n",
      "Epoch: 600 Loss: 0.06435490399599075\n",
      "Epoch: 601 Loss: 0.0778890997171402\n",
      "Epoch: 602 Loss: 0.14087992906570435\n",
      "Epoch: 603 Loss: 0.18535852432250977\n",
      "Epoch: 604 Loss: 0.04634534567594528\n",
      "Epoch: 605 Loss: 0.03617248311638832\n",
      "Epoch: 606 Loss: 0.2406264692544937\n",
      "Epoch: 607 Loss: 0.07676202058792114\n",
      "Epoch: 608 Loss: 0.0767698660492897\n",
      "Epoch: 609 Loss: 0.04407178610563278\n",
      "Epoch: 610 Loss: 0.05395309999585152\n",
      "Epoch: 611 Loss: 0.17541223764419556\n",
      "Epoch: 612 Loss: 0.08166562765836716\n",
      "Epoch: 613 Loss: 0.07509402185678482\n",
      "Epoch: 614 Loss: 0.06989828497171402\n",
      "Epoch: 615 Loss: 0.15503914654254913\n",
      "Epoch: 616 Loss: 0.09907563030719757\n",
      "Epoch: 617 Loss: 0.2278081476688385\n",
      "Epoch: 618 Loss: 0.2699294686317444\n",
      "Epoch: 619 Loss: 0.15163519978523254\n",
      "Epoch: 620 Loss: 0.0994725450873375\n",
      "Epoch: 621 Loss: 0.08297570794820786\n",
      "Epoch: 622 Loss: 0.2662946283817291\n",
      "Epoch: 623 Loss: 0.10877891629934311\n",
      "Epoch: 624 Loss: 0.03548827022314072\n",
      "Epoch: 625 Loss: 0.04522279277443886\n",
      "Epoch: 626 Loss: 0.14640292525291443\n",
      "Epoch: 627 Loss: 0.06125929206609726\n",
      "Epoch: 628 Loss: 0.05112171545624733\n",
      "Epoch: 629 Loss: 0.050008274614810944\n",
      "Epoch: 630 Loss: 0.06338822841644287\n",
      "Epoch: 631 Loss: 0.17586825788021088\n",
      "Epoch: 632 Loss: 0.17532870173454285\n",
      "Epoch: 633 Loss: 0.11860243231058121\n",
      "Epoch: 634 Loss: 0.08126471936702728\n",
      "Epoch: 635 Loss: 0.11995553225278854\n",
      "Epoch: 636 Loss: 0.0946526825428009\n",
      "Epoch: 637 Loss: 0.1766735464334488\n",
      "Epoch: 638 Loss: 0.14523212611675262\n",
      "Epoch: 639 Loss: 0.16686944663524628\n",
      "Epoch: 640 Loss: 0.0943855494260788\n",
      "Epoch: 641 Loss: 0.1187836080789566\n",
      "Epoch: 642 Loss: 0.12767837941646576\n",
      "Epoch: 643 Loss: 0.22244715690612793\n",
      "Epoch: 644 Loss: 0.2333802729845047\n",
      "Epoch: 645 Loss: 0.16000664234161377\n",
      "Epoch: 646 Loss: 0.15617361664772034\n",
      "Epoch: 647 Loss: 0.16351798176765442\n",
      "Epoch: 648 Loss: 0.13642315566539764\n",
      "Epoch: 649 Loss: 0.13247624039649963\n",
      "Epoch: 650 Loss: 0.043856166303157806\n",
      "Epoch: 651 Loss: 0.06703272461891174\n",
      "Epoch: 652 Loss: 0.12418671697378159\n",
      "Epoch: 653 Loss: 0.18543557822704315\n",
      "Epoch: 654 Loss: 0.08841454237699509\n",
      "Epoch: 655 Loss: 0.04436982423067093\n",
      "Epoch: 656 Loss: 0.11333946883678436\n",
      "Epoch: 657 Loss: 0.08620166778564453\n",
      "Epoch: 658 Loss: 0.17204692959785461\n",
      "Epoch: 659 Loss: 0.193391352891922\n",
      "Epoch: 660 Loss: 0.22813819348812103\n",
      "Epoch: 661 Loss: 0.2082356959581375\n",
      "Epoch: 662 Loss: 0.0501885823905468\n",
      "Epoch: 663 Loss: 0.1025613322854042\n",
      "Epoch: 664 Loss: 0.14540159702301025\n",
      "Epoch: 665 Loss: 0.11177610605955124\n",
      "Epoch: 666 Loss: 0.095969557762146\n",
      "Epoch: 667 Loss: 0.0909142941236496\n",
      "Epoch: 668 Loss: 0.07088582962751389\n",
      "Epoch: 669 Loss: 0.21703040599822998\n",
      "Epoch: 670 Loss: 0.16831976175308228\n",
      "Epoch: 671 Loss: 0.0992114394903183\n",
      "Epoch: 672 Loss: 0.10116535425186157\n",
      "Epoch: 673 Loss: 0.09949198365211487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 674 Loss: 0.037618622183799744\n",
      "Epoch: 675 Loss: 0.1050843596458435\n",
      "Epoch: 676 Loss: 0.07324570417404175\n",
      "Epoch: 677 Loss: 0.07730214297771454\n",
      "Epoch: 678 Loss: 0.05938930809497833\n",
      "Epoch: 679 Loss: 0.1654759645462036\n",
      "Epoch: 680 Loss: 0.13823512196540833\n",
      "Epoch: 681 Loss: 0.10468531399965286\n",
      "Epoch: 682 Loss: 0.100103959441185\n",
      "Epoch: 683 Loss: 0.08072859793901443\n",
      "Epoch: 684 Loss: 0.1463373601436615\n",
      "Epoch: 685 Loss: 0.05196819081902504\n",
      "Epoch: 686 Loss: 0.11635921895503998\n",
      "Epoch: 687 Loss: 0.15764403343200684\n",
      "Epoch: 688 Loss: 0.11308341473340988\n",
      "Epoch: 689 Loss: 0.17152616381645203\n",
      "Epoch: 690 Loss: 0.06999006122350693\n",
      "Epoch: 691 Loss: 0.10053700953722\n",
      "Epoch: 692 Loss: 0.06216587498784065\n",
      "Epoch: 693 Loss: 0.13231608271598816\n",
      "Epoch: 694 Loss: 0.038682665675878525\n",
      "Epoch: 695 Loss: 0.04859084635972977\n",
      "Epoch: 696 Loss: 0.12026067823171616\n",
      "Epoch: 697 Loss: 0.10688439756631851\n",
      "Epoch: 698 Loss: 0.05926899611949921\n",
      "Epoch: 699 Loss: 0.2465394139289856\n",
      "Epoch: 700 Loss: 0.1771700084209442\n",
      "Epoch: 701 Loss: 0.11962205171585083\n",
      "Epoch: 702 Loss: 0.15888935327529907\n",
      "Epoch: 703 Loss: 0.0521077923476696\n",
      "Epoch: 704 Loss: 0.10025496780872345\n",
      "Epoch: 705 Loss: 0.22578835487365723\n",
      "Epoch: 706 Loss: 0.1526297628879547\n",
      "Epoch: 707 Loss: 0.029352940618991852\n",
      "Epoch: 708 Loss: 0.13479562103748322\n",
      "Epoch: 709 Loss: 0.23726770281791687\n",
      "Epoch: 710 Loss: 0.1847512125968933\n",
      "Epoch: 711 Loss: 0.1749570518732071\n",
      "Epoch: 712 Loss: 0.11156293749809265\n",
      "Epoch: 713 Loss: 0.08439018577337265\n",
      "Epoch: 714 Loss: 0.03756110742688179\n",
      "Epoch: 715 Loss: 0.038171231746673584\n",
      "Epoch: 716 Loss: 0.03855250030755997\n",
      "Epoch: 717 Loss: 0.07730265706777573\n",
      "Epoch: 718 Loss: 0.07968011498451233\n",
      "Epoch: 719 Loss: 0.15267789363861084\n",
      "Epoch: 720 Loss: 0.05600679665803909\n",
      "Epoch: 721 Loss: 0.17554455995559692\n",
      "Epoch: 722 Loss: 0.07687758654356003\n",
      "Epoch: 723 Loss: 0.09664605557918549\n",
      "Epoch: 724 Loss: 0.3104912042617798\n",
      "Epoch: 725 Loss: 0.0758216604590416\n",
      "Epoch: 726 Loss: 0.17165423929691315\n",
      "Epoch: 727 Loss: 0.21441273391246796\n",
      "Epoch: 728 Loss: 0.04468172788619995\n",
      "Epoch: 729 Loss: 0.18333984911441803\n",
      "Epoch: 730 Loss: 0.2061353325843811\n",
      "Epoch: 731 Loss: 0.17191408574581146\n",
      "Epoch: 732 Loss: 0.08903521299362183\n",
      "Epoch: 733 Loss: 0.03407379239797592\n",
      "Epoch: 734 Loss: 0.034917302429676056\n",
      "Epoch: 735 Loss: 0.07075601816177368\n",
      "Epoch: 736 Loss: 0.06536342948675156\n",
      "Epoch: 737 Loss: 0.07873762398958206\n",
      "Epoch: 738 Loss: 0.16734638810157776\n",
      "Epoch: 739 Loss: 0.045299410820007324\n",
      "Epoch: 740 Loss: 0.23827767372131348\n",
      "Epoch: 741 Loss: 0.05674073472619057\n",
      "Epoch: 742 Loss: 0.09597770124673843\n",
      "Epoch: 743 Loss: 0.06391400098800659\n",
      "Epoch: 744 Loss: 0.12242890894412994\n",
      "Epoch: 745 Loss: 0.08258102834224701\n",
      "Epoch: 746 Loss: 0.09803047776222229\n",
      "Epoch: 747 Loss: 0.07651891559362411\n",
      "Epoch: 748 Loss: 0.1441034972667694\n",
      "Epoch: 749 Loss: 0.13394926488399506\n",
      "Epoch: 750 Loss: 0.057869717478752136\n",
      "Epoch: 751 Loss: 0.04013195261359215\n",
      "Epoch: 752 Loss: 0.04820495471358299\n",
      "Epoch: 753 Loss: 0.05771169438958168\n",
      "Epoch: 754 Loss: 0.20731216669082642\n",
      "Epoch: 755 Loss: 0.0958835631608963\n",
      "Epoch: 756 Loss: 0.0721074789762497\n",
      "Epoch: 757 Loss: 0.1565970480442047\n",
      "Epoch: 758 Loss: 0.1831406205892563\n",
      "Epoch: 759 Loss: 0.04195889085531235\n",
      "Epoch: 760 Loss: 0.06099206954240799\n",
      "Epoch: 761 Loss: 0.12275227159261703\n",
      "Epoch: 762 Loss: 0.05358683317899704\n",
      "Epoch: 763 Loss: 0.06429581344127655\n",
      "Epoch: 764 Loss: 0.05108943581581116\n",
      "Epoch: 765 Loss: 0.14575380086898804\n",
      "Epoch: 766 Loss: 0.06144922226667404\n",
      "Epoch: 767 Loss: 0.06748051941394806\n",
      "Epoch: 768 Loss: 0.04931904748082161\n",
      "Epoch: 769 Loss: 0.18532367050647736\n",
      "Epoch: 770 Loss: 0.14435924589633942\n",
      "Epoch: 771 Loss: 0.05758978798985481\n",
      "Epoch: 772 Loss: 0.04937604442238808\n",
      "Epoch: 773 Loss: 0.04320443049073219\n",
      "Epoch: 774 Loss: 0.2175678312778473\n",
      "Epoch: 775 Loss: 0.056796543300151825\n",
      "Epoch: 776 Loss: 0.08959301561117172\n",
      "Epoch: 777 Loss: 0.22962146997451782\n",
      "Epoch: 778 Loss: 0.0843416303396225\n",
      "Epoch: 779 Loss: 0.12587423622608185\n",
      "Epoch: 780 Loss: 0.31191566586494446\n",
      "Epoch: 781 Loss: 0.06377702951431274\n",
      "Epoch: 782 Loss: 0.07241161912679672\n",
      "Epoch: 783 Loss: 0.08770926296710968\n",
      "Epoch: 784 Loss: 0.1344231516122818\n",
      "Epoch: 785 Loss: 0.10347313433885574\n",
      "Epoch: 786 Loss: 0.24999402463436127\n",
      "Epoch: 787 Loss: 0.05564145743846893\n",
      "Epoch: 788 Loss: 0.10009248554706573\n",
      "Epoch: 789 Loss: 0.08205640316009521\n",
      "Epoch: 790 Loss: 0.17018155753612518\n",
      "Epoch: 791 Loss: 0.12735453248023987\n",
      "Epoch: 792 Loss: 0.08067340403795242\n",
      "Epoch: 793 Loss: 0.048430435359478\n",
      "Epoch: 794 Loss: 0.030989304184913635\n",
      "Epoch: 795 Loss: 0.04828328266739845\n",
      "Epoch: 796 Loss: 0.18570096790790558\n",
      "Epoch: 797 Loss: 0.11614012718200684\n",
      "Epoch: 798 Loss: 0.07896605879068375\n",
      "Epoch: 799 Loss: 0.17342673242092133\n",
      "Epoch: 800 Loss: 0.06958021223545074\n",
      "Epoch: 801 Loss: 0.07748021185398102\n",
      "Epoch: 802 Loss: 0.08070015162229538\n",
      "Epoch: 803 Loss: 0.07200393825769424\n",
      "Epoch: 804 Loss: 0.26087456941604614\n",
      "Epoch: 805 Loss: 0.06827487796545029\n",
      "Epoch: 806 Loss: 0.1141849085688591\n",
      "Epoch: 807 Loss: 0.06448082625865936\n",
      "Epoch: 808 Loss: 0.08098111301660538\n",
      "Epoch: 809 Loss: 0.16186700761318207\n",
      "Epoch: 810 Loss: 0.11633928120136261\n",
      "Epoch: 811 Loss: 0.039190758019685745\n",
      "Epoch: 812 Loss: 0.11968564987182617\n",
      "Epoch: 813 Loss: 0.12079980969429016\n",
      "Epoch: 814 Loss: 0.07433196157217026\n",
      "Epoch: 815 Loss: 0.23498541116714478\n",
      "Epoch: 816 Loss: 0.09786582738161087\n",
      "Epoch: 817 Loss: 0.11417992413043976\n",
      "Epoch: 818 Loss: 0.2392735332250595\n",
      "Epoch: 819 Loss: 0.18819668889045715\n",
      "Epoch: 820 Loss: 0.05620919540524483\n",
      "Epoch: 821 Loss: 0.1090737134218216\n",
      "Epoch: 822 Loss: 0.12540115416049957\n",
      "Epoch: 823 Loss: 0.06101042032241821\n",
      "Epoch: 824 Loss: 0.07917586714029312\n",
      "Epoch: 825 Loss: 0.07638083398342133\n",
      "Epoch: 826 Loss: 0.11760216951370239\n",
      "Epoch: 827 Loss: 0.05185093358159065\n",
      "Epoch: 828 Loss: 0.20704108476638794\n",
      "Epoch: 829 Loss: 0.12277361005544662\n",
      "Epoch: 830 Loss: 0.11111316084861755\n",
      "Epoch: 831 Loss: 0.042802248150110245\n",
      "Epoch: 832 Loss: 0.0668853223323822\n",
      "Epoch: 833 Loss: 0.07434197515249252\n",
      "Epoch: 834 Loss: 0.16368283331394196\n",
      "Epoch: 835 Loss: 0.17978155612945557\n",
      "Epoch: 836 Loss: 0.06587779521942139\n",
      "Epoch: 837 Loss: 0.10420557856559753\n",
      "Epoch: 838 Loss: 0.0422215461730957\n",
      "Epoch: 839 Loss: 0.22453224658966064\n",
      "Epoch: 840 Loss: 0.09507649391889572\n",
      "Epoch: 841 Loss: 0.1163281798362732\n",
      "Epoch: 842 Loss: 0.10730943828821182\n",
      "Epoch: 843 Loss: 0.0769420936703682\n",
      "Epoch: 844 Loss: 0.04056200385093689\n",
      "Epoch: 845 Loss: 0.06880464404821396\n",
      "Epoch: 846 Loss: 0.07881461828947067\n",
      "Epoch: 847 Loss: 0.14052295684814453\n",
      "Epoch: 848 Loss: 0.08560740202665329\n",
      "Epoch: 849 Loss: 0.13812455534934998\n",
      "Epoch: 850 Loss: 0.08835642784833908\n",
      "Epoch: 851 Loss: 0.1727152168750763\n",
      "Epoch: 852 Loss: 0.11487434804439545\n",
      "Epoch: 853 Loss: 0.05444619804620743\n",
      "Epoch: 854 Loss: 0.07629534602165222\n",
      "Epoch: 855 Loss: 0.12981083989143372\n",
      "Epoch: 856 Loss: 0.12245488166809082\n",
      "Epoch: 857 Loss: 0.06989465653896332\n",
      "Epoch: 858 Loss: 0.040248461067676544\n",
      "Epoch: 859 Loss: 0.23025812208652496\n",
      "Epoch: 860 Loss: 0.12375567853450775\n",
      "Epoch: 861 Loss: 0.11714234948158264\n",
      "Epoch: 862 Loss: 0.22779011726379395\n",
      "Epoch: 863 Loss: 0.05054589360952377\n",
      "Epoch: 864 Loss: 0.21056459844112396\n",
      "Epoch: 865 Loss: 0.1333308219909668\n",
      "Epoch: 866 Loss: 0.04721786454319954\n",
      "Epoch: 867 Loss: 0.07502564042806625\n",
      "Epoch: 868 Loss: 0.06133204698562622\n",
      "Epoch: 869 Loss: 0.1263524740934372\n",
      "Epoch: 870 Loss: 0.18293878436088562\n",
      "Epoch: 871 Loss: 0.1301484853029251\n",
      "Epoch: 872 Loss: 0.028301997110247612\n",
      "Epoch: 873 Loss: 0.05025527998805046\n",
      "Epoch: 874 Loss: 0.12344624847173691\n",
      "Epoch: 875 Loss: 0.16639626026153564\n",
      "Epoch: 876 Loss: 0.04703504592180252\n",
      "Epoch: 877 Loss: 0.07256243377923965\n",
      "Epoch: 878 Loss: 0.089693084359169\n",
      "Epoch: 879 Loss: 0.1526540070772171\n",
      "Epoch: 880 Loss: 0.10551781207323074\n",
      "Epoch: 881 Loss: 0.050686340779066086\n",
      "Epoch: 882 Loss: 0.04149286076426506\n",
      "Epoch: 883 Loss: 0.035965703427791595\n",
      "Epoch: 884 Loss: 0.1484442502260208\n",
      "Epoch: 885 Loss: 0.08149762451648712\n",
      "Epoch: 886 Loss: 0.059371791779994965\n",
      "Epoch: 887 Loss: 0.09064667671918869\n",
      "Epoch: 888 Loss: 0.11729441583156586\n",
      "Epoch: 889 Loss: 0.04740545153617859\n",
      "Epoch: 890 Loss: 0.09304377436637878\n",
      "Epoch: 891 Loss: 0.03877466544508934\n",
      "Epoch: 892 Loss: 0.07758360356092453\n",
      "Epoch: 893 Loss: 0.05057466775178909\n",
      "Epoch: 894 Loss: 0.047574326395988464\n",
      "Epoch: 895 Loss: 0.13483893871307373\n",
      "Epoch: 896 Loss: 0.08452516794204712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 897 Loss: 0.04912498965859413\n",
      "Epoch: 898 Loss: 0.19796785712242126\n",
      "Epoch: 899 Loss: 0.09828601777553558\n",
      "Epoch: 900 Loss: 0.10518238693475723\n",
      "Epoch: 901 Loss: 0.1384740173816681\n",
      "Epoch: 902 Loss: 0.14670926332473755\n",
      "Epoch: 903 Loss: 0.04669814556837082\n",
      "Epoch: 904 Loss: 0.11263088136911392\n",
      "Epoch: 905 Loss: 0.03854811191558838\n",
      "Epoch: 906 Loss: 0.1389341503381729\n",
      "Epoch: 907 Loss: 0.10816651582717896\n",
      "Epoch: 908 Loss: 0.08258107304573059\n",
      "Epoch: 909 Loss: 0.059611521661281586\n",
      "Epoch: 910 Loss: 0.05386069416999817\n",
      "Epoch: 911 Loss: 0.03399866819381714\n",
      "Epoch: 912 Loss: 0.11144061386585236\n",
      "Epoch: 913 Loss: 0.09271983057260513\n",
      "Epoch: 914 Loss: 0.24467594921588898\n",
      "Epoch: 915 Loss: 0.19418559968471527\n",
      "Epoch: 916 Loss: 0.0814606323838234\n",
      "Epoch: 917 Loss: 0.0580340214073658\n",
      "Epoch: 918 Loss: 0.11952399462461472\n",
      "Epoch: 919 Loss: 0.09276904910802841\n",
      "Epoch: 920 Loss: 0.08259749412536621\n",
      "Epoch: 921 Loss: 0.15138739347457886\n",
      "Epoch: 922 Loss: 0.06576043367385864\n",
      "Epoch: 923 Loss: 0.21681556105613708\n",
      "Epoch: 924 Loss: 0.03976195678114891\n",
      "Epoch: 925 Loss: 0.11518892645835876\n",
      "Epoch: 926 Loss: 0.11324398219585419\n",
      "Epoch: 927 Loss: 0.14364910125732422\n",
      "Epoch: 928 Loss: 0.18402157723903656\n",
      "Epoch: 929 Loss: 0.185270294547081\n",
      "Epoch: 930 Loss: 0.060235582292079926\n",
      "Epoch: 931 Loss: 0.1463545262813568\n",
      "Epoch: 932 Loss: 0.03231904283165932\n",
      "Epoch: 933 Loss: 0.03627771511673927\n",
      "Epoch: 934 Loss: 0.14832830429077148\n",
      "Epoch: 935 Loss: 0.07958021014928818\n",
      "Epoch: 936 Loss: 0.039922356605529785\n",
      "Epoch: 937 Loss: 0.07144620269536972\n",
      "Epoch: 938 Loss: 0.227944016456604\n",
      "Epoch: 939 Loss: 0.053629785776138306\n",
      "Epoch: 940 Loss: 0.2778928875923157\n",
      "Epoch: 941 Loss: 0.06405486166477203\n",
      "Epoch: 942 Loss: 0.28186482191085815\n",
      "Epoch: 943 Loss: 0.03694438561797142\n",
      "Epoch: 944 Loss: 0.0952107310295105\n",
      "Epoch: 945 Loss: 0.08984393626451492\n",
      "Epoch: 946 Loss: 0.1171736866235733\n",
      "Epoch: 947 Loss: 0.06018487736582756\n",
      "Epoch: 948 Loss: 0.1009562760591507\n",
      "Epoch: 949 Loss: 0.10812927782535553\n",
      "Epoch: 950 Loss: 0.06505945324897766\n",
      "Epoch: 951 Loss: 0.06282323598861694\n",
      "Epoch: 952 Loss: 0.04491960629820824\n",
      "Epoch: 953 Loss: 0.16101793944835663\n",
      "Epoch: 954 Loss: 0.04074596241116524\n",
      "Epoch: 955 Loss: 0.1773984730243683\n",
      "Epoch: 956 Loss: 0.05121266469359398\n",
      "Epoch: 957 Loss: 0.17513827979564667\n",
      "Epoch: 958 Loss: 0.04194638133049011\n",
      "Epoch: 959 Loss: 0.08736308664083481\n",
      "Epoch: 960 Loss: 0.05455341190099716\n",
      "Epoch: 961 Loss: 0.18358632922172546\n",
      "Epoch: 962 Loss: 0.15689758956432343\n",
      "Epoch: 963 Loss: 0.10338716208934784\n",
      "Epoch: 964 Loss: 0.04589655250310898\n",
      "Epoch: 965 Loss: 0.0815647542476654\n",
      "Epoch: 966 Loss: 0.0723990648984909\n",
      "Epoch: 967 Loss: 0.04018467292189598\n",
      "Epoch: 968 Loss: 0.09157250821590424\n",
      "Epoch: 969 Loss: 0.16507013142108917\n",
      "Epoch: 970 Loss: 0.13517434895038605\n",
      "Epoch: 971 Loss: 0.04433108866214752\n",
      "Epoch: 972 Loss: 0.25932902097702026\n",
      "Epoch: 973 Loss: 0.12067132443189621\n",
      "Epoch: 974 Loss: 0.03911025822162628\n",
      "Epoch: 975 Loss: 0.11019640415906906\n",
      "Epoch: 976 Loss: 0.05078619718551636\n",
      "Epoch: 977 Loss: 0.19183990359306335\n",
      "Epoch: 978 Loss: 0.0641874223947525\n",
      "Epoch: 979 Loss: 0.17324216663837433\n",
      "Epoch: 980 Loss: 0.0971115455031395\n",
      "Epoch: 981 Loss: 0.08434920012950897\n",
      "Epoch: 982 Loss: 0.0912456288933754\n",
      "Epoch: 983 Loss: 0.054801370948553085\n",
      "Epoch: 984 Loss: 0.07169157266616821\n",
      "Epoch: 985 Loss: 0.07155648618936539\n",
      "Epoch: 986 Loss: 0.04362517595291138\n",
      "Epoch: 987 Loss: 0.19717872142791748\n",
      "Epoch: 988 Loss: 0.09982109814882278\n",
      "Epoch: 989 Loss: 0.06110088527202606\n",
      "Epoch: 990 Loss: 0.07739204168319702\n",
      "Epoch: 991 Loss: 0.06449596583843231\n",
      "Epoch: 992 Loss: 0.03071429207921028\n",
      "Epoch: 993 Loss: 0.12622563540935516\n",
      "Epoch: 994 Loss: 0.11467043310403824\n",
      "Epoch: 995 Loss: 0.13960875570774078\n",
      "Epoch: 996 Loss: 0.17341674864292145\n",
      "Epoch: 997 Loss: 0.16678066551685333\n",
      "Epoch: 998 Loss: 0.0663851946592331\n",
      "Epoch: 999 Loss: 0.10100498050451279\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    losses = []\n",
    "    for batch_idx, (data1, data2, targets) in enumerate(train_loader):\n",
    "        data1 = data1.to(device=device)\n",
    "        data2 = data2.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #output = model(data1, data2)\n",
    "        \n",
    "        output = model(\n",
    "            past_values=data1,\n",
    "            future_values=data2,\n",
    "        )\n",
    "        \n",
    "        loss = 10*criterion(output, targets)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    print('Epoch:', epoch, 'Loss:', loss.item())\n",
    "    \n",
    "    if mean_loss<min_val_acc:\n",
    "        min_val_acc =mean_loss\n",
    "        torch.save(model.state_dict(), 'best_cos_3-3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = torch.from_numpy(trainX1_copy).float()\n",
    "test_data2 = torch.from_numpy(trainX2_copy).float()\n",
    "test_label = torch.from_numpy(trainY_copy).float()\n",
    "test_dataset = TensorDataset(test_data1, test_data2, test_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for batch_idx, (data1, data2, targets) in enumerate(test_loader):\n",
    "    data1 = data1.to(device=device)\n",
    "    data2 = data2.to(device=device)\n",
    "    output = model(\n",
    "        past_values=data1,\n",
    "        future_values=data2,\n",
    "    )\n",
    "    targets = targets.to(device=device)\n",
    "    predictions+=output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_copies1 = np.repeat(predictions[:,0].reshape(-1,1), df_for_training.shape[1], axis=-1) \n",
    "predict_1 = scaler.inverse_transform(predict_copies1)[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_copies1 = np.repeat(trainY_copy[:,0].reshape(-1,1), df_for_training.shape[1], axis=-1)\n",
    "trainY_1 = scaler.inverse_transform(trainY_copies1)[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "plt.xlabel('样本 /个',fontdict={ 'size'   : 30})\n",
    "plt.ylabel('推力值 /N',fontdict={ 'size'   : 30})\n",
    "plt.plot(trainY_1[200:11000],marker='.',label='true')\n",
    "plt.plot(predict_1[200:11000],'r',marker='.',label='predicted')                  #sample的时刻是一致的\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
