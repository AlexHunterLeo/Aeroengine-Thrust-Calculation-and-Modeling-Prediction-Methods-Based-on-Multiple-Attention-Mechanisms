{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed a sequence of categorical features.\n",
    "    Args:\n",
    "        cardinalities (`list[int]`):\n",
    "            List of cardinalities of the categorical features.\n",
    "        embedding_dims (`list[int]`):\n",
    "            List of embedding dimensions of the categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = len(cardinalities)\n",
    "        self.embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.num_features > 1:\n",
    "            # we slice the last dimension, giving an array of length\n",
    "            # self.num_features with shape (N,T) or (N)\n",
    "            cat_feature_slices = torch.chunk(features, self.num_features, dim=-1)\n",
    "        else:\n",
    "            cat_feature_slices = [features]\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                embed(cat_feature_slice.squeeze(-1))\n",
    "                for embed, cat_feature_slice in zip(self.embedders, cat_feature_slices)\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesStdScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Standardize features by calculating the mean and scaling along some given dimension `dim`, and then normalizes it\n",
    "    by subtracting from the mean and dividing by the standard deviation.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to calculate the mean and standard deviation.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-5):\n",
    "            Default scale that is used for elements that are constantly zero along dimension `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False, minimum_scale: float = 1e-5):\n",
    "        super().__init__()\n",
    "        if not dim > 0:\n",
    "            raise ValueError(\"Cannot compute scale along dim = 0 (batch dimension), please provide dim > 0\")\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        denominator = weights.sum(self.dim, keepdim=self.keepdim)\n",
    "        denominator = denominator.clamp_min(1.0)\n",
    "        loc = (data * weights).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "\n",
    "        variance = (((data - loc) * weights) ** 2).sum(self.dim, keepdim=self.keepdim) / denominator\n",
    "        scale = torch.sqrt(variance + self.minimum_scale)\n",
    "        return (data - loc) / scale, loc, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesMeanScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a scaling factor as the weighted average absolute value along dimension `dim`, and scales the data\n",
    "    accordingly.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "        default_scale (`float`, *optional*, defaults to `None`):\n",
    "            Default scale that is used for elements that are constantly zero. If `None`, we use the scale of the batch.\n",
    "        minimum_scale (`float`, *optional*, defaults to 1e-10):\n",
    "            Default minimum possible scale that is used for any item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int = -1, keepdim: bool = True, default_scale: Optional[float] = None, minimum_scale: float = 1e-10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "        self.minimum_scale = minimum_scale\n",
    "        self.default_scale = default_scale\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, data: torch.Tensor, observed_indicator: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # shape: (N, [C], T=1)\n",
    "        ts_sum = (data * observed_indicator).abs().sum(self.dim, keepdim=True)\n",
    "        num_observed = observed_indicator.sum(self.dim, keepdim=True)\n",
    "\n",
    "        scale = ts_sum / torch.clamp(num_observed, min=1)\n",
    "\n",
    "        # If `default_scale` is provided, we use it, otherwise we use the scale\n",
    "        # of the batch.\n",
    "        if self.default_scale is None:\n",
    "            batch_sum = ts_sum.sum(dim=0)\n",
    "            batch_observations = torch.clamp(num_observed.sum(0), min=1)\n",
    "            default_scale = torch.squeeze(batch_sum / batch_observations)\n",
    "        else:\n",
    "            default_scale = self.default_scale * torch.ones_like(scale)\n",
    "\n",
    "        # apply default scale where there are no observations\n",
    "        scale = torch.where(num_observed > 0, scale, default_scale)\n",
    "\n",
    "        # ensure the scale is at least `self.minimum_scale`\n",
    "        scale = torch.clamp(scale, min=self.minimum_scale)\n",
    "        scaled_data = data / scale\n",
    "\n",
    "        if not self.keepdim:\n",
    "            scale = scale.squeeze(dim=self.dim)\n",
    "\n",
    "        return scaled_data, torch.zeros_like(scale), scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesNOPScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Assigns a scaling factor equal to 1 along dimension `dim`, and therefore applies no scaling to the input data.\n",
    "    Args:\n",
    "        dim (`int`):\n",
    "            Dimension along which to compute the scale.\n",
    "        keepdim (`bool`, *optional*, defaults to `False`):\n",
    "            Controls whether to retain dimension `dim` (of length 1) in the scale tensor, or suppress it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, keepdim: bool = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "\n",
    "    def forward(\n",
    "        self, data: torch.Tensor, observed_indicator: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        scale = torch.ones_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        loc = torch.zeros_like(data, requires_grad=False).mean(dim=self.dim, keepdim=self.keepdim)\n",
    "        return data, loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the negative log likelihood loss from input distribution with respect to target.\n",
    "    \"\"\"\n",
    "    return -input.log_prob(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n",
    "    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n",
    "    Args:\n",
    "        input_tensor (`torch.FloatTensor`):\n",
    "            Input tensor, of which the average must be computed.\n",
    "        weights (`torch.FloatTensor`, *optional*):\n",
    "            Weights tensor, of the same shape as `input_tensor`.\n",
    "        dim (`int`, *optional*):\n",
    "            The dim along which to average `input_tensor`.\n",
    "    Returns:\n",
    "        `torch.FloatTensor`: The tensor with values averaged along the specified `dim`.\n",
    "    \"\"\"\n",
    "    if weights is not None:\n",
    "        weighted_tensor = torch.where(weights != 0, input_tensor * weights, torch.zeros_like(input_tensor))\n",
    "        sum_weights = torch.clamp(weights.sum(dim=dim) if dim else weights.sum(), min=1.0)\n",
    "        return (weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()) / sum_weights\n",
    "    else:\n",
    "        return input_tensor.mean(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesValueEmbedding(nn.Module):\n",
    "    def __init__(self, feature_size, d_model):\n",
    "        super().__init__()\n",
    "        self.value_projection = nn.Linear(in_features=feature_size, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.value_projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
    "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
    "        # the provided `key_value_states` to support prefix tuning\n",
    "        if (\n",
    "            is_cross_attention\n",
    "            and past_key_value is not None\n",
    "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
    "        ):\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.reshape(*proj_shape)\n",
    "        value_states = value_states.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned across GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, encoder_attention_heads, attention_dropout, \n",
    "                 dropout, activation_dropout, encoder_ffn_dim,):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "        self.self_attn = TimeSeriesTransformerAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=encoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "        layer_head_mask: torch.FloatTensor,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, decoder_attention_heads, attention_dropout,\n",
    "                 dropout, activation_dropout, decoder_ffn_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = d_model\n",
    "\n",
    "        self.self_attn = TimeSeriesTransformerAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.GELU()\n",
    "        self.activation_dropout = activation_dropout\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.encoder_attn = TimeSeriesTransformerAttention(\n",
    "            self.embed_dim,\n",
    "            decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)       \n",
    "        self.fc1 = nn.Linear(self.embed_dim, decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                layer_head_mask=cross_attn_layer_head_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`TimeSeriesTransformerEncoderLayer`].\n",
    "    Args:\n",
    "        config: TimeSeriesTransformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, encoder_layers, encoder_layerdrop, context_length, \n",
    "                 prediction_length, d_model, encoder_attention_heads, attention_dropout, \n",
    "                 dropout, activation_dropout, encoder_ffn_dim, output_attentions,\n",
    "                 output_hidden_states, use_return_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = encoder_layerdrop\n",
    "        if prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = TimeSeriesValueEmbedding(feature_size=feature_size, \n",
    "                                                        d_model=d_model)\n",
    "        self.embed_positions = TimeSeriesSinusoidalPositionalEmbedding(\n",
    "            context_length + prediction_length, d_model\n",
    "        )\n",
    "        self.layers = nn.ModuleList([TimeSeriesTransformerEncoderLayer(d_model, encoder_attention_heads, \n",
    "                 attention_dropout, dropout, activation_dropout, encoder_ffn_dim) for _ in range(encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size())\n",
    "\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a\n",
    "    [`TimeSeriesTransformerDecoderLayer`]\n",
    "    Args:\n",
    "        config: TimeSeriesTransformerConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, decoder_layers, decoder_layerdrop, context_length, \n",
    "                 prediction_length, d_model, decoder_attention_heads, attention_dropout, \n",
    "                 dropout, activation_dropout, decoder_ffn_dim, output_attentions,\n",
    "                 output_hidden_states, use_return_dict, use_cache=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = decoder_layerdrop\n",
    "        if prediction_length is None:\n",
    "            raise ValueError(\"The `prediction_length` config needs to be specified.\")\n",
    "\n",
    "        self.value_embedding = TimeSeriesValueEmbedding(feature_size=feature_size, \n",
    "                                                        d_model=d_model)\n",
    "        self.embed_positions = TimeSeriesSinusoidalPositionalEmbedding(\n",
    "            context_length + prediction_length, d_model\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList([TimeSeriesTransformerDecoderLayer(d_model, decoder_attention_heads, \n",
    "                 attention_dropout, dropout, activation_dropout, decoder_ffn_dim) for _ in range(decoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_cache = use_cache\n",
    "        self.use_return_dict = use_return_dict\n",
    "\n",
    "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                device=inputs_embeds.device,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
    "                inputs_embeds.device\n",
    "            )\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
    "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
    "                selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
    "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
    "\n",
    "        hidden_states = self.value_embedding(inputs_embeds)\n",
    "        embed_pos = self.embed_positions(inputs_embeds.size(), past_key_values_length=self.context_length)\n",
    "        hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                if attn_mask.size()[0] != (len(self.layers)):\n",
    "                    raise ValueError(\n",
    "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                        f\" {head_mask.size()[0]}.\"\n",
    "                    )\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):\n",
    "                continue\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, output_attentions, use_cache)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
    "                    None,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    cross_attn_layer_head_mask=(\n",
    "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
    "                    ),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerModel(nn.Module):\n",
    "    def __init__(self, num_static_categorical_features, cardinality, embedding_dimension, \n",
    "                 feature_size, encoder_layers, encoder_layerdrop, decoder_layers, \n",
    "                 decoder_layerdrop, context_length, prediction_length, lags_sequence, input_size, \n",
    "                 d_model, encoder_attention_heads, decoder_attention_heads, attention_dropout, \n",
    "                 dropout, activation_dropout, encoder_ffn_dim, decoder_ffn_dim,\n",
    "                 output_attentions, output_hidden_states, use_return_dict, use_cache, \n",
    "                 scaling = \"std\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if scaling == \"mean\" or scaling:\n",
    "            self.scaler = TimeSeriesMeanScaler(dim=1, keepdim=True)\n",
    "        elif scaling == \"std\":\n",
    "            self.scaler = TimeSeriesStdScaler(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.scaler = TimeSeriesNOPScaler(dim=1, keepdim=True)\n",
    "\n",
    "        if num_static_categorical_features > 0:\n",
    "            self.embedder = TimeSeriesFeatureEmbedder(\n",
    "                cardinalities=cardinality,\n",
    "                embedding_dims=embedding_dimension,\n",
    "            )\n",
    "\n",
    "        # transformer encoder-decoder and mask initializer\n",
    "        self.encoder = TimeSeriesTransformerEncoder(feature_size, encoder_layers, encoder_layerdrop, \n",
    "                 context_length, prediction_length, d_model, encoder_attention_heads, \n",
    "                 attention_dropout, dropout, activation_dropout, encoder_ffn_dim, \n",
    "                 output_attentions, output_hidden_states, use_return_dict)\n",
    "        self.decoder = TimeSeriesTransformerDecoder(feature_size, decoder_layers, decoder_layerdrop, \n",
    "                 context_length, prediction_length, d_model, decoder_attention_heads, \n",
    "                 attention_dropout, dropout, activation_dropout, decoder_ffn_dim,\n",
    "                 output_attentions, output_hidden_states, use_return_dict, use_cache)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.lags_sequence = lags_sequence\n",
    "        self.input_size = input_size\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.use_cache = use_cache\n",
    "        self.use_return_dict = use_return_dict\n",
    "        self.W = nn.Linear(d_model, 1)\n",
    "\n",
    "    @property\n",
    "    def _past_length(self) -> int:\n",
    "        return self.context_length + max(self.lags_sequence)\n",
    "\n",
    "    def get_lagged_subsequences(\n",
    "        self, sequence: torch.Tensor, subsequences_length: int, shift: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns lagged subsequences of a given sequence. Returns a tensor of shape (N, S, C, I),\n",
    "            where S = subsequences_length and I = len(indices), containing lagged subsequences. Specifically, lagged[i,\n",
    "            j, :, k] = sequence[i, -indices[k]-S+j, :].\n",
    "        Args:\n",
    "            sequence: Tensor\n",
    "                The sequence from which lagged subsequences should be extracted. Shape: (N, T, C).\n",
    "            subsequences_length : int\n",
    "                Length of the subsequences to be extracted.\n",
    "            shift: int\n",
    "                Shift the lags by this amount back.\n",
    "        \"\"\"\n",
    "        sequence_length = sequence.shape[1]\n",
    "        indices = [lag - shift for lag in self.lags_sequence]\n",
    "\n",
    "        if max(indices) + subsequences_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"lags cannot go further than history length, found lag {max(indices)} \"\n",
    "                f\"while history length is only {sequence_length}\"\n",
    "            )\n",
    "\n",
    "        lagged_values = []\n",
    "        for lag_index in indices:\n",
    "            begin_index = -lag_index - subsequences_length\n",
    "            end_index = -lag_index if lag_index > 0 else None\n",
    "            lagged_values.append(sequence[:, begin_index:end_index, ...])\n",
    "        return torch.stack(lagged_values, dim=-1)\n",
    "\n",
    "    def create_network_inputs(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: torch.Tensor,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        past_observed_mask: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        # time feature\n",
    "        time_feat = (\n",
    "            torch.cat(\n",
    "                (\n",
    "                    past_time_features[:, self._past_length - self.context_length :, ...],\n",
    "                    future_time_features,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            if future_values is not None\n",
    "            else past_time_features[:, self._past_length - self.context_length :, ...]\n",
    "        )\n",
    "\n",
    "        # target\n",
    "        if past_observed_mask is None:\n",
    "            past_observed_mask = torch.ones_like(past_values)\n",
    "\n",
    "        context = past_values[:, -self.context_length :]\n",
    "        observed_context = past_observed_mask[:, -self.context_length :]\n",
    "        _, loc, scale = self.scaler(context, observed_context)\n",
    "\n",
    "        inputs = (\n",
    "            (torch.cat((past_values, future_values), dim=1) - loc) / scale\n",
    "            if future_values is not None\n",
    "            else (past_values - loc) / scale\n",
    "        )\n",
    "\n",
    "        # static features\n",
    "        log_abs_loc = loc.abs().log1p() if self.input_size == 1 else loc.squeeze(1).abs().log1p()\n",
    "        log_scale = scale.log() if self.input_size == 1 else scale.squeeze(1).log()\n",
    "        static_feat = torch.cat((log_abs_loc, log_scale), dim=1)\n",
    "\n",
    "        if static_real_features is not None:\n",
    "            static_feat = torch.cat((static_real_features, static_feat), dim=1)\n",
    "        if static_categorical_features is not None:\n",
    "            embedded_cat = self.embedder(static_categorical_features)\n",
    "            static_feat = torch.cat((embedded_cat, static_feat), dim=1)\n",
    "        expanded_static_feat = static_feat.unsqueeze(1).expand(-1, time_feat.shape[1], -1)\n",
    "\n",
    "        # all features\n",
    "        features = torch.cat((expanded_static_feat, time_feat), dim=-1)\n",
    "\n",
    "        # lagged features\n",
    "        subsequences_length = (\n",
    "            self.context_length + self.prediction_length\n",
    "            if future_values is not None\n",
    "            else self.context_length\n",
    "        )\n",
    "        lagged_sequence = self.get_lagged_subsequences(sequence=inputs, subsequences_length=subsequences_length)\n",
    "        lags_shape = lagged_sequence.shape\n",
    "        reshaped_lagged_sequence = lagged_sequence.reshape(lags_shape[0], lags_shape[1], -1)\n",
    "\n",
    "        if reshaped_lagged_sequence.shape[1] != time_feat.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"input length {reshaped_lagged_sequence.shape[1]} and time feature lengths {time_feat.shape[1]} does not match\"\n",
    "            )\n",
    "\n",
    "        # transformer inputs\n",
    "        transformer_inputs = torch.cat((reshaped_lagged_sequence, features), dim=-1)\n",
    "\n",
    "        return transformer_inputs, loc, scale, static_feat\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        past_time_features: Optional[torch.LongTensor] = None,\n",
    "        past_observed_mask: Optional[torch.LongTensor] = None,\n",
    "        static_categorical_features: Optional[torch.Tensor] = None,\n",
    "        static_real_features: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        future_time_features: Optional[torch.Tensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from huggingface_hub import hf_hub_download\n",
    "        >>> import torch\n",
    "        >>> from transformers import TimeSeriesTransformerModel\n",
    "        >>> file = hf_hub_download(\n",
    "        ...     repo_id=\"kashif/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    "        ... )\n",
    "        >>> batch = torch.load(file)\n",
    "        >>> model = TimeSeriesTransformerModel.from_pretrained(\"huggingface/time-series-transformer-tourism-monthly\")\n",
    "        >>> # during training, one provides both past and future values\n",
    "        >>> # as well as possible additional features\n",
    "        >>> outputs = model(\n",
    "        ...     past_values=batch[\"past_values\"],\n",
    "        ...     past_time_features=batch[\"past_time_features\"],\n",
    "        ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
    "        ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
    "        ...     static_real_features=batch[\"static_real_features\"],\n",
    "        ...     future_values=batch[\"future_values\"],\n",
    "        ...     future_time_features=batch[\"future_time_features\"],\n",
    "        ... )\n",
    "        >>> last_hidden_state = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        \n",
    "        batch_size, sequence_length, num_features = past_values.shape\n",
    "        past_values = past_values\n",
    "        past_time_features = torch.randn(batch_size, sequence_length, num_features)\n",
    "        past_observed_mask = torch.ones(batch_size, sequence_length, num_features)\n",
    "        static_categorical_features = torch.randint(0, 4, (batch_size, 2))\n",
    "        static_real_features = torch.randn(batch_size, num_features) # Changed this line to match the expected dimension\n",
    "        future_values = future_values\n",
    "        future_time_features = torch.randn(batch_size, 1, num_features)\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        transformer_inputs, loc, scale, static_feat = self.create_network_inputs(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            static_categorical_features=static_categorical_features,\n",
    "            static_real_features=static_real_features,\n",
    "            future_values=future_values,\n",
    "            future_time_features=future_time_features,\n",
    "        )\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            enc_input = transformer_inputs[:, : self.context_length, ...]\n",
    "            encoder_outputs = self.encoder(\n",
    "                inputs_embeds=enc_input,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        dec_input = transformer_inputs[:, self.context_length :, ...]\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=dec_input,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        decoder_outputs = self.W(torch.sum(decoder_outputs[0],dim=1)/decoder_outputs[0].size(1))\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input tensors\n",
    "batch_size = 5\n",
    "sequence_length = 21\n",
    "num_features = 20\n",
    "\n",
    "past_values = torch.randn(batch_size, sequence_length, num_features)\n",
    "future_values = torch.randn(batch_size, 1, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesTransformerModel(\n",
    "    num_static_categorical_features=2,\n",
    "    cardinality=[4, 4],\n",
    "    embedding_dimension=[2, 2],\n",
    "    feature_size=144,\n",
    "    encoder_layers=2,\n",
    "    encoder_layerdrop=0,\n",
    "    decoder_layers=2,\n",
    "    decoder_layerdrop=0,\n",
    "    context_length=21,\n",
    "    prediction_length=1,\n",
    "    lags_sequence=[0, 0, 0],\n",
    "    input_size=20,\n",
    "    d_model=20,\n",
    "    encoder_attention_heads=2,\n",
    "    decoder_attention_heads=2,\n",
    "    attention_dropout=0,\n",
    "    dropout=0,\n",
    "    activation_dropout=0,\n",
    "    encoder_ffn_dim=64,\n",
    "    decoder_ffn_dim=64,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    use_return_dict=False,\n",
    "    use_cache=True,\n",
    "    scaling=\"std\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    past_values=past_values,\n",
    "    future_values=future_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[0.6630],\n",
      "        [1.0944],\n",
      "        [1.2543],\n",
      "        [1.0875],\n",
      "        [0.6107]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>AOAL</th>\n",
       "      <th>AOAR</th>\n",
       "      <th>PITCH</th>\n",
       "      <th>W</th>\n",
       "      <th>MACH</th>\n",
       "      <th>AIRSPD</th>\n",
       "      <th>TEFLAPL</th>\n",
       "      <th>XIDA</th>\n",
       "      <th>T</th>\n",
       "      <th>...</th>\n",
       "      <th>OPR</th>\n",
       "      <th>OTL</th>\n",
       "      <th>OTR</th>\n",
       "      <th>VIBN1L</th>\n",
       "      <th>VIBN1R</th>\n",
       "      <th>VIBN2L</th>\n",
       "      <th>VIBN2R</th>\n",
       "      <th>FE</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>FS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-10.55</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63901.05853</td>\n",
       "      <td>0.084</td>\n",
       "      <td>56.3</td>\n",
       "      <td>5</td>\n",
       "      <td>6.855</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>23995.070700</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>577.896783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-9.84</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63900.67347</td>\n",
       "      <td>0.087</td>\n",
       "      <td>58.2</td>\n",
       "      <td>5</td>\n",
       "      <td>6.325</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>23453.775200</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1119.192283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-9.14</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>63900.23197</td>\n",
       "      <td>0.090</td>\n",
       "      <td>60.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.710</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22821.215600</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1751.751883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-8.09</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>63899.73805</td>\n",
       "      <td>0.093</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.630</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22738.950900</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>1834.016583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>63899.14333</td>\n",
       "      <td>0.096</td>\n",
       "      <td>63.9</td>\n",
       "      <td>5</td>\n",
       "      <td>5.365</td>\n",
       "      <td>20.50</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22464.769900</td>\n",
       "      <td>24572.967483</td>\n",
       "      <td>2108.197583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>11544</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.87869</td>\n",
       "      <td>0.150</td>\n",
       "      <td>56.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.950</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7668.487126</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11095.857388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>11545</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>56378.72144</td>\n",
       "      <td>0.150</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.220</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>7393.237132</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11371.107383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>11546</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>56378.57427</td>\n",
       "      <td>0.150</td>\n",
       "      <td>49.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.130</td>\n",
       "      <td>23.75</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7484.958267</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11279.386247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>11547</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.43517</td>\n",
       "      <td>0.150</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.950</td>\n",
       "      <td>24.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7652.561590</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11111.782924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>11548</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>56378.30816</td>\n",
       "      <td>0.150</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.125</td>\n",
       "      <td>24.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7474.749119</td>\n",
       "      <td>18764.344514</td>\n",
       "      <td>11289.595395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11524 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   AOAL  AOAR  PITCH            W   MACH  AIRSPD  TEFLAPL   XIDA  \\\n",
       "0          0 -10.55 -4.22  -0.53  63901.05853  0.084    56.3        5  6.855   \n",
       "1          1  -9.84 -3.87  -0.53  63900.67347  0.087    58.2        5  6.325   \n",
       "2          2  -9.14 -3.34  -0.53  63900.23197  0.090    60.1        5  5.710   \n",
       "3          3  -8.09 -3.87  -0.35  63899.73805  0.093    62.0        5  5.630   \n",
       "4          4  -7.56 -3.87  -0.35  63899.14333  0.096    63.9        5  5.365   \n",
       "...      ...    ...   ...    ...          ...    ...     ...      ...    ...   \n",
       "11519  11544  -2.81 -3.69  -0.70  56378.87869  0.150    56.0       30  3.950   \n",
       "11520  11545  -2.99 -3.69  -0.88  56378.72144  0.150    52.0       30  4.220   \n",
       "11521  11546  -2.81 -3.69  -0.88  56378.57427  0.150    49.0       30  4.130   \n",
       "11522  11547  -2.81 -3.69  -0.70  56378.43517  0.150    46.0       30  3.950   \n",
       "11523  11548  -3.16 -3.69  -0.70  56378.30816  0.150    45.0       30  4.125   \n",
       "\n",
       "           T  ...  OPR  OTL  OTR  VIBN1L  VIBN1R  VIBN2L  VIBN2R  \\\n",
       "0      20.50  ...   24  110  110    0.03    0.05    0.05    0.05   \n",
       "1      20.50  ...   26  110  110    0.03    0.05    0.07    0.05   \n",
       "2      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "3      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "4      20.50  ...   26  110  110    0.03    0.06    0.08    0.05   \n",
       "...      ...  ...  ...  ...  ...     ...     ...     ...     ...   \n",
       "11519  23.75  ...   43   98   98    0.09    0.10    0.10    0.05   \n",
       "11520  23.75  ...   31   98   98    0.10    0.10    0.12    0.07   \n",
       "11521  23.75  ...   31   98   98    0.13    0.12    0.12    0.09   \n",
       "11522  24.00  ...   31   97   97    0.13    0.15    0.18    0.09   \n",
       "11523  24.00  ...   31   97   97    0.14    0.15    0.25    0.09   \n",
       "\n",
       "                 FE          TMAX            FS  \n",
       "0      23995.070700  24572.967483    577.896783  \n",
       "1      23453.775200  24572.967483   1119.192283  \n",
       "2      22821.215600  24572.967483   1751.751883  \n",
       "3      22738.950900  24572.967483   1834.016583  \n",
       "4      22464.769900  24572.967483   2108.197583  \n",
       "...             ...           ...           ...  \n",
       "11519   7668.487126  18764.344514  11095.857388  \n",
       "11520   7393.237132  18764.344514  11371.107383  \n",
       "11521   7484.958267  18764.344514  11279.386247  \n",
       "11522   7652.561590  18764.344514  11111.782924  \n",
       "11523   7474.749119  18764.344514  11289.595395  \n",
       "\n",
       "[11524 rows x 28 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['date','AOAL','AOAR','PITCH','W','MACH','AIRSPD','TEFLAPL','XIDA',\n",
    "               'T','ALTSTD','N1L','N1R','N2L','N2R','EGTL','EGTR','OPL','OPR','OTL',\n",
    "               'OTR','VIBN1L','VIBN1R','VIBN2L','VIBN2R','FE','TMAX','FS']\n",
    "\n",
    "input=pd.read_excel('D:\\\\研究生毕设\\\\practice\\\\QAR_xunlian.xlsx',names=column_names)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['W','MACH','AIRSPD','TEFLAPL','XIDA',\n",
    "               'T','ALTSTD','N1L','N1R','N2L','N2R','EGTL','EGTR','OPL','OPR','OTL',\n",
    "               'OTR','VIBN1L','VIBN1R','VIBN2L','VIBN2R','FE']\n",
    "\n",
    "df=input[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training=df[feature_names].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11524, 22)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler=scaler.fit(df_for_training)\n",
    "df_for_training_scaled=scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=[]\n",
    "trainY=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future=1\n",
    "n_past=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_past,len(df_for_training_scaled)-n_future+1):\n",
    "    trainX.append(df_for_training_scaled[i-n_past:i,0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i:i+n_future,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape==(11504, 20, 22).\n",
      "trainY shape==(11504, 1).\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY=np.array(trainX),np.array(trainY)\n",
    "print('trainX shape=={}.'.format(trainX.shape))\n",
    "print('trainY shape=={}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX0=np.zeros((trainX.shape[0],22,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX0[i]=trainX[i].T\n",
    "trainX1=np.zeros((trainX.shape[0],21,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX1[i]=trainX0[i][0:21,:]\n",
    "trainX2=np.zeros((trainX.shape[0],1,20))\n",
    "for i in range(0,trainX.shape[0]):\n",
    "    trainX2[i]=trainX0[i][21,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX1_copy,trainX2_copy,trainY_copy=trainX1.copy(),trainX2.copy(),trainY.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(data1,data2,label):\n",
    "    randnum = np.random.randint(0, len(label))\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(data1)\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(data2)\n",
    "    np.random.seed(randnum)\n",
    "    np.random.shuffle(label)\n",
    "    return data1,data2,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1,data2,label=random_shuffle(trainX1,trainX2,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training data\n",
    "data1 = torch.from_numpy(data1).float()\n",
    "data2 = torch.from_numpy(data2).float()\n",
    "label = torch.from_numpy(label).float()\n",
    "train_dataset = TensorDataset(data1, data2, label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "min_val_acc=1000000000000000\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0 )\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6656625270843506\n",
      "Epoch: 1 Loss: 0.10405278950929642\n",
      "Epoch: 2 Loss: 0.9159541130065918\n",
      "Epoch: 3 Loss: 0.47460708022117615\n",
      "Epoch: 4 Loss: 2.5507876873016357\n",
      "Epoch: 5 Loss: 0.7818528413772583\n",
      "Epoch: 6 Loss: 1.5438241958618164\n",
      "Epoch: 7 Loss: 0.326394259929657\n",
      "Epoch: 8 Loss: 0.32449716329574585\n",
      "Epoch: 9 Loss: 0.1372503638267517\n",
      "Epoch: 10 Loss: 0.11475757509469986\n",
      "Epoch: 11 Loss: 0.09803575277328491\n",
      "Epoch: 12 Loss: 0.38802680373191833\n",
      "Epoch: 13 Loss: 1.2202632427215576\n",
      "Epoch: 14 Loss: 0.08504021167755127\n",
      "Epoch: 15 Loss: 0.2650621235370636\n",
      "Epoch: 16 Loss: 0.11031235754489899\n",
      "Epoch: 17 Loss: 0.16970551013946533\n",
      "Epoch: 18 Loss: 0.24307331442832947\n",
      "Epoch: 19 Loss: 2.4321000576019287\n",
      "Epoch: 20 Loss: 0.15363310277462006\n",
      "Epoch: 21 Loss: 0.08794854581356049\n",
      "Epoch: 22 Loss: 0.25778913497924805\n",
      "Epoch: 23 Loss: 0.11751137673854828\n",
      "Epoch: 24 Loss: 0.13151638209819794\n",
      "Epoch: 25 Loss: 0.6474249958992004\n",
      "Epoch: 26 Loss: 0.8239991068840027\n",
      "Epoch: 27 Loss: 0.2855514883995056\n",
      "Epoch: 28 Loss: 0.5737122297286987\n",
      "Epoch: 29 Loss: 0.1696523278951645\n",
      "Epoch: 30 Loss: 0.48105567693710327\n",
      "Epoch: 31 Loss: 0.3416357636451721\n",
      "Epoch: 32 Loss: 0.09292248636484146\n",
      "Epoch: 33 Loss: 0.2670416235923767\n",
      "Epoch: 34 Loss: 0.3068405091762543\n",
      "Epoch: 35 Loss: 0.7374023795127869\n",
      "Epoch: 36 Loss: 0.2402234971523285\n",
      "Epoch: 37 Loss: 0.2169620394706726\n",
      "Epoch: 38 Loss: 0.3749854862689972\n",
      "Epoch: 39 Loss: 0.31149113178253174\n",
      "Epoch: 40 Loss: 0.23765327036380768\n",
      "Epoch: 41 Loss: 0.3030107021331787\n",
      "Epoch: 42 Loss: 0.11203356832265854\n",
      "Epoch: 43 Loss: 0.38290125131607056\n",
      "Epoch: 44 Loss: 0.26835140585899353\n",
      "Epoch: 45 Loss: 0.9068647027015686\n",
      "Epoch: 46 Loss: 0.11392302066087723\n",
      "Epoch: 47 Loss: 0.2495429813861847\n",
      "Epoch: 48 Loss: 0.16316132247447968\n",
      "Epoch: 49 Loss: 0.48699915409088135\n",
      "Epoch: 50 Loss: 0.23381800949573517\n",
      "Epoch: 51 Loss: 0.09911307692527771\n",
      "Epoch: 52 Loss: 0.786598801612854\n",
      "Epoch: 53 Loss: 0.16772949695587158\n",
      "Epoch: 54 Loss: 0.6584354639053345\n",
      "Epoch: 55 Loss: 0.21065115928649902\n",
      "Epoch: 56 Loss: 0.44060349464416504\n",
      "Epoch: 57 Loss: 0.12878437340259552\n",
      "Epoch: 58 Loss: 0.722642183303833\n",
      "Epoch: 59 Loss: 0.11996728181838989\n",
      "Epoch: 60 Loss: 0.057492367923259735\n",
      "Epoch: 61 Loss: 0.11001267284154892\n",
      "Epoch: 62 Loss: 0.12288302928209305\n",
      "Epoch: 63 Loss: 0.3186817467212677\n",
      "Epoch: 64 Loss: 0.519737184047699\n",
      "Epoch: 65 Loss: 0.7164531350135803\n",
      "Epoch: 66 Loss: 0.1511867195367813\n",
      "Epoch: 67 Loss: 0.14613983035087585\n",
      "Epoch: 68 Loss: 0.1519181877374649\n",
      "Epoch: 69 Loss: 0.3348521590232849\n",
      "Epoch: 70 Loss: 0.22066429257392883\n",
      "Epoch: 71 Loss: 0.1442234069108963\n",
      "Epoch: 72 Loss: 0.2952430844306946\n",
      "Epoch: 73 Loss: 0.30875447392463684\n",
      "Epoch: 74 Loss: 0.18892070651054382\n",
      "Epoch: 75 Loss: 0.09953628480434418\n",
      "Epoch: 76 Loss: 0.7248178124427795\n",
      "Epoch: 77 Loss: 0.1003488227725029\n",
      "Epoch: 78 Loss: 0.33735257387161255\n",
      "Epoch: 79 Loss: 0.055328089743852615\n",
      "Epoch: 80 Loss: 0.09159516543149948\n",
      "Epoch: 81 Loss: 0.12185630202293396\n",
      "Epoch: 82 Loss: 0.28125131130218506\n",
      "Epoch: 83 Loss: 0.2163013219833374\n",
      "Epoch: 84 Loss: 0.0681920126080513\n",
      "Epoch: 85 Loss: 0.49418655037879944\n",
      "Epoch: 86 Loss: 0.3986666798591614\n",
      "Epoch: 87 Loss: 0.1538786143064499\n",
      "Epoch: 88 Loss: 0.35185006260871887\n",
      "Epoch: 89 Loss: 0.12707337737083435\n",
      "Epoch: 90 Loss: 0.13331171870231628\n",
      "Epoch: 91 Loss: 0.1298067271709442\n",
      "Epoch: 92 Loss: 0.2914855480194092\n",
      "Epoch: 93 Loss: 0.08037509024143219\n",
      "Epoch: 94 Loss: 0.3341583013534546\n",
      "Epoch: 95 Loss: 0.1607874184846878\n",
      "Epoch: 96 Loss: 0.25240039825439453\n",
      "Epoch: 97 Loss: 0.10403425991535187\n",
      "Epoch: 98 Loss: 0.18046529591083527\n",
      "Epoch: 99 Loss: 0.09020206332206726\n",
      "Epoch: 100 Loss: 0.05447804555296898\n",
      "Epoch: 101 Loss: 0.39184343814849854\n",
      "Epoch: 102 Loss: 0.08977818489074707\n",
      "Epoch: 103 Loss: 0.1168413758277893\n",
      "Epoch: 104 Loss: 0.49120038747787476\n",
      "Epoch: 105 Loss: 0.15249525010585785\n",
      "Epoch: 106 Loss: 0.44024187326431274\n",
      "Epoch: 107 Loss: 0.19517762959003448\n",
      "Epoch: 108 Loss: 0.4196709096431732\n",
      "Epoch: 109 Loss: 0.12417289614677429\n",
      "Epoch: 110 Loss: 0.20044757425785065\n",
      "Epoch: 111 Loss: 0.20175033807754517\n",
      "Epoch: 112 Loss: 0.11737246066331863\n",
      "Epoch: 113 Loss: 0.06811598688364029\n",
      "Epoch: 114 Loss: 0.15869250893592834\n",
      "Epoch: 115 Loss: 0.11339917778968811\n",
      "Epoch: 116 Loss: 0.3480551242828369\n",
      "Epoch: 117 Loss: 0.11526341736316681\n",
      "Epoch: 118 Loss: 0.187191441655159\n",
      "Epoch: 119 Loss: 0.21714702248573303\n",
      "Epoch: 120 Loss: 0.29548129439353943\n",
      "Epoch: 121 Loss: 0.2218029648065567\n",
      "Epoch: 122 Loss: 0.386275976896286\n",
      "Epoch: 123 Loss: 0.0896724984049797\n",
      "Epoch: 124 Loss: 0.2683941125869751\n",
      "Epoch: 125 Loss: 0.3109068274497986\n",
      "Epoch: 126 Loss: 0.23802945017814636\n",
      "Epoch: 127 Loss: 0.17937341332435608\n",
      "Epoch: 128 Loss: 0.09849254041910172\n",
      "Epoch: 129 Loss: 0.3089810609817505\n",
      "Epoch: 130 Loss: 0.35812902450561523\n",
      "Epoch: 131 Loss: 0.14708206057548523\n",
      "Epoch: 132 Loss: 0.2973353862762451\n",
      "Epoch: 133 Loss: 0.1162436231970787\n",
      "Epoch: 134 Loss: 0.22635334730148315\n",
      "Epoch: 135 Loss: 0.5354673266410828\n",
      "Epoch: 136 Loss: 0.06992103159427643\n",
      "Epoch: 137 Loss: 0.07783429324626923\n",
      "Epoch: 138 Loss: 0.09914539754390717\n",
      "Epoch: 139 Loss: 0.09903163462877274\n",
      "Epoch: 140 Loss: 0.08514980971813202\n",
      "Epoch: 141 Loss: 0.32004833221435547\n",
      "Epoch: 142 Loss: 0.09059426188468933\n",
      "Epoch: 143 Loss: 0.3468940854072571\n",
      "Epoch: 144 Loss: 0.10489161312580109\n",
      "Epoch: 145 Loss: 0.2648811936378479\n",
      "Epoch: 146 Loss: 0.13388743996620178\n",
      "Epoch: 147 Loss: 0.27914759516716003\n",
      "Epoch: 148 Loss: 0.17050009965896606\n",
      "Epoch: 149 Loss: 0.07460007071495056\n",
      "Epoch: 150 Loss: 0.13289636373519897\n",
      "Epoch: 151 Loss: 0.0879138857126236\n",
      "Epoch: 152 Loss: 0.1314142942428589\n",
      "Epoch: 153 Loss: 0.12571167945861816\n",
      "Epoch: 154 Loss: 0.09170687943696976\n",
      "Epoch: 155 Loss: 0.17157934606075287\n",
      "Epoch: 156 Loss: 0.2488081157207489\n",
      "Epoch: 157 Loss: 0.1184869259595871\n",
      "Epoch: 158 Loss: 0.23219770193099976\n",
      "Epoch: 159 Loss: 0.24756284058094025\n",
      "Epoch: 160 Loss: 0.1634482443332672\n",
      "Epoch: 161 Loss: 0.1868993192911148\n",
      "Epoch: 162 Loss: 0.19919884204864502\n",
      "Epoch: 163 Loss: 0.27081096172332764\n",
      "Epoch: 164 Loss: 0.1341703236103058\n",
      "Epoch: 165 Loss: 0.17649351060390472\n",
      "Epoch: 166 Loss: 0.11880859732627869\n",
      "Epoch: 167 Loss: 0.24384282529354095\n",
      "Epoch: 168 Loss: 0.1937856376171112\n",
      "Epoch: 169 Loss: 0.40644142031669617\n",
      "Epoch: 170 Loss: 0.12834030389785767\n",
      "Epoch: 171 Loss: 0.11941198259592056\n",
      "Epoch: 172 Loss: 0.17929279804229736\n",
      "Epoch: 173 Loss: 0.16254308819770813\n",
      "Epoch: 174 Loss: 0.20386813580989838\n",
      "Epoch: 175 Loss: 0.160978302359581\n",
      "Epoch: 176 Loss: 0.08701577037572861\n",
      "Epoch: 177 Loss: 0.10878120362758636\n",
      "Epoch: 178 Loss: 0.07709594815969467\n",
      "Epoch: 179 Loss: 0.1937035620212555\n",
      "Epoch: 180 Loss: 0.14085540175437927\n",
      "Epoch: 181 Loss: 0.0634230300784111\n",
      "Epoch: 182 Loss: 0.17164357006549835\n",
      "Epoch: 183 Loss: 0.20914806425571442\n",
      "Epoch: 184 Loss: 0.07530949264764786\n",
      "Epoch: 185 Loss: 0.3313401937484741\n",
      "Epoch: 186 Loss: 0.17621102929115295\n",
      "Epoch: 187 Loss: 0.09659874439239502\n",
      "Epoch: 188 Loss: 0.224026158452034\n",
      "Epoch: 189 Loss: 0.07539635896682739\n",
      "Epoch: 190 Loss: 0.05950227379798889\n",
      "Epoch: 191 Loss: 0.20624467730522156\n",
      "Epoch: 192 Loss: 0.1250622570514679\n",
      "Epoch: 193 Loss: 0.0910431444644928\n",
      "Epoch: 194 Loss: 0.41423919796943665\n",
      "Epoch: 195 Loss: 0.33511266112327576\n",
      "Epoch: 196 Loss: 0.08660786598920822\n",
      "Epoch: 197 Loss: 0.1771652102470398\n",
      "Epoch: 198 Loss: 0.29840683937072754\n",
      "Epoch: 199 Loss: 0.17434073984622955\n",
      "Epoch: 200 Loss: 0.11939294636249542\n",
      "Epoch: 201 Loss: 0.10809223353862762\n",
      "Epoch: 202 Loss: 0.08083340525627136\n",
      "Epoch: 203 Loss: 0.11163107305765152\n",
      "Epoch: 204 Loss: 0.09195490926504135\n",
      "Epoch: 205 Loss: 0.14242339134216309\n",
      "Epoch: 206 Loss: 0.16744212806224823\n",
      "Epoch: 207 Loss: 0.1040998101234436\n",
      "Epoch: 208 Loss: 0.13267572224140167\n",
      "Epoch: 209 Loss: 0.21440796554088593\n",
      "Epoch: 210 Loss: 0.20707084238529205\n",
      "Epoch: 211 Loss: 0.17255349457263947\n",
      "Epoch: 212 Loss: 0.1106657013297081\n",
      "Epoch: 213 Loss: 0.1576012521982193\n",
      "Epoch: 214 Loss: 0.14963045716285706\n",
      "Epoch: 215 Loss: 0.24814432859420776\n",
      "Epoch: 216 Loss: 0.11372003704309464\n",
      "Epoch: 217 Loss: 0.29420503973960876\n",
      "Epoch: 218 Loss: 0.14461830258369446\n",
      "Epoch: 219 Loss: 0.24263343214988708\n",
      "Epoch: 220 Loss: 0.11348963528871536\n",
      "Epoch: 221 Loss: 0.05371380224823952\n",
      "Epoch: 222 Loss: 0.09044285118579865\n",
      "Epoch: 223 Loss: 0.14235106110572815\n",
      "Epoch: 224 Loss: 0.3681143820285797\n",
      "Epoch: 225 Loss: 0.1669263243675232\n",
      "Epoch: 226 Loss: 0.3178231418132782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227 Loss: 0.38394778966903687\n",
      "Epoch: 228 Loss: 0.09492447972297668\n",
      "Epoch: 229 Loss: 0.08816308528184891\n",
      "Epoch: 230 Loss: 0.24163052439689636\n",
      "Epoch: 231 Loss: 0.1282612532377243\n",
      "Epoch: 232 Loss: 0.04961646720767021\n",
      "Epoch: 233 Loss: 0.23664532601833344\n",
      "Epoch: 234 Loss: 0.11092983931303024\n",
      "Epoch: 235 Loss: 1.2188022136688232\n",
      "Epoch: 236 Loss: 0.17807596921920776\n",
      "Epoch: 237 Loss: 0.05792549252510071\n",
      "Epoch: 238 Loss: 0.15984447300434113\n",
      "Epoch: 239 Loss: 0.21102061867713928\n",
      "Epoch: 240 Loss: 0.05537191033363342\n",
      "Epoch: 241 Loss: 0.0924847424030304\n",
      "Epoch: 242 Loss: 0.08779653906822205\n",
      "Epoch: 243 Loss: 0.10529648512601852\n",
      "Epoch: 244 Loss: 0.09294840693473816\n",
      "Epoch: 245 Loss: 0.05814070254564285\n",
      "Epoch: 246 Loss: 0.08042727410793304\n",
      "Epoch: 247 Loss: 0.10661929100751877\n",
      "Epoch: 248 Loss: 0.15734440088272095\n",
      "Epoch: 249 Loss: 0.056293606758117676\n",
      "Epoch: 250 Loss: 0.15454457700252533\n",
      "Epoch: 251 Loss: 0.5496593713760376\n",
      "Epoch: 252 Loss: 0.1189180463552475\n",
      "Epoch: 253 Loss: 0.19212564826011658\n",
      "Epoch: 254 Loss: 0.09466353058815002\n",
      "Epoch: 255 Loss: 0.20957574248313904\n",
      "Epoch: 256 Loss: 0.10421346873044968\n",
      "Epoch: 257 Loss: 0.14240452647209167\n",
      "Epoch: 258 Loss: 0.053308602422475815\n",
      "Epoch: 259 Loss: 0.12878474593162537\n",
      "Epoch: 260 Loss: 0.16266308724880219\n",
      "Epoch: 261 Loss: 0.22754640877246857\n",
      "Epoch: 262 Loss: 0.09832330048084259\n",
      "Epoch: 263 Loss: 0.11499945819377899\n",
      "Epoch: 264 Loss: 0.2607002556324005\n",
      "Epoch: 265 Loss: 0.06569726765155792\n",
      "Epoch: 266 Loss: 0.15601736307144165\n",
      "Epoch: 267 Loss: 0.1391659528017044\n",
      "Epoch: 268 Loss: 0.26295509934425354\n",
      "Epoch: 269 Loss: 0.11177562922239304\n",
      "Epoch: 270 Loss: 0.10734796524047852\n",
      "Epoch: 271 Loss: 0.08437901735305786\n",
      "Epoch: 272 Loss: 0.11875908076763153\n",
      "Epoch: 273 Loss: 0.09339336305856705\n",
      "Epoch: 274 Loss: 0.1447981297969818\n",
      "Epoch: 275 Loss: 0.14251437783241272\n",
      "Epoch: 276 Loss: 0.06637581437826157\n",
      "Epoch: 277 Loss: 0.2623935341835022\n",
      "Epoch: 278 Loss: 0.13064369559288025\n",
      "Epoch: 279 Loss: 0.15813857316970825\n",
      "Epoch: 280 Loss: 0.12032429873943329\n",
      "Epoch: 281 Loss: 0.12910348176956177\n",
      "Epoch: 282 Loss: 0.21426290273666382\n",
      "Epoch: 283 Loss: 0.10259906947612762\n",
      "Epoch: 284 Loss: 0.08814410120248795\n",
      "Epoch: 285 Loss: 0.1310020536184311\n",
      "Epoch: 286 Loss: 0.05825001746416092\n",
      "Epoch: 287 Loss: 0.07417576014995575\n",
      "Epoch: 288 Loss: 0.1114504411816597\n",
      "Epoch: 289 Loss: 0.12704002857208252\n",
      "Epoch: 290 Loss: 0.19655899703502655\n",
      "Epoch: 291 Loss: 0.18861165642738342\n",
      "Epoch: 292 Loss: 0.11948823928833008\n",
      "Epoch: 293 Loss: 0.11494629830121994\n",
      "Epoch: 294 Loss: 0.06776627153158188\n",
      "Epoch: 295 Loss: 0.1030661091208458\n",
      "Epoch: 296 Loss: 0.04796016588807106\n",
      "Epoch: 297 Loss: 0.07004532963037491\n",
      "Epoch: 298 Loss: 0.18858590722084045\n",
      "Epoch: 299 Loss: 0.1380949765443802\n",
      "Epoch: 300 Loss: 0.13291281461715698\n",
      "Epoch: 301 Loss: 0.12451595813035965\n",
      "Epoch: 302 Loss: 0.06497234106063843\n",
      "Epoch: 303 Loss: 0.09724215418100357\n",
      "Epoch: 304 Loss: 0.2558744251728058\n",
      "Epoch: 305 Loss: 0.133527010679245\n",
      "Epoch: 306 Loss: 0.14814676344394684\n",
      "Epoch: 307 Loss: 0.13442562520503998\n",
      "Epoch: 308 Loss: 0.08721517026424408\n",
      "Epoch: 309 Loss: 0.16018113493919373\n",
      "Epoch: 310 Loss: 0.10162044316530228\n",
      "Epoch: 311 Loss: 0.11039793491363525\n",
      "Epoch: 312 Loss: 0.05678771436214447\n",
      "Epoch: 313 Loss: 0.1812470257282257\n",
      "Epoch: 314 Loss: 0.08636950701475143\n",
      "Epoch: 315 Loss: 0.10205191373825073\n",
      "Epoch: 316 Loss: 0.0829392671585083\n",
      "Epoch: 317 Loss: 0.07470621168613434\n",
      "Epoch: 318 Loss: 0.06934762001037598\n",
      "Epoch: 319 Loss: 0.12934422492980957\n",
      "Epoch: 320 Loss: 0.06180119141936302\n",
      "Epoch: 321 Loss: 0.06613091379404068\n",
      "Epoch: 322 Loss: 0.16341492533683777\n",
      "Epoch: 323 Loss: 0.3211120069026947\n",
      "Epoch: 324 Loss: 0.08935587108135223\n",
      "Epoch: 325 Loss: 0.047880351543426514\n",
      "Epoch: 326 Loss: 0.2667056918144226\n",
      "Epoch: 327 Loss: 0.07133598625659943\n",
      "Epoch: 328 Loss: 0.21552740037441254\n",
      "Epoch: 329 Loss: 0.13713903725147247\n",
      "Epoch: 330 Loss: 0.14611829817295074\n",
      "Epoch: 331 Loss: 0.06086129695177078\n",
      "Epoch: 332 Loss: 0.1256101131439209\n",
      "Epoch: 333 Loss: 0.09023749083280563\n",
      "Epoch: 334 Loss: 0.04578579589724541\n",
      "Epoch: 335 Loss: 0.10801547020673752\n",
      "Epoch: 336 Loss: 0.26467734575271606\n",
      "Epoch: 337 Loss: 0.10559295117855072\n",
      "Epoch: 338 Loss: 0.08831609040498734\n",
      "Epoch: 339 Loss: 0.15282557904720306\n",
      "Epoch: 340 Loss: 0.15643565356731415\n",
      "Epoch: 341 Loss: 0.22095642983913422\n",
      "Epoch: 342 Loss: 0.21062059700489044\n",
      "Epoch: 343 Loss: 0.04228898882865906\n",
      "Epoch: 344 Loss: 0.21737128496170044\n",
      "Epoch: 345 Loss: 0.05656972527503967\n",
      "Epoch: 346 Loss: 0.2027602642774582\n",
      "Epoch: 347 Loss: 0.3909487724304199\n",
      "Epoch: 348 Loss: 0.3518660068511963\n",
      "Epoch: 349 Loss: 0.05833491310477257\n",
      "Epoch: 350 Loss: 0.06686575710773468\n",
      "Epoch: 351 Loss: 0.16507001221179962\n",
      "Epoch: 352 Loss: 0.07322853058576584\n",
      "Epoch: 353 Loss: 0.12131419032812119\n",
      "Epoch: 354 Loss: 0.24349471926689148\n",
      "Epoch: 355 Loss: 0.27054235339164734\n",
      "Epoch: 356 Loss: 0.11826469749212265\n",
      "Epoch: 357 Loss: 0.2577202320098877\n",
      "Epoch: 358 Loss: 0.2908704876899719\n",
      "Epoch: 359 Loss: 0.06694824248552322\n",
      "Epoch: 360 Loss: 0.09469024091959\n",
      "Epoch: 361 Loss: 0.4310721158981323\n",
      "Epoch: 362 Loss: 0.16772127151489258\n",
      "Epoch: 363 Loss: 0.08659281581640244\n",
      "Epoch: 364 Loss: 0.08929542452096939\n",
      "Epoch: 365 Loss: 0.20931319892406464\n",
      "Epoch: 366 Loss: 0.07566656917333603\n",
      "Epoch: 367 Loss: 0.17047442495822906\n",
      "Epoch: 368 Loss: 0.07081760466098785\n",
      "Epoch: 369 Loss: 0.03431449085474014\n",
      "Epoch: 370 Loss: 0.10548904538154602\n",
      "Epoch: 371 Loss: 0.12463313341140747\n",
      "Epoch: 372 Loss: 0.09551745653152466\n",
      "Epoch: 373 Loss: 0.11047936975955963\n",
      "Epoch: 374 Loss: 0.08385354280471802\n",
      "Epoch: 375 Loss: 0.09453535825014114\n",
      "Epoch: 376 Loss: 0.13059642910957336\n",
      "Epoch: 377 Loss: 0.18203900754451752\n",
      "Epoch: 378 Loss: 0.298519104719162\n",
      "Epoch: 379 Loss: 0.09722980856895447\n",
      "Epoch: 380 Loss: 0.29499298334121704\n",
      "Epoch: 381 Loss: 0.05333187058568001\n",
      "Epoch: 382 Loss: 0.18418726325035095\n",
      "Epoch: 383 Loss: 0.21768413484096527\n",
      "Epoch: 384 Loss: 0.12518557906150818\n",
      "Epoch: 385 Loss: 0.14608797430992126\n",
      "Epoch: 386 Loss: 0.09561561793088913\n",
      "Epoch: 387 Loss: 0.07951420545578003\n",
      "Epoch: 388 Loss: 0.7951310276985168\n",
      "Epoch: 389 Loss: 0.1639939546585083\n",
      "Epoch: 390 Loss: 0.10593117028474808\n",
      "Epoch: 391 Loss: 0.11014551669359207\n",
      "Epoch: 392 Loss: 0.11098620295524597\n",
      "Epoch: 393 Loss: 0.16465595364570618\n",
      "Epoch: 394 Loss: 0.15561853349208832\n",
      "Epoch: 395 Loss: 0.23335695266723633\n",
      "Epoch: 396 Loss: 0.15863195061683655\n",
      "Epoch: 397 Loss: 0.07757376879453659\n",
      "Epoch: 398 Loss: 0.15842294692993164\n",
      "Epoch: 399 Loss: 0.04394068941473961\n",
      "Epoch: 400 Loss: 0.08826304972171783\n",
      "Epoch: 401 Loss: 0.21006500720977783\n",
      "Epoch: 402 Loss: 0.3274152874946594\n",
      "Epoch: 403 Loss: 0.12704682350158691\n",
      "Epoch: 404 Loss: 0.10467402637004852\n",
      "Epoch: 405 Loss: 0.05904657766222954\n",
      "Epoch: 406 Loss: 0.17491084337234497\n",
      "Epoch: 407 Loss: 0.09298714995384216\n",
      "Epoch: 408 Loss: 0.05336971580982208\n",
      "Epoch: 409 Loss: 0.03871574252843857\n",
      "Epoch: 410 Loss: 0.2656034231185913\n",
      "Epoch: 411 Loss: 0.08914843946695328\n",
      "Epoch: 412 Loss: 0.13463656604290009\n",
      "Epoch: 413 Loss: 0.16524341702461243\n",
      "Epoch: 414 Loss: 0.11365658789873123\n",
      "Epoch: 415 Loss: 0.06977562606334686\n",
      "Epoch: 416 Loss: 0.29013341665267944\n",
      "Epoch: 417 Loss: 0.14717665314674377\n",
      "Epoch: 418 Loss: 0.1691512167453766\n",
      "Epoch: 419 Loss: 0.22633196413516998\n",
      "Epoch: 420 Loss: 0.14452460408210754\n",
      "Epoch: 421 Loss: 0.09781514108181\n",
      "Epoch: 422 Loss: 0.07788561284542084\n",
      "Epoch: 423 Loss: 0.086380735039711\n",
      "Epoch: 424 Loss: 0.21447697281837463\n",
      "Epoch: 425 Loss: 0.20177710056304932\n",
      "Epoch: 426 Loss: 0.05549032241106033\n",
      "Epoch: 427 Loss: 0.18078461289405823\n",
      "Epoch: 428 Loss: 0.05831487476825714\n",
      "Epoch: 429 Loss: 0.17014624178409576\n",
      "Epoch: 430 Loss: 0.07765182852745056\n",
      "Epoch: 431 Loss: 0.09387169033288956\n",
      "Epoch: 432 Loss: 0.08216224610805511\n",
      "Epoch: 433 Loss: 0.07083995640277863\n",
      "Epoch: 434 Loss: 0.14267165958881378\n",
      "Epoch: 435 Loss: 0.08282610028982162\n",
      "Epoch: 436 Loss: 0.18701022863388062\n",
      "Epoch: 437 Loss: 0.18703904747962952\n",
      "Epoch: 438 Loss: 0.21111451089382172\n",
      "Epoch: 439 Loss: 0.19600754976272583\n",
      "Epoch: 440 Loss: 0.2923029065132141\n",
      "Epoch: 441 Loss: 0.20716077089309692\n",
      "Epoch: 442 Loss: 0.2275678515434265\n",
      "Epoch: 443 Loss: 0.14674194157123566\n",
      "Epoch: 444 Loss: 0.18298612534999847\n",
      "Epoch: 445 Loss: 0.29025498032569885\n",
      "Epoch: 446 Loss: 0.08617381751537323\n",
      "Epoch: 447 Loss: 0.14517918229103088\n",
      "Epoch: 448 Loss: 0.11851649731397629\n",
      "Epoch: 449 Loss: 0.07332336157560349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450 Loss: 0.14106149971485138\n",
      "Epoch: 451 Loss: 0.1906728893518448\n",
      "Epoch: 452 Loss: 0.07284166663885117\n",
      "Epoch: 453 Loss: 0.1216944009065628\n",
      "Epoch: 454 Loss: 0.13243769109249115\n",
      "Epoch: 455 Loss: 0.09977773576974869\n",
      "Epoch: 456 Loss: 0.21886268258094788\n",
      "Epoch: 457 Loss: 0.09571627527475357\n",
      "Epoch: 458 Loss: 0.056375615298748016\n",
      "Epoch: 459 Loss: 0.14501598477363586\n",
      "Epoch: 460 Loss: 0.10755225270986557\n",
      "Epoch: 461 Loss: 0.778139591217041\n",
      "Epoch: 462 Loss: 0.12846681475639343\n",
      "Epoch: 463 Loss: 0.0725259780883789\n",
      "Epoch: 464 Loss: 0.049238234758377075\n",
      "Epoch: 465 Loss: 0.2840619683265686\n",
      "Epoch: 466 Loss: 0.1565830260515213\n",
      "Epoch: 467 Loss: 0.08338480442762375\n",
      "Epoch: 468 Loss: 0.1801365613937378\n",
      "Epoch: 469 Loss: 0.07889358699321747\n",
      "Epoch: 470 Loss: 0.06844250857830048\n",
      "Epoch: 471 Loss: 0.15005263686180115\n",
      "Epoch: 472 Loss: 0.08686958998441696\n",
      "Epoch: 473 Loss: 0.08291454613208771\n",
      "Epoch: 474 Loss: 0.04821421205997467\n",
      "Epoch: 475 Loss: 0.15608038008213043\n",
      "Epoch: 476 Loss: 0.0519983284175396\n",
      "Epoch: 477 Loss: 0.1831282079219818\n",
      "Epoch: 478 Loss: 0.08099303394556046\n",
      "Epoch: 479 Loss: 0.2700575590133667\n",
      "Epoch: 480 Loss: 0.10017107427120209\n",
      "Epoch: 481 Loss: 0.08961862325668335\n",
      "Epoch: 482 Loss: 0.046065378934144974\n",
      "Epoch: 483 Loss: 0.1045384556055069\n",
      "Epoch: 484 Loss: 0.05494380369782448\n",
      "Epoch: 485 Loss: 0.08799529820680618\n",
      "Epoch: 486 Loss: 0.0768846869468689\n",
      "Epoch: 487 Loss: 0.15561704337596893\n",
      "Epoch: 488 Loss: 0.11699595302343369\n",
      "Epoch: 489 Loss: 0.12270095944404602\n",
      "Epoch: 490 Loss: 0.10497454553842545\n",
      "Epoch: 491 Loss: 0.23796230554580688\n",
      "Epoch: 492 Loss: 0.11355835199356079\n",
      "Epoch: 493 Loss: 0.10722506046295166\n",
      "Epoch: 494 Loss: 0.09117302298545837\n",
      "Epoch: 495 Loss: 0.08066077530384064\n",
      "Epoch: 496 Loss: 0.23523098230361938\n",
      "Epoch: 497 Loss: 0.08880694210529327\n",
      "Epoch: 498 Loss: 0.10167669504880905\n",
      "Epoch: 499 Loss: 0.11660066246986389\n",
      "Epoch: 500 Loss: 0.1430979073047638\n",
      "Epoch: 501 Loss: 0.21145544946193695\n",
      "Epoch: 502 Loss: 0.10723724961280823\n",
      "Epoch: 503 Loss: 0.19401051104068756\n",
      "Epoch: 504 Loss: 0.12188251316547394\n",
      "Epoch: 505 Loss: 0.20412132143974304\n",
      "Epoch: 506 Loss: 0.7043386101722717\n",
      "Epoch: 507 Loss: 0.09474671632051468\n",
      "Epoch: 508 Loss: 0.161343052983284\n",
      "Epoch: 509 Loss: 0.05072011053562164\n",
      "Epoch: 510 Loss: 0.10415871441364288\n",
      "Epoch: 511 Loss: 0.11753387749195099\n",
      "Epoch: 512 Loss: 0.045807868242263794\n",
      "Epoch: 513 Loss: 0.06819445639848709\n",
      "Epoch: 514 Loss: 0.2166472226381302\n",
      "Epoch: 515 Loss: 0.10040321201086044\n",
      "Epoch: 516 Loss: 0.0717056393623352\n",
      "Epoch: 517 Loss: 0.2322540581226349\n",
      "Epoch: 518 Loss: 0.1588064283132553\n",
      "Epoch: 519 Loss: 0.16851486265659332\n",
      "Epoch: 520 Loss: 0.14998534321784973\n",
      "Epoch: 521 Loss: 0.07822519540786743\n",
      "Epoch: 522 Loss: 0.07116209715604782\n",
      "Epoch: 523 Loss: 0.16195012629032135\n",
      "Epoch: 524 Loss: 0.12356281280517578\n",
      "Epoch: 525 Loss: 0.09195215255022049\n",
      "Epoch: 526 Loss: 0.0471646748483181\n",
      "Epoch: 527 Loss: 0.11163454502820969\n",
      "Epoch: 528 Loss: 0.21336637437343597\n",
      "Epoch: 529 Loss: 0.2661859691143036\n",
      "Epoch: 530 Loss: 0.23748137056827545\n",
      "Epoch: 531 Loss: 0.10209868848323822\n",
      "Epoch: 532 Loss: 0.049437131732702255\n",
      "Epoch: 533 Loss: 0.09562951326370239\n",
      "Epoch: 534 Loss: 0.1254364550113678\n",
      "Epoch: 535 Loss: 0.05068989098072052\n",
      "Epoch: 536 Loss: 0.3426257073879242\n",
      "Epoch: 537 Loss: 0.21323159337043762\n",
      "Epoch: 538 Loss: 0.25991207361221313\n",
      "Epoch: 539 Loss: 0.19743835926055908\n",
      "Epoch: 540 Loss: 0.10819114744663239\n",
      "Epoch: 541 Loss: 0.11979088932275772\n",
      "Epoch: 542 Loss: 0.18128103017807007\n",
      "Epoch: 543 Loss: 0.07341811060905457\n",
      "Epoch: 544 Loss: 0.2666100561618805\n",
      "Epoch: 545 Loss: 0.2523799240589142\n",
      "Epoch: 546 Loss: 0.08708894997835159\n",
      "Epoch: 547 Loss: 0.12213248759508133\n",
      "Epoch: 548 Loss: 0.25621429085731506\n",
      "Epoch: 549 Loss: 0.03681021183729172\n",
      "Epoch: 550 Loss: 0.18441975116729736\n",
      "Epoch: 551 Loss: 0.10569646209478378\n",
      "Epoch: 552 Loss: 0.06073586642742157\n",
      "Epoch: 553 Loss: 0.05728711932897568\n",
      "Epoch: 554 Loss: 0.07590574771165848\n",
      "Epoch: 555 Loss: 0.04245733842253685\n",
      "Epoch: 556 Loss: 0.06474608182907104\n",
      "Epoch: 557 Loss: 0.27343904972076416\n",
      "Epoch: 558 Loss: 0.06451873481273651\n",
      "Epoch: 559 Loss: 0.14072665572166443\n",
      "Epoch: 560 Loss: 0.3323000967502594\n",
      "Epoch: 561 Loss: 0.16789069771766663\n",
      "Epoch: 562 Loss: 0.09512685984373093\n",
      "Epoch: 563 Loss: 0.8336496949195862\n",
      "Epoch: 564 Loss: 0.07916122674942017\n",
      "Epoch: 565 Loss: 0.10830822587013245\n",
      "Epoch: 566 Loss: 0.11510913074016571\n",
      "Epoch: 567 Loss: 0.04068050533533096\n",
      "Epoch: 568 Loss: 0.1294475495815277\n",
      "Epoch: 569 Loss: 0.08826703578233719\n",
      "Epoch: 570 Loss: 0.3009468913078308\n",
      "Epoch: 571 Loss: 0.11292166262865067\n",
      "Epoch: 572 Loss: 0.04853131249547005\n",
      "Epoch: 573 Loss: 0.0877266377210617\n",
      "Epoch: 574 Loss: 0.04885377734899521\n",
      "Epoch: 575 Loss: 0.05151320621371269\n",
      "Epoch: 576 Loss: 0.1569618284702301\n",
      "Epoch: 577 Loss: 0.047976054251194\n",
      "Epoch: 578 Loss: 0.15922963619232178\n",
      "Epoch: 579 Loss: 0.027740569785237312\n",
      "Epoch: 580 Loss: 0.1630292683839798\n",
      "Epoch: 581 Loss: 0.072134830057621\n",
      "Epoch: 582 Loss: 0.07361071556806564\n",
      "Epoch: 583 Loss: 0.19171443581581116\n",
      "Epoch: 584 Loss: 0.05442623049020767\n",
      "Epoch: 585 Loss: 0.3864903748035431\n",
      "Epoch: 586 Loss: 0.09036921709775925\n",
      "Epoch: 587 Loss: 0.1411229372024536\n",
      "Epoch: 588 Loss: 0.12448977679014206\n",
      "Epoch: 589 Loss: 0.05145552009344101\n",
      "Epoch: 590 Loss: 0.08634671568870544\n",
      "Epoch: 591 Loss: 0.10052283108234406\n",
      "Epoch: 592 Loss: 0.09230910986661911\n",
      "Epoch: 593 Loss: 0.06928031146526337\n",
      "Epoch: 594 Loss: 0.14355409145355225\n",
      "Epoch: 595 Loss: 0.130685955286026\n",
      "Epoch: 596 Loss: 0.20991607010364532\n",
      "Epoch: 597 Loss: 0.16260185837745667\n",
      "Epoch: 598 Loss: 0.1126735582947731\n",
      "Epoch: 599 Loss: 0.2887992262840271\n",
      "Epoch: 600 Loss: 0.09656944125890732\n",
      "Epoch: 601 Loss: 0.06714111566543579\n",
      "Epoch: 602 Loss: 0.17303982377052307\n",
      "Epoch: 603 Loss: 0.060716405510902405\n",
      "Epoch: 604 Loss: 0.09162143617868423\n",
      "Epoch: 605 Loss: 0.053337328135967255\n",
      "Epoch: 606 Loss: 0.10002118349075317\n",
      "Epoch: 607 Loss: 0.08046961575746536\n",
      "Epoch: 608 Loss: 0.08857700228691101\n",
      "Epoch: 609 Loss: 0.06566086411476135\n",
      "Epoch: 610 Loss: 0.06190432235598564\n",
      "Epoch: 611 Loss: 0.07505980134010315\n",
      "Epoch: 612 Loss: 0.11138129234313965\n",
      "Epoch: 613 Loss: 0.09890128672122955\n",
      "Epoch: 614 Loss: 0.16932529211044312\n",
      "Epoch: 615 Loss: 0.2729329466819763\n",
      "Epoch: 616 Loss: 0.20377099514007568\n",
      "Epoch: 617 Loss: 0.14600658416748047\n",
      "Epoch: 618 Loss: 0.08243462443351746\n",
      "Epoch: 619 Loss: 0.08967278897762299\n",
      "Epoch: 620 Loss: 0.1166466549038887\n",
      "Epoch: 621 Loss: 0.04403645545244217\n",
      "Epoch: 622 Loss: 0.13129058480262756\n",
      "Epoch: 623 Loss: 0.0705147311091423\n",
      "Epoch: 624 Loss: 0.16673199832439423\n",
      "Epoch: 625 Loss: 0.07258967310190201\n",
      "Epoch: 626 Loss: 0.0834692120552063\n",
      "Epoch: 627 Loss: 0.2174738049507141\n",
      "Epoch: 628 Loss: 0.19053317606449127\n",
      "Epoch: 629 Loss: 0.2072603851556778\n",
      "Epoch: 630 Loss: 0.047934092581272125\n",
      "Epoch: 631 Loss: 0.1651734709739685\n",
      "Epoch: 632 Loss: 0.21350333094596863\n",
      "Epoch: 633 Loss: 0.152408167719841\n",
      "Epoch: 634 Loss: 0.14156094193458557\n",
      "Epoch: 635 Loss: 0.09652739763259888\n",
      "Epoch: 636 Loss: 0.13038957118988037\n",
      "Epoch: 637 Loss: 0.11886392533779144\n",
      "Epoch: 638 Loss: 0.04579102247953415\n",
      "Epoch: 639 Loss: 0.11813376843929291\n",
      "Epoch: 640 Loss: 0.29214543104171753\n",
      "Epoch: 641 Loss: 0.05541019141674042\n",
      "Epoch: 642 Loss: 0.13197924196720123\n",
      "Epoch: 643 Loss: 0.20946770906448364\n",
      "Epoch: 644 Loss: 0.06353557854890823\n",
      "Epoch: 645 Loss: 0.17199403047561646\n",
      "Epoch: 646 Loss: 0.07208836823701859\n",
      "Epoch: 647 Loss: 0.047824181616306305\n",
      "Epoch: 648 Loss: 0.036156874150037766\n",
      "Epoch: 649 Loss: 0.06726360321044922\n",
      "Epoch: 650 Loss: 0.12270182371139526\n",
      "Epoch: 651 Loss: 0.046171464025974274\n",
      "Epoch: 652 Loss: 0.08874110877513885\n",
      "Epoch: 653 Loss: 0.05025462806224823\n",
      "Epoch: 654 Loss: 0.10575602948665619\n",
      "Epoch: 655 Loss: 0.05299711599946022\n",
      "Epoch: 656 Loss: 0.11565756052732468\n",
      "Epoch: 657 Loss: 0.08743901550769806\n",
      "Epoch: 658 Loss: 0.07719112932682037\n",
      "Epoch: 659 Loss: 0.08699611574411392\n",
      "Epoch: 660 Loss: 0.11831435561180115\n",
      "Epoch: 661 Loss: 0.16851991415023804\n",
      "Epoch: 662 Loss: 0.1972033828496933\n",
      "Epoch: 663 Loss: 0.04595767334103584\n",
      "Epoch: 664 Loss: 0.16405928134918213\n",
      "Epoch: 665 Loss: 0.10639391094446182\n",
      "Epoch: 666 Loss: 0.10728880763053894\n",
      "Epoch: 667 Loss: 0.23695333302021027\n",
      "Epoch: 668 Loss: 0.07775759696960449\n",
      "Epoch: 669 Loss: 0.07956948131322861\n",
      "Epoch: 670 Loss: 0.18580740690231323\n",
      "Epoch: 671 Loss: 0.1671002209186554\n",
      "Epoch: 672 Loss: 0.0989900454878807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 673 Loss: 0.0877896398305893\n",
      "Epoch: 674 Loss: 0.07196149230003357\n",
      "Epoch: 675 Loss: 0.2806103825569153\n",
      "Epoch: 676 Loss: 0.11602415889501572\n",
      "Epoch: 677 Loss: 0.09437385946512222\n",
      "Epoch: 678 Loss: 0.053729549050331116\n",
      "Epoch: 679 Loss: 0.05615952983498573\n",
      "Epoch: 680 Loss: 0.08818919956684113\n",
      "Epoch: 681 Loss: 0.21957671642303467\n",
      "Epoch: 682 Loss: 0.06797349452972412\n",
      "Epoch: 683 Loss: 0.04446768760681152\n",
      "Epoch: 684 Loss: 0.09584368765354156\n",
      "Epoch: 685 Loss: 0.11364240944385529\n",
      "Epoch: 686 Loss: 0.28875717520713806\n",
      "Epoch: 687 Loss: 0.12047606706619263\n",
      "Epoch: 688 Loss: 0.22003328800201416\n",
      "Epoch: 689 Loss: 0.11964168399572372\n",
      "Epoch: 690 Loss: 0.16294337809085846\n",
      "Epoch: 691 Loss: 0.053326111286878586\n",
      "Epoch: 692 Loss: 0.09236115217208862\n",
      "Epoch: 693 Loss: 0.17253071069717407\n",
      "Epoch: 694 Loss: 0.05340537056326866\n",
      "Epoch: 695 Loss: 0.06310539692640305\n",
      "Epoch: 696 Loss: 0.05924861505627632\n",
      "Epoch: 697 Loss: 0.1533517986536026\n",
      "Epoch: 698 Loss: 0.0742613896727562\n",
      "Epoch: 699 Loss: 0.20601995289325714\n",
      "Epoch: 700 Loss: 0.03741244971752167\n",
      "Epoch: 701 Loss: 0.0964001715183258\n",
      "Epoch: 702 Loss: 0.18478074669837952\n",
      "Epoch: 703 Loss: 0.04618380591273308\n",
      "Epoch: 704 Loss: 0.0889069139957428\n",
      "Epoch: 705 Loss: 0.17155982553958893\n",
      "Epoch: 706 Loss: 0.10055489093065262\n",
      "Epoch: 707 Loss: 0.11045016348361969\n",
      "Epoch: 708 Loss: 0.06393199414014816\n",
      "Epoch: 709 Loss: 0.1505419909954071\n",
      "Epoch: 710 Loss: 0.31324928998947144\n",
      "Epoch: 711 Loss: 0.22215470671653748\n",
      "Epoch: 712 Loss: 0.24431155622005463\n",
      "Epoch: 713 Loss: 0.14702360332012177\n",
      "Epoch: 714 Loss: 0.06832584738731384\n",
      "Epoch: 715 Loss: 0.08070706576108932\n",
      "Epoch: 716 Loss: 0.041370127350091934\n",
      "Epoch: 717 Loss: 0.14876462519168854\n",
      "Epoch: 718 Loss: 0.1456165611743927\n",
      "Epoch: 719 Loss: 0.11611417680978775\n",
      "Epoch: 720 Loss: 0.041583556681871414\n",
      "Epoch: 721 Loss: 0.045000895857810974\n",
      "Epoch: 722 Loss: 0.0888594388961792\n",
      "Epoch: 723 Loss: 0.09500762820243835\n",
      "Epoch: 724 Loss: 0.08225409686565399\n",
      "Epoch: 725 Loss: 0.09639951586723328\n",
      "Epoch: 726 Loss: 0.04488946497440338\n",
      "Epoch: 727 Loss: 0.05429136008024216\n",
      "Epoch: 728 Loss: 0.07409472018480301\n",
      "Epoch: 729 Loss: 0.038315534591674805\n",
      "Epoch: 730 Loss: 0.10771844536066055\n",
      "Epoch: 731 Loss: 0.17658205330371857\n",
      "Epoch: 732 Loss: 0.042799800634384155\n",
      "Epoch: 733 Loss: 0.2000817507505417\n",
      "Epoch: 734 Loss: 0.13893122971057892\n",
      "Epoch: 735 Loss: 0.24751818180084229\n",
      "Epoch: 736 Loss: 0.10874039679765701\n",
      "Epoch: 737 Loss: 0.09149997681379318\n",
      "Epoch: 738 Loss: 0.041810810565948486\n",
      "Epoch: 739 Loss: 0.19445045292377472\n",
      "Epoch: 740 Loss: 0.10439778864383698\n",
      "Epoch: 741 Loss: 0.23364900052547455\n",
      "Epoch: 742 Loss: 0.1657993495464325\n",
      "Epoch: 743 Loss: 0.0834471806883812\n",
      "Epoch: 744 Loss: 0.07691949605941772\n",
      "Epoch: 745 Loss: 0.19388125836849213\n",
      "Epoch: 746 Loss: 0.27445611357688904\n",
      "Epoch: 747 Loss: 0.0697382390499115\n",
      "Epoch: 748 Loss: 0.07903745770454407\n",
      "Epoch: 749 Loss: 0.08559872210025787\n",
      "Epoch: 750 Loss: 0.11703833192586899\n",
      "Epoch: 751 Loss: 0.11814757436513901\n",
      "Epoch: 752 Loss: 0.08661259710788727\n",
      "Epoch: 753 Loss: 0.1349448412656784\n",
      "Epoch: 754 Loss: 0.13957908749580383\n",
      "Epoch: 755 Loss: 0.032326411455869675\n",
      "Epoch: 756 Loss: 0.055775441229343414\n",
      "Epoch: 757 Loss: 0.05689186602830887\n",
      "Epoch: 758 Loss: 0.16104502975940704\n",
      "Epoch: 759 Loss: 0.25182414054870605\n",
      "Epoch: 760 Loss: 0.09029988944530487\n",
      "Epoch: 761 Loss: 0.34190770983695984\n",
      "Epoch: 762 Loss: 0.10545619577169418\n",
      "Epoch: 763 Loss: 0.09814951568841934\n",
      "Epoch: 764 Loss: 0.10942473262548447\n",
      "Epoch: 765 Loss: 0.16062042117118835\n",
      "Epoch: 766 Loss: 0.12699291110038757\n",
      "Epoch: 767 Loss: 0.2859758734703064\n",
      "Epoch: 768 Loss: 0.09374526143074036\n",
      "Epoch: 769 Loss: 0.2387615144252777\n",
      "Epoch: 770 Loss: 0.05487030744552612\n",
      "Epoch: 771 Loss: 0.12046194821596146\n",
      "Epoch: 772 Loss: 0.037016794085502625\n",
      "Epoch: 773 Loss: 0.10108374059200287\n",
      "Epoch: 774 Loss: 0.20194560289382935\n",
      "Epoch: 775 Loss: 0.07388129085302353\n",
      "Epoch: 776 Loss: 0.24098211526870728\n",
      "Epoch: 777 Loss: 0.07086329162120819\n",
      "Epoch: 778 Loss: 0.04481959342956543\n",
      "Epoch: 779 Loss: 0.143823504447937\n",
      "Epoch: 780 Loss: 0.054084412753582\n",
      "Epoch: 781 Loss: 0.09878046065568924\n",
      "Epoch: 782 Loss: 0.051944464445114136\n",
      "Epoch: 783 Loss: 0.061339303851127625\n",
      "Epoch: 784 Loss: 0.09549285471439362\n",
      "Epoch: 785 Loss: 0.2392972856760025\n",
      "Epoch: 786 Loss: 0.1791742593050003\n",
      "Epoch: 787 Loss: 0.17496901750564575\n",
      "Epoch: 788 Loss: 0.05064377188682556\n",
      "Epoch: 789 Loss: 0.03609750419855118\n",
      "Epoch: 790 Loss: 0.22276417911052704\n",
      "Epoch: 791 Loss: 0.16517722606658936\n",
      "Epoch: 792 Loss: 0.16294193267822266\n",
      "Epoch: 793 Loss: 0.18048407137393951\n",
      "Epoch: 794 Loss: 0.06417389959096909\n",
      "Epoch: 795 Loss: 0.07570672035217285\n",
      "Epoch: 796 Loss: 0.06974981725215912\n",
      "Epoch: 797 Loss: 0.20253637433052063\n",
      "Epoch: 798 Loss: 0.06877833604812622\n",
      "Epoch: 799 Loss: 0.09589450061321259\n",
      "Epoch: 800 Loss: 0.11153679341077805\n",
      "Epoch: 801 Loss: 0.25416529178619385\n",
      "Epoch: 802 Loss: 0.11375033110380173\n",
      "Epoch: 803 Loss: 0.30420786142349243\n",
      "Epoch: 804 Loss: 0.08260668069124222\n",
      "Epoch: 805 Loss: 0.07138220220804214\n",
      "Epoch: 806 Loss: 0.060249123722314835\n",
      "Epoch: 807 Loss: 0.06084231659770012\n",
      "Epoch: 808 Loss: 0.06744983047246933\n",
      "Epoch: 809 Loss: 0.0985734835267067\n",
      "Epoch: 810 Loss: 0.09747997671365738\n",
      "Epoch: 811 Loss: 0.04682868346571922\n",
      "Epoch: 812 Loss: 0.049895238131284714\n",
      "Epoch: 813 Loss: 0.10277018696069717\n",
      "Epoch: 814 Loss: 0.13122770190238953\n",
      "Epoch: 815 Loss: 0.15846437215805054\n",
      "Epoch: 816 Loss: 0.3711923360824585\n",
      "Epoch: 817 Loss: 0.22093458473682404\n",
      "Epoch: 818 Loss: 0.0878247618675232\n",
      "Epoch: 819 Loss: 0.086451955139637\n",
      "Epoch: 820 Loss: 0.0751895159482956\n",
      "Epoch: 821 Loss: 0.10298731923103333\n",
      "Epoch: 822 Loss: 0.28903844952583313\n",
      "Epoch: 823 Loss: 0.10486088693141937\n",
      "Epoch: 824 Loss: 0.3029089868068695\n",
      "Epoch: 825 Loss: 0.13927790522575378\n",
      "Epoch: 826 Loss: 0.22508418560028076\n",
      "Epoch: 827 Loss: 0.14796431362628937\n",
      "Epoch: 828 Loss: 0.11513147503137589\n",
      "Epoch: 829 Loss: 0.03799574822187424\n",
      "Epoch: 830 Loss: 0.1273745447397232\n",
      "Epoch: 831 Loss: 0.09051098674535751\n",
      "Epoch: 832 Loss: 0.17267173528671265\n",
      "Epoch: 833 Loss: 0.1535315215587616\n",
      "Epoch: 834 Loss: 0.04853205382823944\n",
      "Epoch: 835 Loss: 0.20790354907512665\n",
      "Epoch: 836 Loss: 0.17129601538181305\n",
      "Epoch: 837 Loss: 0.10671626776456833\n",
      "Epoch: 838 Loss: 0.13528336584568024\n",
      "Epoch: 839 Loss: 0.05233537033200264\n",
      "Epoch: 840 Loss: 0.07274571806192398\n",
      "Epoch: 841 Loss: 0.08944858610630035\n",
      "Epoch: 842 Loss: 0.0439375601708889\n",
      "Epoch: 843 Loss: 0.08777409046888351\n",
      "Epoch: 844 Loss: 0.09703639894723892\n",
      "Epoch: 845 Loss: 0.11944390833377838\n",
      "Epoch: 846 Loss: 0.2070581167936325\n",
      "Epoch: 847 Loss: 0.09741506725549698\n",
      "Epoch: 848 Loss: 0.09156931936740875\n",
      "Epoch: 849 Loss: 0.11417340487241745\n",
      "Epoch: 850 Loss: 0.1452237218618393\n",
      "Epoch: 851 Loss: 0.25815969705581665\n",
      "Epoch: 852 Loss: 0.11822640150785446\n",
      "Epoch: 853 Loss: 0.051172465085983276\n",
      "Epoch: 854 Loss: 0.10523726046085358\n",
      "Epoch: 855 Loss: 0.09451749920845032\n",
      "Epoch: 856 Loss: 0.22091656923294067\n",
      "Epoch: 857 Loss: 0.16155584156513214\n",
      "Epoch: 858 Loss: 0.05289885774254799\n",
      "Epoch: 859 Loss: 0.0897083729505539\n",
      "Epoch: 860 Loss: 0.10987311601638794\n",
      "Epoch: 861 Loss: 0.19318297505378723\n",
      "Epoch: 862 Loss: 0.12182585895061493\n",
      "Epoch: 863 Loss: 0.07534264028072357\n",
      "Epoch: 864 Loss: 0.18869341909885406\n",
      "Epoch: 865 Loss: 0.1413053721189499\n",
      "Epoch: 866 Loss: 0.09327840059995651\n",
      "Epoch: 867 Loss: 0.11286374926567078\n",
      "Epoch: 868 Loss: 0.13490375876426697\n",
      "Epoch: 869 Loss: 0.07168473303318024\n",
      "Epoch: 870 Loss: 0.07450708746910095\n",
      "Epoch: 871 Loss: 0.14199386537075043\n",
      "Epoch: 872 Loss: 0.05450880527496338\n",
      "Epoch: 873 Loss: 0.09362952411174774\n",
      "Epoch: 874 Loss: 0.1032676175236702\n",
      "Epoch: 875 Loss: 0.0691988468170166\n",
      "Epoch: 876 Loss: 0.2561796307563782\n",
      "Epoch: 877 Loss: 0.055778905749320984\n",
      "Epoch: 878 Loss: 0.06822919100522995\n",
      "Epoch: 879 Loss: 0.11809825152158737\n",
      "Epoch: 880 Loss: 0.14517848193645477\n",
      "Epoch: 881 Loss: 0.05677126348018646\n",
      "Epoch: 882 Loss: 0.05361342430114746\n",
      "Epoch: 883 Loss: 0.07553132623434067\n",
      "Epoch: 884 Loss: 0.1873944103717804\n",
      "Epoch: 885 Loss: 0.17619965970516205\n",
      "Epoch: 886 Loss: 0.2325032651424408\n",
      "Epoch: 887 Loss: 0.09639716893434525\n",
      "Epoch: 888 Loss: 0.6551593542098999\n",
      "Epoch: 889 Loss: 0.6812126636505127\n",
      "Epoch: 890 Loss: 0.0962221696972847\n",
      "Epoch: 891 Loss: 0.1872567981481552\n",
      "Epoch: 892 Loss: 0.07569907605648041\n",
      "Epoch: 893 Loss: 0.14072898030281067\n",
      "Epoch: 894 Loss: 0.03706778585910797\n",
      "Epoch: 895 Loss: 0.14293606579303741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 896 Loss: 0.06327003985643387\n",
      "Epoch: 897 Loss: 0.17623037099838257\n",
      "Epoch: 898 Loss: 0.10806209594011307\n",
      "Epoch: 899 Loss: 0.13631533086299896\n",
      "Epoch: 900 Loss: 0.17754992842674255\n",
      "Epoch: 901 Loss: 0.1079322025179863\n",
      "Epoch: 902 Loss: 0.17323797941207886\n",
      "Epoch: 903 Loss: 0.1483253836631775\n",
      "Epoch: 904 Loss: 0.15364450216293335\n",
      "Epoch: 905 Loss: 0.11186617612838745\n",
      "Epoch: 906 Loss: 0.035727351903915405\n",
      "Epoch: 907 Loss: 0.09208840131759644\n",
      "Epoch: 908 Loss: 0.21955148875713348\n",
      "Epoch: 909 Loss: 0.08833200484514236\n",
      "Epoch: 910 Loss: 0.05526692047715187\n",
      "Epoch: 911 Loss: 0.0651019960641861\n",
      "Epoch: 912 Loss: 0.06816917657852173\n",
      "Epoch: 913 Loss: 0.05233636498451233\n",
      "Epoch: 914 Loss: 0.08572294563055038\n",
      "Epoch: 915 Loss: 0.10050404071807861\n",
      "Epoch: 916 Loss: 0.15696080029010773\n",
      "Epoch: 917 Loss: 0.0459427684545517\n",
      "Epoch: 918 Loss: 0.07087153941392899\n",
      "Epoch: 919 Loss: 0.07504300773143768\n",
      "Epoch: 920 Loss: 0.05212865769863129\n",
      "Epoch: 921 Loss: 0.1289859116077423\n",
      "Epoch: 922 Loss: 0.11267311125993729\n",
      "Epoch: 923 Loss: 0.09884496033191681\n",
      "Epoch: 924 Loss: 0.10854306817054749\n",
      "Epoch: 925 Loss: 0.0674244612455368\n",
      "Epoch: 926 Loss: 0.17916469275951385\n",
      "Epoch: 927 Loss: 0.171513631939888\n",
      "Epoch: 928 Loss: 0.07990501821041107\n",
      "Epoch: 929 Loss: 0.07234430313110352\n",
      "Epoch: 930 Loss: 0.05840221047401428\n",
      "Epoch: 931 Loss: 0.25522661209106445\n",
      "Epoch: 932 Loss: 0.07797485589981079\n",
      "Epoch: 933 Loss: 0.1294112652540207\n",
      "Epoch: 934 Loss: 0.08830089122056961\n",
      "Epoch: 935 Loss: 0.04001065716147423\n",
      "Epoch: 936 Loss: 0.11050005257129669\n",
      "Epoch: 937 Loss: 0.0470241941511631\n",
      "Epoch: 938 Loss: 0.07737809419631958\n",
      "Epoch: 939 Loss: 0.1170811802148819\n",
      "Epoch: 940 Loss: 0.10834965854883194\n",
      "Epoch: 941 Loss: 0.07087744027376175\n",
      "Epoch: 942 Loss: 0.046081651002168655\n",
      "Epoch: 943 Loss: 0.1582808643579483\n",
      "Epoch: 944 Loss: 0.18657496571540833\n",
      "Epoch: 945 Loss: 0.35312432050704956\n",
      "Epoch: 946 Loss: 0.08250278234481812\n",
      "Epoch: 947 Loss: 0.0477725975215435\n",
      "Epoch: 948 Loss: 0.11482126265764236\n",
      "Epoch: 949 Loss: 0.053715504705905914\n",
      "Epoch: 950 Loss: 0.08440322428941727\n",
      "Epoch: 951 Loss: 0.1645471304655075\n",
      "Epoch: 952 Loss: 0.07783196121454239\n",
      "Epoch: 953 Loss: 0.2019433081150055\n",
      "Epoch: 954 Loss: 0.08548297733068466\n",
      "Epoch: 955 Loss: 0.07539226114749908\n",
      "Epoch: 956 Loss: 0.1315571516752243\n",
      "Epoch: 957 Loss: 0.08099602162837982\n",
      "Epoch: 958 Loss: 0.10026059299707413\n",
      "Epoch: 959 Loss: 0.0833776518702507\n",
      "Epoch: 960 Loss: 0.11005174368619919\n",
      "Epoch: 961 Loss: 0.16669529676437378\n",
      "Epoch: 962 Loss: 0.09665210545063019\n",
      "Epoch: 963 Loss: 0.1716168075799942\n",
      "Epoch: 964 Loss: 0.4507431387901306\n",
      "Epoch: 965 Loss: 0.03571715205907822\n",
      "Epoch: 966 Loss: 0.10344073176383972\n",
      "Epoch: 967 Loss: 0.06899666041135788\n",
      "Epoch: 968 Loss: 0.06454472243785858\n",
      "Epoch: 969 Loss: 0.03388083726167679\n",
      "Epoch: 970 Loss: 0.21206291019916534\n",
      "Epoch: 971 Loss: 0.16673466563224792\n",
      "Epoch: 972 Loss: 0.07122812420129776\n",
      "Epoch: 973 Loss: 0.0745791345834732\n",
      "Epoch: 974 Loss: 0.09802139550447464\n",
      "Epoch: 975 Loss: 0.13204307854175568\n",
      "Epoch: 976 Loss: 0.17277023196220398\n",
      "Epoch: 977 Loss: 0.06108596548438072\n",
      "Epoch: 978 Loss: 0.07326663285493851\n",
      "Epoch: 979 Loss: 0.05739206820726395\n",
      "Epoch: 980 Loss: 0.09343941509723663\n",
      "Epoch: 981 Loss: 0.09449702501296997\n",
      "Epoch: 982 Loss: 0.07365137338638306\n",
      "Epoch: 983 Loss: 0.03406378626823425\n",
      "Epoch: 984 Loss: 0.05499642342329025\n",
      "Epoch: 985 Loss: 0.608803927898407\n",
      "Epoch: 986 Loss: 0.11475865542888641\n",
      "Epoch: 987 Loss: 0.16318896412849426\n",
      "Epoch: 988 Loss: 0.09533806890249252\n",
      "Epoch: 989 Loss: 0.12046552449464798\n",
      "Epoch: 990 Loss: 0.0683586448431015\n",
      "Epoch: 991 Loss: 0.0711599662899971\n",
      "Epoch: 992 Loss: 0.060871053487062454\n",
      "Epoch: 993 Loss: 0.2739102244377136\n",
      "Epoch: 994 Loss: 0.09724409878253937\n",
      "Epoch: 995 Loss: 0.11200568825006485\n",
      "Epoch: 996 Loss: 0.10459008812904358\n",
      "Epoch: 997 Loss: 0.09779089689254761\n",
      "Epoch: 998 Loss: 0.12074500322341919\n",
      "Epoch: 999 Loss: 0.2532060444355011\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    losses = []\n",
    "    for batch_idx, (data1, data2, targets) in enumerate(train_loader):\n",
    "        data1 = data1.to(device=device)\n",
    "        data2 = data2.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #output = model(data1, data2)\n",
    "        \n",
    "        output = model(\n",
    "            past_values=data1,\n",
    "            future_values=data2,\n",
    "        )\n",
    "        \n",
    "        loss = 10*criterion(output, targets)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    print('Epoch:', epoch, 'Loss:', loss.item())\n",
    "    \n",
    "    if mean_loss<min_val_acc:\n",
    "        min_val_acc =mean_loss\n",
    "        torch.save(model.state_dict(), 'best_cos_3-3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = torch.from_numpy(trainX1_copy).float()\n",
    "test_data2 = torch.from_numpy(trainX2_copy).float()\n",
    "test_label = torch.from_numpy(trainY_copy).float()\n",
    "test_dataset = TensorDataset(test_data1, test_data2, test_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for batch_idx, (data1, data2, targets) in enumerate(test_loader):\n",
    "    data1 = data1.to(device=device)\n",
    "    data2 = data2.to(device=device)\n",
    "    output = model(\n",
    "        past_values=data1,\n",
    "        future_values=data2,\n",
    "    )\n",
    "    targets = targets.to(device=device)\n",
    "    predictions+=output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_copies1 = np.repeat(predictions[:,0].reshape(-1,1), df_for_training.shape[1], axis=-1) \n",
    "predict_1 = scaler.inverse_transform(predict_copies1)[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_copies1 = np.repeat(trainY_copy[:,0].reshape(-1,1), df_for_training.shape[1], axis=-1)\n",
    "trainY_1 = scaler.inverse_transform(trainY_copies1)[:,21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "plt.xlabel('样本 /个',fontdict={ 'size'   : 30})\n",
    "plt.ylabel('推力值 /N',fontdict={ 'size'   : 30})\n",
    "plt.plot(trainY_1[200:11000],marker='.',label='true')\n",
    "plt.plot(predict_1[200:11000],'r',marker='.',label='predicted')                  #sample的时刻是一致的\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
